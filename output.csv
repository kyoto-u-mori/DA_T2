K-means_label,category,title,abstract
1,0,Channel Cycle Time: A New Measure of Short-term Fairness,"This paper puts forth a new metric, dubbed channel cycle time, to measure the
short-term fairness of communication networks. Channel cycle time characterizes
the average duration between two successful transmissions of a user, during
which all other users have successfully accessed the channel at least once.
Compared with existing short-term fairness measures, channel cycle time
provides a comprehensive picture of the transient behavior of communication
networks, and is a single real value that is easy to compute. To demonstrate
the effectiveness of our new approach, we analytically characterize the channel
cycle time of slotted Aloha and CSMA/CA. It is shown that CSMA/CA is a
short-term fairer protocol than slotted Aloha. Channel cycle time can serve as
a promising design principle for future communication networks, placing greater
emphasis on optimizing short-term behaviors like fairness, delay, and jitter."
0,0,Developing Multi-Agent Systems with Degrees of Neuro-Symbolic Integration [A Position Paper],"In this short position paper we highlight our ongoing work on verifiable
heterogeneous multi-agent systems and, in particular, the complex (and often
non-functional) issues that impact the choice of structure within each agent."
0,0,Terraforming -- Environment Manipulation during Disruptions for Multi-Agent Pickup and Delivery,"In automated warehouses, teams of mobile robots fulfill the packaging process
by transferring inventory pods to designated workstations while navigating
narrow aisles formed by tightly packed pods. This problem is typically modeled
as a Multi-Agent Pickup and Delivery (MAPD) problem, which is then solved by
repeatedly planning collision-free paths for agents on a fixed graph, as in the
Rolling-Horizon Collision Resolution (RHCR) algorithm. However, existing
approaches make the limiting assumption that agents are only allowed to move
pods that correspond to their current task, while considering the other pods as
stationary obstacles (even though all pods are movable). This behavior can
result in unnecessarily long paths which could otherwise be avoided by opening
additional corridors via pod manipulation. To this end, we explore the
implications of allowing agents the flexibility of dynamically relocating pods.
We call this new problem Terraforming MAPD (tMAPD) and develop an RHCR-based
approach to tackle it. As the extra flexibility of terraforming comes at a
significant computational cost, we utilize this capability judiciously by
identifying situations where it could make a significant impact on the solution
quality. In particular, we invoke terraforming in response to disruptions that
often occur in automated warehouses, e.g., when an item is dropped from a pod
or when agents malfunction. Empirically, using our approach for tMAPD, where
disruptions are modeled via a stochastic process, we improve throughput by over
10%, reduce the maximum service time (the difference between the drop-off time
and the pickup time of a pod) by more than 50%, without drastically increasing
the runtime, compared to the MAPD setting."
0,0,Learning Diverse Risk Preferences in Population-based Self-play,"Among the great successes of Reinforcement Learning (RL), self-play
algorithms play an essential role in solving competitive games. Current
self-play algorithms optimize the agent to maximize expected win-rates against
its current or historical copies, making it often stuck in the local optimum
and its strategy style simple and homogeneous. A possible solution is to
improve the diversity of policies, which helps the agent break the stalemate
and enhances its robustness when facing different opponents. However, enhancing
diversity in the self-play algorithms is not trivial. In this paper, we aim to
introduce diversity from the perspective that agents could have diverse risk
preferences in the face of uncertainty. Specifically, we design a novel
reinforcement learning algorithm called Risk-sensitive Proximal Policy
Optimization (RPPO), which smoothly interpolates between worst-case and
best-case policy learning and allows for policy learning with desired risk
preferences. Seamlessly integrating RPPO with population-based self-play,
agents in the population optimize dynamic risk-sensitive objectives with
experiences from playing against diverse opponents. Empirical results show that
our method achieves comparable or superior performance in competitive games and
that diverse modes of behaviors emerge. Our code is public online at
\url{https://github.com/Jackory/RPBT}."
0,0,Counterfactual Fairness Filter for Fair-Delay Multi-Robot Navigation,"Multi-robot navigation is the task of finding trajectories for a team of
robotic agents to reach their destinations as quickly as possible without
collisions. In this work, we introduce a new problem: fair-delay multi-robot
navigation, which aims not only to enable such efficient, safe travels but also
to equalize the travel delays among agents in terms of actual trajectories as
compared to the best possible trajectories. The learning of a navigation policy
to achieve this objective requires resolving a nontrivial credit assignment
problem with robotic agents having continuous action spaces. Hence, we
developed a new algorithm called Navigation with Counterfactual Fairness Filter
(NCF2). With NCF2, each agent performs counterfactual inference on whether it
can advance toward its goal or should stay still to let other agents go. Doing
so allows us to effectively address the aforementioned credit assignment
problem and improve fairness regarding travel delays while maintaining high
efficiency and safety. Our extensive experimental results in several
challenging multi-robot navigation environments demonstrate the greater
effectiveness of NCF2 as compared to state-of-the-art fairness-aware
multi-agent reinforcement learning methods. Our demo videos and code are
available on the project webpage: https://omron-sinicx.github.io/ncf2/"
1,0,Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions,"Finding an agreement among diverse opinions is a challenging topic in
multiagent systems. Recently, large language models (LLMs) have shown great
potential in addressing this challenge due to their remarkable capabilities in
comprehending human opinions and generating human-like text. However, they
typically rely on extensive human-annotated data. In this paper, we propose
Self-Agreement, a novel framework for fine-tuning LLMs to autonomously find
agreement using data generated by LLM itself. Specifically, our approach
employs the generative pre-trained transformer-3 (GPT-3) to generate multiple
opinions for each question in a question dataset and create several agreement
candidates among these opinions. Then, a bidirectional encoder representations
from transformers (BERT)-based model evaluates the agreement score of each
agreement candidate and selects the one with the highest agreement score. This
process yields a dataset of question-opinion-agreements, which we use to
fine-tune a pre-trained LLM for discovering agreements among diverse opinions.
Remarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework
achieves comparable performance to GPT-3 with only 1/25 of its parameters,
showcasing its ability to identify agreement among various opinions without the
need for human-annotated data."
0,0,Understanding the World to Solve Social Dilemmas Using Multi-Agent Reinforcement Learning,"Social dilemmas are situations where groups of individuals can benefit from
mutual cooperation but conflicting interests impede them from doing so. This
type of situations resembles many of humanity's most critical challenges, and
discovering mechanisms that facilitate the emergence of cooperative behaviors
is still an open problem. In this paper, we study the behavior of
self-interested rational agents that learn world models in a multi-agent
reinforcement learning (RL) setting and that coexist in environments where
social dilemmas can arise. Our simulation results show that groups of agents
endowed with world models outperform all the other tested ones when dealing
with scenarios where social dilemmas can arise. We exploit the world model
architecture to qualitatively assess the learnt dynamics and confirm that each
agent's world model is capable to encode information of the behavior of the
changing environment and the other agent's actions. This is the first work that
shows that world models facilitate the emergence of complex coordinated
behaviors that enable interacting agents to ``understand'' both environmental
and social dynamics."
0,0,Collective Reasoning for Safe Autonomous Systems,"Collaboration in multi-agent autonomous systems is critical to increase
performance while ensuring safety. However, due to heterogeneity of their
features in, e.g., perception qualities, some autonomous systems have to be
considered more trustworthy than others when contributing to collaboratively
build a common environmental model, especially under uncertainty. In this
paper, we introduce the idea of increasing the reliability of autonomous
systems by relying on collective intelligence. We borrow concepts from social
epistemology to exploit individual characteristics of autonomous systems, and
define and formalize at design rules for collective reasoning to achieve
collaboratively increased safety, trustworthiness and good decision making."
0,0,Constrained Environment Optimization for Prioritized Multi-Agent Navigation,"Traditional approaches to the design of multi-agent navigation algorithms
consider the environment as a fixed constraint, despite the influence of
spatial constraints on agents' performance. Yet hand-designing conducive
environment layouts is inefficient and potentially expensive. The goal of this
paper is to consider the environment as a decision variable in a system-level
optimization problem, where both agent performance and environment cost are
incorporated. Towards this end, we propose novel problems of unprioritized and
prioritized environment optimization, where the former considers agents
unbiasedly and the latter accounts for agent priorities. We show, through
formal proofs, under which conditions the environment can change while
guaranteeing completeness (i.e., all agents reach goals), and analyze the role
of agent priorities in the environment optimization. We proceed to impose
real-world constraints on the environment optimization and formulate it
mathematically as a constrained stochastic optimization problem. Since the
relation between agents, environment and performance is challenging to model,
we leverage reinforcement learning to develop a model-free solution and a
primal-dual mechanism to handle constraints. Distinct information processing
architectures are integrated for various implementation scenarios, including
online/offline optimization and discrete/continuous environment. Numerical
results corroborate the theory and demonstrate the validity and adaptability of
our approach."
1,0,Worst-Case VCG Redistribution Mechanism Design Based on the Lottery Ticket Hypothesis,"We study worst-case VCG redistribution mechanism design for the public
project problem. For VCG redistribution mechanisms, the mechanism design task
is only on setting the payments, subject to strategy-proofness and the
non-deficit constraint, while maximizing the worst-case allocative efficiency
ratio.
  We use a multilayer perceptron (MLP) with ReLU activation to model the
payment function. We use mixed integer programming (MIP) to solve for the
worst-case type profiles that maximally violate the mechanism design
constraints. We collect these worst-case type profiles and use them as training
samples to train toward better worst-case mechanisms.
  We require a tiny network structure for MIP to scale. The Lottery Ticket
Hypothesis states that a large network is likely to contain a winning ticket --
a much smaller subnetwork that won the initialization lottery, which makes its
training particularly effective. We train a large network and prune it into a
tiny subnetwork (i.e., draw a ticket). We run MIP-based worst-case training on
the drawn subnetwork and evaluate the training result's worst-case performance
(i.e., scratch the ticket). If the subnetwork can not achieve good worst-case
performance, then we record the type profiles that cause the current draw to be
bad. To draw again, we restore the large network to its initial weights and
prune using recorded type profiles from earlier draws (i.e., redraw from the
original ticket pot while avoiding drawing the same ticket twice). With a large
enough initial network and a large enough number of draws, we expect to
eventually encounter a tiny trainable subnetwork.
  We find an optimal mechanism for 3 agents that uses only 2 hidden nodes! We
also find previously unknown optimal mechanisms for 4 and 5 agents. For up to
20 agents, we derive significantly improved worst-case mechanisms compared to
existing manual results."
0,0,Sharing Lifelong Reinforcement Learning Knowledge via Modulating Masks,"Lifelong learning agents aim to learn multiple tasks sequentially over a
lifetime. This involves the ability to exploit previous knowledge when learning
new tasks and to avoid forgetting. Modulating masks, a specific type of
parameter isolation approach, have recently shown promise in both supervised
and reinforcement learning. While lifelong learning algorithms have been
investigated mainly within a single-agent approach, a question remains on how
multiple agents can share lifelong learning knowledge with each other. We show
that the parameter isolation mechanism used by modulating masks is particularly
suitable for exchanging knowledge among agents in a distributed and
decentralized system of lifelong learners. The key idea is that the isolation
of specific task knowledge to specific masks allows agents to transfer only
specific knowledge on-demand, resulting in robust and effective distributed
lifelong learning. We assume fully distributed and asynchronous scenarios with
dynamic agent numbers and connectivity. An on-demand communication protocol
ensures agents query their peers for specific masks to be transferred and
integrated into their policies when facing each task. Experiments indicate that
on-demand mask communication is an effective way to implement distributed
lifelong reinforcement learning and provides a lifelong learning benefit with
respect to distributed RL baselines such as DD-PPO, IMPALA, and PPO+EWC. The
system is particularly robust to connection drops and demonstrates rapid
learning due to knowledge exchange."
1,0,Participatory Budgeting With Multiple Degrees of Projects And Ranged Approval Votes,"In an indivisible participatory budgeting (PB) framework, we have a limited
budget that is to be distributed among a set of projects, by aggregating the
preferences of voters for the projects. All the prior work on indivisible PB
assumes that each project has only one possible cost. In this work, we let each
project have a set of permissible costs, each reflecting a possible degree of
sophistication of the project. Each voter approves a range of costs for each
project, by giving an upper and lower bound on the cost that she thinks the
project deserves. The outcome of a PB rule selects a subset of projects and
also specifies their corresponding costs. We study different utility notions
and prove that the existing positive results when every project has exactly one
permissible cost can also be extended to our framework where a project has
several permissible costs. We also analyze the fixed parameter tractability of
the problem. Finally, we propose some important and intuitive axioms and
analyze their satisfiability by different PB rules. We conclude by making some
crucial remarks."
0,0,TPMDP: Threshold Personalized Multi-party Differential Privacy via Optimal Gaussian Mechanism,"In modern distributed computing applications, such as federated learning and
AIoT systems, protecting privacy is crucial to prevent misbehaving parties from
colluding to steal others' private information. However, guaranteeing the
utility of computation outcomes while protecting all parties' privacy can be
challenging, particularly when the parties' privacy requirements are highly
heterogeneous. In this paper, we propose a novel privacy framework for
multi-party computation called Threshold Personalized Multi-party Differential
Privacy (TPMDP), which addresses a limited number of semi-honest colluding
adversaries. Our framework enables each party to have a personalized privacy
budget. We design a multi-party Gaussian mechanism that is easy to implement
and satisfies TPMDP, wherein each party perturbs the computation outcome in a
secure multi-party computation protocol using Gaussian noise. To optimize the
utility of the mechanism, we cast the utility loss minimization problem into a
linear programming (LP) problem. We exploit the specific structure of this LP
problem to compute the optimal solution after O(n) computations, where n is the
number of parties, while a generic solver may require exponentially many
computations. Extensive experiments demonstrate the benefits of our approach in
terms of low utility loss and high efficiency compared to existing private
mechanisms that do not consider personalized privacy requirements or collusion
thresholds."
0,0,Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning,"The difficulty of appropriately assigning credit is particularly heightened
in cooperative MARL with sparse reward, due to the concurrent time and
structural scales involved. Automatic subgoal generation (ASG) has recently
emerged as a viable MARL approach inspired by utilizing subgoals in
intrinsically motivated reinforcement learning. However, end-to-end learning of
complex task planning from sparse rewards without prior knowledge, undoubtedly
requires massive training samples. Moreover, the diversity-promoting nature of
existing ASG methods can lead to the ""over-representation"" of subgoals,
generating numerous spurious subgoals of limited relevance to the actual task
reward and thus decreasing the sample efficiency of the algorithm. To address
this problem and inspired by the disentangled representation learning, we
propose a novel ""disentangled"" decision-making method, Semantically Aligned
task decomposition in MARL (SAMA), that prompts pretrained language models with
chain-of-thought that can suggest potential goals, provide suitable goal
decomposition and subgoal allocation as well as self-reflection-based
replanning. Additionally, SAMA incorporates language-grounded RL to train each
agent's subgoal-conditioned policy. SAMA demonstrates considerable advantages
in sample efficiency compared to state-of-the-art ASG methods, as evidenced by
its performance on two challenging sparse-reward tasks, Overcooked and MiniRTS."
1,0,Q-SHED: Distributed Optimization at the Edge via Hessian Eigenvectors Quantization,"Edge networks call for communication efficient (low overhead) and robust
distributed optimization (DO) algorithms. These are, in fact, desirable
qualities for DO frameworks, such as federated edge learning techniques, in the
presence of data and system heterogeneity, and in scenarios where internode
communication is the main bottleneck. Although computationally demanding,
Newton-type (NT) methods have been recently advocated as enablers of robust
convergence rates in challenging DO problems where edge devices have sufficient
computational power. Along these lines, in this work we propose Q-SHED, an
original NT algorithm for DO featuring a novel bit-allocation scheme based on
incremental Hessian eigenvectors quantization. The proposed technique is
integrated with the recent SHED algorithm, from which it inherits appealing
features like the small number of required Hessian computations, while being
bandwidth-versatile at a bit-resolution level. Our empirical evaluation against
competing approaches shows that Q-SHED can reduce by up to 60% the number of
communication rounds required for convergence."
0,0,On the Geometric Convergence of Byzantine-Resilient Distributed Optimization Algorithms,"The problem of designing distributed optimization algorithms that are
resilient to Byzantine adversaries has received significant attention. For the
Byzantine-resilient distributed optimization problem, the goal is to
(approximately) minimize the average of the local cost functions held by the
regular (non adversarial) agents in the network. In this paper, we provide a
general algorithmic framework for Byzantine-resilient distributed optimization
which includes some state-of-the-art algorithms as special cases. We analyze
the convergence of algorithms within the framework, and derive a geometric rate
of convergence of all regular agents to a ball around the optimal solution
(whose size we characterize). Furthermore, we show that approximate consensus
can be achieved geometrically fast under some minimal conditions. Our analysis
provides insights into the relationship among the convergence region, distance
between regular agents' values, step-size, and properties of the agents'
functions for Byzantine-resilient distributed optimization."
0,0,Discovering Individual Rewards in Collective Behavior through Inverse Multi-Agent Reinforcement Learning,"The discovery of individual objectives in collective behavior of complex
dynamical systems such as fish schools and bacteria colonies is a long-standing
challenge. Inverse reinforcement learning is a potent approach for addressing
this challenge but its applicability to dynamical systems, involving continuous
state-action spaces and multiple interacting agents, has been limited. In this
study, we tackle this challenge by introducing an off-policy inverse
multi-agent reinforcement learning algorithm (IMARL). Our approach combines the
ReF-ER techniques with guided cost learning. By leveraging demonstrations, our
algorithm automatically uncovers the reward function and learns an effective
policy for the agents. Through extensive experimentation, we demonstrate that
the proposed policy captures the behavior observed in the provided data, and
achieves promising results across problem domains including single agent models
in the OpenAI gym and multi-agent models of schooling behavior. The present
study shows that the proposed IMARL algorithm is a significant step towards
understanding collective dynamics from the perspective of its constituents, and
showcases its value as a tool for studying complex physical systems exhibiting
collective behaviour."
0,0,Optimal Seat Arrangement: What Are the Hard and Easy Cases?,"We study four NP-hard optimal seat arrangement problems [Bodlaender et al.,
2020a], which each have as input a set of n agents, where each agent has
cardinal preferences over other agents, and an n-vertex undirected graph
(called seat graph). The task is to assign each agent to a distinct vertex in
the seat graph such that either the sum of utilities or the minimum utility is
maximized, or it is envy-free or exchange-stable. Aiming at identifying hard
and easy cases, we extensively study the algorithmic complexity of the four
problems by looking into natural graph classes for the seat graph (e.g., paths,
cycles, stars, or matchings), problem-specific parameters (e.g., the number of
non-isolated vertices in the seat graph or the maximum number of agents towards
whom an agent has non-zero preferences), and preference structures (e.g.,
non-negative or symmetric preferences). For strict preferences and seat graphs
with disjoint edges and isolated vertices, we correct an error by Bodlaender et
al. [2020b] and show that finding an envy-free arrangement remains NP-hard in
this case."
0,0,Set-Membership Filtering-Based Cooperative State Estimation for Multi-Agent Systems,"In this article, we focus on the cooperative state estimation problem of a
multi-agent system. Each agent is equipped with absolute and relative
measurements. The purpose of this research is to make each agent generate its
own state estimation with only local measurement information and local
communication with neighborhood agents using Set Membership Filter(SMF). To
handle this problem, we analyzed centralized SMF framework as a benchmark of
distributed SMF and propose a finite-horizon method called OIT-Inspired
centralized constrained zonotopic algorithm. Moreover, we put forward a
distributed Set Membership Filtering(SMFing) framework and develop a
distributed constained zonotopic algorithm. Finally, simulation verified our
theoretical results, that our proposed algorithms can effectively estimate the
state of each agent."
0,0,Principal-Agent Boolean Games,"We introduce and study a computational version of the principal-agent problem
-- a classic problem in Economics that arises when a principal desires to
contract an agent to carry out some task, but has incomplete information about
the agent or their subsequent actions. The key challenge in this setting is for
the principal to design a contract for the agent such that the agent's
preferences are then aligned with those of the principal. We study this problem
using a variation of Boolean games, where multiple players each choose
valuations for Boolean variables under their control, seeking the satisfaction
of a personal goal, given as a Boolean logic formula. In our setting, the
principal can only observe some subset of these variables, and the principal
chooses a contract which rewards players on the basis of the assignments they
make for the variables that are observable to the principal. The principal's
challenge is to design a contract so that, firstly, the principal's goal is
achieved in some or all Nash equilibrium choices, and secondly, that the
principal is able to verify that their goal is satisfied. In this paper, we
formally define this problem and completely characterise the computational
complexity of the most relevant decision problems associated with it."
0,0,Agent Heterogeneity Mediates Extremism in an Adaptive Social Network Model,"An existing model of opinion dynamics on an adaptive social network is
extended to introduce update policy heterogeneity, representing the fact that
individual differences between social animals can affect their tendency to
form, and be influenced by, their social bonds with other animals. As in the
original model, the opinions and social connections of a population of model
agents change due to three social processes: conformity, homophily and
neophily. Here, however, we explore the case in which each node's
susceptibility to these three processes is parameterised by node-specific
values drawn independently at random from some distribution. This introduction
of heterogeneity increases both the degree of extremism and connectedness in
the final population (relative to comparable homogeneous networks) and leads to
significant assortativity with respect to node update policy parameters as well
as node opinions. Each node's update policy parameters also predict properties
of the community that they will belong to in the final network configuration.
These results suggest that update policy heterogeneity in social populations
may have a significant impact on the formation of extremist communities in
real-world populations."
0,0,Collective Large-scale Wind Farm Multivariate Power Output Control Based on Hierarchical Communication Multi-Agent Proximal Policy Optimization,"Wind power is becoming an increasingly important source of renewable energy
worldwide. However, wind farm power control faces significant challenges due to
the high system complexity inherent in these farms. A novel communication-based
multi-agent deep reinforcement learning large-scale wind farm multivariate
control is proposed to handle this challenge and maximize power output. A wind
farm multivariate power model is proposed to study the influence of wind
turbines (WTs) wake on power. The multivariate model includes axial induction
factor, yaw angle, and tilt angle controllable variables. The hierarchical
communication multi-agent proximal policy optimization (HCMAPPO) algorithm is
proposed to coordinate the multivariate large-scale wind farm continuous
controls. The large-scale wind farm is divided into multiple wind turbine
aggregators (WTAs), and neighboring WTAs can exchange information through
hierarchical communication to maximize the wind farm power output. Simulation
results demonstrate that the proposed multivariate HCMAPPO can significantly
increase wind farm power output compared to the traditional PID control,
coordinated model-based predictive control, and multi-agent deep deterministic
policy gradient algorithm. Particularly, the HCMAPPO algorithm can be trained
with the environment based on the thirteen-turbine wind farm and effectively
applied to larger wind farms. At the same time, there is no significant
increase in the fatigue damage of the wind turbine blade from the wake control
as the wind farm scale increases. The multivariate HCMAPPO control can realize
the collective large-scale wind farm maximum power output."
0,0,Synthesizing Resilient Strategies for Infinite-Horizon Objectives in Multi-Agent Systems,"We consider the problem of synthesizing resilient and stochastically stable
strategies for systems of cooperating agents striving to minimize the expected
time between consecutive visits to selected locations in a known environment. A
strategy profile is resilient if it retains its functionality even if some of
the agents fail, and stochastically stable if the visiting time variance is
small. We design a novel specification language for objectives involving
resilience and stochastic stability, and we show how to efficiently compute
strategy profiles (for both autonomous and coordinated agents) optimizing these
objectives. Our experiments show that our strategy synthesis algorithm can
construct highly non-trivial and efficient strategy profiles for environments
with general topology."
0,0,A Deep RL Approach on Task Placement and Scaling of Edge Resources for Cellular Vehicle-to-Network Service Provisioning,"Cellular-Vehicle-to-Everything (C-V2X) is currently at the forefront of the
digital transformation of our society. By enabling vehicles to communicate with
each other and with the traffic environment using cellular networks, we
redefine transportation, improving road safety and transportation services,
increasing efficiency of traffic flows, and reducing environmental impact. This
paper proposes a decentralized approach for provisioning Cellular
Vehicular-to-Network (C-V2N) services, addressing the coupled problems of
service task placement and scaling of edge resources. We formalize the joint
problem and prove its complexity. We propose an approach to tackle it, linking
the two problems, employing decentralized decision-making using (i) a greedy
approach for task placement and (ii) a Deep Deterministic Policy Gradient
(DDPG) based approach for scaling. We benchmark the performance of our
approach, focusing on the scaling agent, against several State-of-the-Art (SoA)
scaling approaches via simulations using a real C-V2N traffic data set. The
results show that DDPG-based solutions outperform SoA solutions, keeping the
latency experienced by the C-V2N service below the target delay while
optimizing the use of computing resources. By conducting a complexity analysis,
we prove that DDPG-based solutions achieve runtimes in the range of
sub-milliseconds, meeting the strict latency requirements of C-V2N services."
0,0,RAMario: Experimental Approach to Reptile Algorithm -- Reinforcement Learning for Mario,"This research paper presents an experimental approach to using the Reptile
algorithm for reinforcement learning to train a neural network to play Super
Mario Bros. We implement the Reptile algorithm using the Super Mario Bros Gym
library and TensorFlow in Python, creating a neural network model with a single
convolutional layer, a flatten layer, and a dense layer. We define the
optimizer and use the Reptile class to create an instance of the Reptile
meta-learning algorithm. We train the model using multiple tasks and episodes,
choosing actions using the current weights of the neural network model, taking
those actions in the environment, and updating the model weights using the
Reptile algorithm. We evaluate the performance of the algorithm by printing the
total reward for each episode. In addition, we compare the performance of the
Reptile algorithm approach to two other popular reinforcement learning
algorithms, Proximal Policy Optimization (PPO) and Deep Q-Network (DQN),
applied to the same Super Mario Bros task. Our results demonstrate that the
Reptile algorithm provides a promising approach to few-shot learning in video
game AI, with comparable or even better performance than the other two
algorithms, particularly in terms of moves vs distance that agent performs for
1M episodes of training. The results shows that best total distance for world
1-2 in the game environment were ~1732 (PPO), ~1840 (DQN) and ~2300 (RAMario).
Full code is available at https://github.com/s4nyam/RAMario."
0,0,An Empirical Study on Google Research Football Multi-agent Scenarios,"Few multi-agent reinforcement learning (MARL) research on Google Research
Football (GRF) focus on the 11v11 multi-agent full-game scenario and to the
best of our knowledge, no open benchmark on this scenario has been released to
the public. In this work, we fill the gap by providing a population-based MARL
training pipeline and hyperparameter settings on multi-agent football scenario
that outperforms the bot with difficulty 1.0 from scratch within 2 million
steps. Our experiments serve as a reference for the expected performance of
Independent Proximal Policy Optimization (IPPO), a state-of-the-art multi-agent
reinforcement learning algorithm where each agent tries to maximize its own
policy independently across various training configurations. Meanwhile, we
open-source our training framework Light-MALib which extends the MALib codebase
by distributed and asynchronized implementation with additional analytical
tools for football games. Finally, we provide guidance for building strong
football AI with population-based training and release diverse pretrained
policies for benchmarking. The goal is to provide the community with a head
start for whoever experiment their works on GRF and a simple-to-use
population-based training framework for further improving their agents through
self-play. The implementation is available at
https://github.com/Shanghai-Digital-Brain-Laboratory/DB-Football."
0,0,Establishing Shared Query Understanding in an Open Multi-Agent System,"We propose a method that allows to develop shared understanding between two
agents for the purpose of performing a task that requires cooperation. Our
method focuses on efficiently establishing successful task-oriented
communication in an open multi-agent system, where the agents do not know
anything about each other and can only communicate via grounded interaction.
The method aims to assist researchers that work on human-machine interaction or
scenarios that require a human-in-the-loop, by defining interaction
restrictions and efficiency metrics. To that end, we point out the challenges
and limitations of such a (diverse) setup, while also restrictions and
requirements which aim to ensure that high task performance truthfully reflects
the extent to which the agents correctly understand each other. Furthermore, we
demonstrate a use-case where our method can be applied for the task of
cooperative query answering. We design the experiments by modifying an
established ontology alignment benchmark. In this example, the agents want to
query each other, while representing different databases, defined in their own
ontologies that contain different and incomplete knowledge. Grounded
interaction here has the form of examples that consists of common instances,
for which the agents are expected to have similar knowledge. Our experiments
demonstrate successful communication establishment under the required
restrictions, and compare different agent policies that aim to solve the task
in an efficient manner."
1,0,Robust Auction Design with Support Information,"A seller wants to sell an item to $n$ buyers. The buyer valuations are drawn
i.i.d. from a distribution, but the seller does not know this distribution; the
seller only knows the support $[a,b]$. To be robust against the lack of
knowledge of the environment and buyers' behavior, the seller optimizes over
DSIC mechanisms, and measures the worst-case performance relative to an oracle
with complete knowledge of buyers' valuations. Our analysis encompasses both
the regret and the ratio objectives.
  For these objectives, we derive an optimal mechanism in closed form as a
function of the support and the number of buyers $n$. Our analysis reveals
three regimes of support information and a new class of robust mechanisms. i.)
With ""low"" support information, the optimal mechanism is a second-price auction
(SPA) with a random reserve, a focal class in the earlier literature. ii.) With
""high"" support information, we show that second-price auctions are strictly
suboptimal, and an optimal mechanism belongs to a novel class of mechanisms we
introduce, which we call $\textbf{pooling auctions}$ (POOL); whenever the
highest value is above a threshold, the mechanism still allocates to the
highest bidder, but otherwise the mechanism allocates to a uniformly random
buyer, i.e., pools low types. iii.) With ""moderate"" support information, a
randomization between SPA and POOL is optimal.
  We also characterize optimal mechanisms within nested central subclasses of
mechanisms: standard mechanisms (only allocate to the highest bidder), SPA with
random reserve, and SPA with no reserve. We show strict separations across
classes, implying that deviating from standard mechanisms is necessary for
robustness. Lastly, we show that the same results hold under other distribution
classes that capture ""positive dependence"" (mixture of i.i.d., exchangeable and
affiliated), as well as i.i.d. regular distributions."
1,0,Deliberation and Voting in Approval-Based Multi-Winner Elections,"Citizen-focused democratic processes where participants deliberate on
alternatives and then vote to make the final decision are increasingly popular
today. While the computational social choice literature has extensively
investigated voting rules, there is limited work that explicitly looks at the
interplay of the deliberative process and voting. In this paper, we build a
deliberation model using established models from the opinion-dynamics
literature and study the effect of different deliberation mechanisms on voting
outcomes achieved when using well-studied voting rules. Our results show that
deliberation generally improves welfare and representation guarantees, but the
results are sensitive to how the deliberation process is organized. We also
show, experimentally, that simple voting rules, such as approval voting,
perform as well as more sophisticated rules such as proportional approval
voting or method of equal shares if deliberation is properly supported. This
has ramifications on the practical use of such voting rules in citizen-focused
democratic processes."
0,0,Exploration of unknown indoor regions by a swarm of energy-constrained drones,"Several distributed algorithms are presented for the exploration of unknown
indoor regions by a swarm of flying, energy constrained agents. The agents,
which are identical, autonomous, anonymous and oblivious, uniformly cover the
region and thus explore it using predefined action rules based on locally
sensed information and the energy level of the agents. While flying drones have
many advantages in search and rescue scenarios, their main drawback is a high
power consumption during flight combined with limited, on-board energy.
Furthermore, in these scenarios agent size is severely limited and consequently
so are the total weight and capabilities of the agents. The region is modeled
as a connected sub-set of a regular grid composed of square cells that the
agents enter, over time, via entry points. Some of the agents may settle in
unoccupied cells as the exploration progresses. Settled agents conserve energy
and become virtual pheromones for the exploration and coverage process, beacons
that subsequently aid the remaining, and still exploring, mobile agents. The
termination of the coverage process is based on a backward propagating
information diffusion scheme. Various algorithmical alternatives are discussed
and upper bounds derived and compared to experimental results. Finally, an
optimal entry rate that minimizes the total energy consumption is derived for
the case of a linear regions."
0,0,Multi-Cluster Aggregative Games: A Linearly Convergent Nash Equilibrium Seeking Algorithm and its Applications in Energy Management,"We propose a type of non-cooperative game, termed multi-cluster aggregative
game, which is composed of clusters as players, where each cluster consists of
collaborative agents with cost functions depending on their own decisions and
the aggregate quantity of each participant cluster to modeling large-scale and
hierarchical multi-agent systems. This novel game model is motivated by
decision-making problems in competitive-cooperative network systems with
large-scale nodes, such as the Energy Internet. To address challenges arising
in seeking Nash equilibrium for such network systems, we develop an algorithm
with a hierarchical communication topology which is a hybrid with distributed
and semi-decentralized protocols. The upper level consists of cluster
coordinators estimating the aggregate quantities with local communications,
while the lower level is cluster subnets composed of its coordinator and agents
aiming to track the gradient of the corresponding cluster. In particular, the
clusters exchange the aggregate quantities instead of their decisions to
relieve the burden of communication. Under strongly monotone and mildly
Lipschitz continuous assumptions, we rigorously prove that the algorithm
linearly converges to a Nash equilibrium with a fixed step size.We present the
applications in the context of the Energy Internet. Furthermore, the numerical
results verify the effectiveness of the algorithm."
0,0,Task-Oriented Communication Design at Scale,"With countless promising applications in various domains such as IoT and
industry 4.0, task-oriented communication design (TOCD) is getting accelerated
attention from the research community. This paper presents a novel approach for
designing scalable task-oriented quantization and communications in cooperative
multi-agent systems (MAS). The proposed approach utilizes the TOCD framework
and the value of information (VoI) concept to enable efficient communication of
quantized observations among agents while maximizing the average return
performance of the MAS, a parameter that quantifies the MAS's task
effectiveness. The computational complexity of learning the VoI, however, grows
exponentially with the number of agents. Thus, we propose a three-step
framework: i) learning the VoI (using reinforcement learning (RL)) for a
two-agent system, ii) designing the quantization policy for an $N$-agent MAS
using the learned VoI for a range of bit-budgets and, (iii) learning the
agents' control policies using RL while following the designed quantization
policies in the earlier step. We observe that one can reduce the computational
cost of obtaining the value of information by exploiting insights gained from
studying a similar two-agent system - instead of the original $N$-agent system.
We then quantize agents' observations such that their more valuable
observations are communicated more precisely. Our analytical results show the
applicability of the proposed framework under a wide range of problems.
Numerical results show striking improvements in reducing the computational
complexity of obtaining VoI needed for the TOCD in a MAS problem without
compromising the average return performance of the MAS."
0,0,More Like Real World Game Challenge for Partially Observable Multi-Agent Cooperation,"Some standardized environments have been designed for partially observable
multi-agent cooperation, but we find most current environments are synchronous,
whereas real-world agents often have their own action spaces leading to
asynchrony. Furthermore, fixed agents number limits the scalability of action
space, whereas in reality agents number can change resulting in a flexible
action space. In addition, current environments are balanced, which is not
always the case in the real world where there may be an ability gap between
different parties leading to asymmetry. Finally, current environments tend to
have less stochasticity with simple state transitions, whereas real-world
environments can be highly stochastic and result in extremely risky. To address
this gap, we propose WarGame Challenge (WGC) inspired by the Wargame. WGC is a
lightweight, flexible, and easy-to-use environment with a clear framework that
can be easily configured by users. Along with the benchmark, we provide MARL
baseline algorithms such as QMIX and a toolkit to help algorithms complete
performance tests on WGC. Finally, we present baseline experiment results,
which demonstrate the challenges of WGC. We think WGC enrichs the partially
observable multi-agent cooperation domain and introduces more challenges that
better reflect the real-world characteristics. Code is release in
http://turingai.ia.ac.cn/data\_center/show/10."
0,0,Spiral Sweeping Search for Smart Evaders,"Consider a given planar circular region, in which there is an unknown number
of smart mobile evaders. We wish to detect evaders using a line formation of
sweeping agents whose total sensing length is predetermined. We propose
procedures for designing spiral sweeping protocols that ensure the successful
completion of the task, thus deriving conditions on the sweeping speed of the
linear formation and its path. Successful completion of the task implies that
evaders with a given limit on their speed cannot escape the sweeping agents. A
simpler task for the sweeping formation is the confinement of evaders to a
desired region, such as their original domain. The feasibility of completing
these tasks depends on geometric and dynamic constraints that impose a lower
bound on the speed that the sweeping agents must have. This critical speed is
derived to ensure the satisfaction of the confinement task. Increasing the
speed above the lower bound enables the sweepers to complete the search task as
well. We develop two spiral line formation search processes for smart evaders,
that address current limitations in search against smart evaders. Additionally,
we present a quantitative and qualitative comparison analysis between the total
search time of circular line formation sweep processes and spiral line
formation processes. We evaluate the different strategies by using two metrics,
total search time and the minimal critical speed required for a successful
search."
0,0,Federated TD Learning over Finite-Rate Erasure Channels: Linear Speedup under Markovian Sampling,"Federated learning (FL) has recently gained much attention due to its
effectiveness in speeding up supervised learning tasks under communication and
privacy constraints. However, whether similar speedups can be established for
reinforcement learning remains much less understood theoretically. Towards this
direction, we study a federated policy evaluation problem where agents
communicate via a central aggregator to expedite the evaluation of a common
policy. To capture typical communication constraints in FL, we consider finite
capacity up-link channels that can drop packets based on a Bernoulli erasure
model. Given this setting, we propose and analyze QFedTD - a quantized
federated temporal difference learning algorithm with linear function
approximation. Our main technical contribution is to provide a finite-sample
analysis of QFedTD that (i) highlights the effect of quantization and erasures
on the convergence rate; and (ii) establishes a linear speedup w.r.t. the
number of agents under Markovian sampling. Notably, while different
quantization mechanisms and packet drop models have been extensively studied in
the federated learning, distributed optimization, and networked control systems
literature, our work is the first to provide a non-asymptotic analysis of their
effects in multi-agent and federated reinforcement learning."
0,0,Network-GIANT: Fully distributed Newton-type optimization via harmonic Hessian consensus,"This paper considers the problem of distributed multi-agent learning, where
the global aim is to minimize a sum of local objective (empirical loss)
functions through local optimization and information exchange between
neighbouring nodes. We introduce a Newton-type fully distributed optimization
algorithm, Network-GIANT, which is based on GIANT, a Federated learning
algorithm that relies on a centralized parameter server. The Network-GIANT
algorithm is designed via a combination of gradient-tracking and a Newton-type
iterative algorithm at each node with consensus based averaging of local
gradient and Newton updates. We prove that our algorithm guarantees semi-global
and exponential convergence to the exact solution over the network assuming
strongly convex and smooth loss functions. We provide empirical evidence of the
superior convergence performance of Network-GIANT over other state-of-art
distributed learning algorithms such as Network-DANE and Newton-Raphson
Consensus."
0,0,Stackelberg Decision Transformer for Asynchronous Action Coordination in Multi-Agent Systems,"Asynchronous action coordination presents a pervasive challenge in
Multi-Agent Systems (MAS), which can be represented as a Stackelberg game (SG).
However, the scalability of existing Multi-Agent Reinforcement Learning (MARL)
methods based on SG is severely constrained by network structures or
environmental limitations. To address this issue, we propose the Stackelberg
Decision Transformer (STEER), a heuristic approach that resolves the
difficulties of hierarchical coordination among agents. STEER efficiently
manages decision-making processes in both spatial and temporal contexts by
incorporating the hierarchical decision structure of SG, the modeling
capability of autoregressive sequence models, and the exploratory learning
methodology of MARL. Our research contributes to the development of an
effective and adaptable asynchronous action coordination method that can be
widely applied to various task types and environmental configurations in MAS.
Experimental results demonstrate that our method can converge to Stackelberg
equilibrium solutions and outperforms other existing methods in complex
scenarios."
0,0,On a Voter Model with Context-Dependent Opinion Adoption,"Opinion diffusion is a crucial phenomenon in social networks, often
underlying the way in which a collective of agents develops a consensus on
relevant decisions. The voter model is a well-known theoretical model to study
opinion spreading in social networks and structured populations. Its simplest
version assumes that an updating agent will adopt the opinion of a neighboring
agent chosen at random. The model allows us to study, for example, the
probability that a certain opinion will fixate into a consensus opinion, as
well as the expected time it takes for a consensus opinion to emerge.
  Standard voter models are oblivious to the opinions held by the agents
involved in the opinion adoption process. We propose and study a
context-dependent opinion spreading process on an arbitrary social graph, in
which the probability that an agent abandons opinion $a$ in favor of opinion
$b$ depends on both $a$ and $b$. We discuss the relations of the model with
existing voter models and then derive theoretical results for both the fixation
probability and the expected consensus time for two opinions, for both the
synchronous and the asynchronous update models."
0,0,Multi-Value Alignment in Normative Multi-Agent System: Evolutionary Optimisation Approach,"Value-alignment in normative multi-agent systems is used to promote a certain
value and to ensure the consistent behavior of agents in autonomous intelligent
systems with human values. However, the current literature is limited to
incorporation of effective norms for single value alignment with no
consideration of agents' heterogeneity and the requirement of simultaneous
promotion and alignment of multiple values. This research proposes a
multi-value promotion model that uses multi-objective evolutionary algorithms
to produce the optimum parametric set of norms that is aligned with multiple
simultaneous values of heterogeneous agents and the system. To understand
various aspects of this complex problem, several evolutionary algorithms were
used to find a set of optimised norm parameters considering two toy tax
scenarios with two and five values are considered. The results are analysed
from different perspectives to show the impact of a selected evolutionary
algorithm on the solution, and the importance of understanding the relation
between values when prioritising them."
1,0,Building resilient organizations: The roles of top-down vs. bottom-up organizing,"Organizations face numerous challenges posed by unexpected events such as
energy price hikes, pandemic disruptions, terrorist attacks, and natural
disasters, and the factors that contribute to organizational success in dealing
with such disruptions often remain unclear. This paper analyzes the roles of
top-down and bottom-up organizational structures in promoting organizational
resilience. To do so, an agent-based model of stylized organizations is
introduced that features learning, adaptation, different modes of organizing,
and environmental disruptions. The results indicate that bottom-up designed
organizations tend to have a higher ability to absorb the effects of
environmental disruptions, and situations are identified in which either
top-down or bottom-up designed organizations have an advantage in recovering
from shocks."
0,0,Boosting Value Decomposition via Unit-Wise Attentive State Representation for Cooperative Multi-Agent Reinforcement Learning,"In cooperative multi-agent reinforcement learning (MARL), the environmental
stochasticity and uncertainties will increase exponentially when the number of
agents increases, which puts hard pressure on how to come up with a compact
latent representation from partial observation for boosting value
decomposition. To tackle these issues, we propose a simple yet powerful method
that alleviates partial observability and efficiently promotes coordination by
introducing the UNit-wise attentive State Representation (UNSR). In UNSR, each
agent learns a compact and disentangled unit-wise state representation
outputted from transformer blocks, and produces its local action-value
function. The proposed UNSR is used to boost the value decomposition with a
multi-head attention mechanism for producing efficient credit assignment in the
mixing network, providing an efficient reasoning path between the individual
value function and joint value function. Experimental results demonstrate that
our method achieves superior performance and data efficiency compared to solid
baselines on the StarCraft II micromanagement challenge. Additional ablation
experiments also help identify the key factors contributing to the performance
of UNSR."
1,0,Conflict Mitigation Framework and Conflict Detection in O-RAN Near-RT RIC,"The steady evolution of the Open RAN concept sheds light on xApps and their
potential use cases in O-RANcompliant deployments. There are several areas
where xApps can be used that are being widely investigated, but the issue of
mitigating conflicts between xApp decisions requires further in-depth
investigation. This article defines a conflict mitigation framework (CMF) built
into the existing O-RAN architecture; it enables the Conflict Mitigation
component in O-RAN's Near- Real-Time RAN Intelligent Controller (Near-RT RIC)
to detect and resolve all conflict types defined in the O-RAN Alliance's
technical specifications. Methods for detecting each type of conflict are
defined, including message flows between Near-RT RIC components. The
suitability of the proposed CMF is proven with a simulation of an O-RAN
network. Results of the simulation show that enabling the CMF allows balancing
the network control capabilities of conflicting xApps to significantly improve
network performance, with a small negative impact on its reliability. It is
concluded that defining a unified CMF in Near-RT RIC is the first step towards
providing a standardized method of conflict detection and resolution in O-RAN
environments."
1,0,Stability and Convergence of Distributed Stochastic Approximations with large Unbounded Stochastic Information Delays,"We generalize the Borkar-Meyn stability Theorem (BMT) to distributed
stochastic approximations (SAs) with information delays that possess an
arbitrary moment bound. To model the delays, we introduce Age of Information
Processes (AoIPs): stochastic processes on the non-negative integers with a
unit growth property. We show that AoIPs with an arbitrary moment bound cannot
exceed any fraction of time infinitely often. In combination with a suitably
chosen stepsize, this property turns out to be sufficient for the stability of
distributed SAs. Compared to the BMT, our analysis requires crucial
modifications and a new line of argument to handle the SA errors caused by AoI.
In our analysis, we show that these SA errors satisfy a recursive inequality.
To evaluate this recursion, we propose a new Gronwall-type inequality for
time-varying lower limits of summations. As applications to our distributed
BMT, we discuss distributed gradient-based optimization and a new approach to
analyzing SAs with momentum."
1,0,Path-Based Sensors: Will the Knowledge of Correlation in Random Variables Accelerate Information Gathering?,"Effective communication is crucial for deploying robots in mission-specific
tasks, but inadequate or unreliable communication can greatly reduce mission
efficacy, for example in search and rescue missions where communication-denied
conditions may occur. In such missions, robots are deployed to locate targets,
such as human survivors, but they might get trapped at hazardous locations,
such as in a trapping pit or by debris. Thus, the information the robot
collected is lost owing to the lack of communication. In our prior work, we
developed the notion of a path-based sensor. A path-based sensor detects
whether or not an event has occurred along a particular path, but it does not
provide the exact location of the event. Such path-based sensor observations
are well-suited to communication-denied environments, and various studies have
explored methods to improve information gathering in such settings. In some
missions it is typical for target elements to be in close proximity to
hazardous factors that hinder the information-gathering process. In this study,
we examine a similar scenario and conduct experiments to determine if
additional knowledge about the correlation between hazards and targets improves
the efficiency of information gathering. To incorporate this knowledge, we
utilize a Bayesian network representation of domain knowledge and develop an
algorithm based on this representation. Our empirical investigation reveals
that such additional information on correlation is beneficial only in
environments with moderate hazard lethality, suggesting that while knowledge of
correlation helps, further research and development is necessary for optimal
outcomes."
0,0,Cooperative Multi-Agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation,"We study multi-agent reinforcement learning in the setting of episodic Markov
decision processes, where multiple agents cooperate via communication through a
central server. We propose a provably efficient algorithm based on value
iteration that enable asynchronous communication while ensuring the advantage
of cooperation with low communication overhead. With linear function
approximation, we prove that our algorithm enjoys an
$\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with
$\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature
dimension, $H$ is the horizon length, $M$ is the total number of agents, and
$K$ is the total number of episodes. We also provide a lower bound showing that
a minimal $\Omega(dM)$ communication complexity is required to improve the
performance through collaboration."
0,0,"Learning Optimal ""Pigovian Tax"" in Sequential Social Dilemmas","In multi-agent reinforcement learning, each agent acts to maximize its
individual accumulated rewards. Nevertheless, individual accumulated rewards
could not fully reflect how others perceive them, resulting in selfish
behaviors that undermine global performance. The externality theory, defined as
``the activities of one economic actor affect the activities of another in ways
that are not reflected in market transactions,'' is applicable to analyze the
social dilemmas in MARL. One of its most profound non-market solutions,
``Pigovian Tax'', which internalizes externalities by taxing those who create
negative externalities and subsidizing those who create positive externalities,
could aid in developing a mechanism to resolve MARL's social dilemmas. The
purpose of this paper is to apply externality theory to analyze social dilemmas
in MARL. To internalize the externalities in MARL, the \textbf{L}earning
\textbf{O}ptimal \textbf{P}igovian \textbf{T}ax method (LOPT), is proposed,
where an additional agent is introduced to learn the tax/allowance allocation
policy so as to approximate the optimal ``Pigovian Tax'' which accurately
reflects the externalities for all agents. Furthermore, a reward shaping
mechanism based on the approximated optimal ``Pigovian Tax'' is applied to
reduce the social cost of each agent and tries to alleviate the social
dilemmas. Compared with existing state-of-the-art methods, the proposed LOPT
leads to higher collective social welfare in both the Escape Room and the
Cleanup environments, which shows the superiority of our method in solving
social dilemmas."
0,0,Fast Teammate Adaptation in the Presence of Sudden Policy Change,"In cooperative multi-agent reinforcement learning (MARL), where an agent
coordinates with teammate(s) for a shared goal, it may sustain non-stationary
caused by the policy change of teammates. Prior works mainly concentrate on the
policy change during the training phase or teammates altering cross episodes,
ignoring the fact that teammates may suffer from policy change suddenly within
an episode, which might lead to miscoordination and poor performance as a
result. We formulate the problem as an open Dec-POMDP, where we control some
agents to coordinate with uncontrolled teammates, whose policies could be
changed within one episode. Then we develop a new framework, fast teammates
adaptation (Fastap), to address the problem. Concretely, we first train
versatile teammates' policies and assign them to different clusters via the
Chinese Restaurant Process (CRP). Then, we train the controlled agent(s) to
coordinate with the sampled uncontrolled teammates by capturing their
identifications as context for fast adaptation. Finally, each agent applies its
local information to anticipate the teammates' context for decision-making
accordingly. This process proceeds alternately, leading to a robust policy that
can adapt to any teammates during the decentralized execution phase. We show in
multiple multi-agent benchmarks that Fastap can achieve superior performance
than multiple baselines in stationary and non-stationary scenarios."
0,0,Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers,"Cooperative multi-agent reinforcement learning (CMARL) has shown to be
promising for many real-world applications. Previous works mainly focus on
improving coordination ability via solving MARL-specific challenges (e.g.,
non-stationarity, credit assignment, scalability), but ignore the policy
perturbation issue when testing in a different environment. This issue hasn't
been considered in problem formulation or efficient algorithm design. To
address this issue, we firstly model the problem as a limited policy adversary
Dec-POMDP (LPA-Dec-POMDP), where some coordinators from a team might
accidentally and unpredictably encounter a limited number of malicious action
attacks, but the regular coordinators still strive for the intended goal. Then,
we propose Robust Multi-Agent Coordination via Evolutionary Generation of
Auxiliary Adversarial Attackers (ROMANCE), which enables the trained policy to
encounter diversified and strong auxiliary adversarial attacks during training,
thus achieving high robustness under various policy perturbations. Concretely,
to avoid the ego-system overfitting to a specific attacker, we maintain a set
of attackers, which is optimized to guarantee the attackers high attacking
quality and behavior diversity. The goal of quality is to minimize the
ego-system coordination effect, and a novel diversity regularizer based on
sparse action is applied to diversify the behaviors among attackers. The
ego-system is then paired with a population of attackers selected from the
maintained attacker set, and alternately trained against the constantly
evolving attackers. Extensive experiments on multiple scenarios from SMAC
indicate our ROMANCE provides comparable or better robustness and
generalization ability than other baselines."
0,0,Mixture of personality improved Spiking actor network for efficient multi-agent cooperation,"Adaptive human-agent and agent-agent cooperation are becoming more and more
critical in the research area of multi-agent reinforcement learning (MARL),
where remarked progress has been made with the help of deep neural networks.
However, many established algorithms can only perform well during the learning
paradigm but exhibit poor generalization during cooperation with other unseen
partners. The personality theory in cognitive psychology describes that humans
can well handle the above cooperation challenge by predicting others'
personalities first and then their complex actions. Inspired by this two-step
psychology theory, we propose a biologically plausible mixture of personality
(MoP) improved spiking actor network (SAN), whereby a determinantal point
process is used to simulate the complex formation and integration of different
types of personality in MoP, and dynamic and spiking neurons are incorporated
into the SAN for the efficient reinforcement learning. The benchmark Overcooked
task, containing a strong requirement for cooperative cooking, is selected to
test the proposed MoP-SAN. The experimental results show that the MoP-SAN can
achieve both high performances during not only the learning paradigm but also
the generalization test (i.e., cooperation with other unseen agents) paradigm
where most counterpart deep actor networks failed. Necessary ablation
experiments and visualization analyses were conducted to explain why MoP and
SAN are effective in multi-agent reinforcement learning scenarios while DNN
performs poorly in the generalization test."
1,0,Persistent synchronization of heterogeneous networks with time-dependent linear diffusive coupling,"We study synchronization for linearly coupled temporal networks of
heterogeneous time-dependent nonlinear agents via the convergence of attracting
trajectories of each node. The results are obtained by constructing and
studying the stability of a suitable linear nonautonomous problem bounding the
evolution of the synchronization errors. Both, the case of the entire network
and only a cluster, are addressed and the persistence of the obtained
synchronization against perturbation is also discussed. Furthermore, a
sufficient condition for the existence of attracting trajectories of each node
is given. In all cases, the considered dependence on time requires only local
integrability, which is a very mild regularity assumption. Moreover, our
results mainly depend on the network structure and its properties, and achieve
synchronization up to a constant in finite time. Hence they are quite suitable
for applications. The applicability of the results is showcased via several
examples: coupled van-der-Pol/FitzHugh-Nagumo oscillators, weighted/signed
opinion dynamics, and coupled Lorenz systems."
0,0,An Algorithm For Adversary Aware Decentralized Networked MARL,"Decentralized multi-agent reinforcement learning (MARL) algorithms have
become popular in the literature since it allows heterogeneous agents to have
their own reward functions as opposed to canonical multi-agent Markov Decision
Process (MDP) settings which assume common reward functions over all agents. In
this work, we follow the existing work on collaborative MARL where agents in a
connected time varying network can exchange information among each other in
order to reach a consensus. We introduce vulnerabilities in the consensus
updates of existing MARL algorithms where agents can deviate from their usual
consensus update, who we term as adversarial agents. We then proceed to provide
an algorithm that allows non-adversarial agents to reach a consensus in the
presence of adversaries under a constrained setting."
0,0,SMAClite: A Lightweight Environment for Multi-Agent Reinforcement Learning,"There is a lack of standard benchmarks for Multi-Agent Reinforcement Learning
(MARL) algorithms. The Starcraft Multi-Agent Challenge (SMAC) has been widely
used in MARL research, but is built on top of a heavy, closed-source computer
game, StarCraft II. Thus, SMAC is computationally expensive and requires
knowledge and the use of proprietary tools specific to the game for any
meaningful alteration or contribution to the environment. We introduce SMAClite
-- a challenge based on SMAC that is both decoupled from Starcraft II and
open-source, along with a framework which makes it possible to create new
content for SMAClite without any special knowledge. We conduct experiments to
show that SMAClite is equivalent to SMAC, by training MARL algorithms on
SMAClite and reproducing SMAC results. We then show that SMAClite outperforms
SMAC in both runtime speed and memory."
0,0,Social Value Orientation and Integral Emotions in Multi-Agent Systems,"Human social behavior is influenced by individual differences in social
preferences. Social value orientation (SVO) is a measurable personality trait
which indicates the relative importance an individual places on their own and
on others' welfare when making decisions. SVO and other individual difference
variables are strong predictors of human behavior and social outcomes. However,
there are transient changes human behavior associated with emotions that are
not captured by individual differences alone. Integral emotions, the emotions
which arise in direct response to a decision-making scenario, have been linked
to temporary shifts in decision-making preferences.
  In this work, we investigated the effects of moderating social preferences
with integral emotions in multi-agent societies. We developed Svoie, a method
for designing agents which make decisions based on established SVO policies, as
well as alternative integral emotion policies in response to task outcomes. We
conducted simulation experiments in a resource-sharing task environment, and
compared societies of Svoie agents with societies of agents with fixed SVO
policies. We find that societies of agents which adapt their behavior through
integral emotions achieved similar collective welfare to societies of agents
with fixed SVO policies, but with significantly reduced inequality between the
welfare of agents with different SVO traits. We observed that by allowing
agents to change their policy in response to task outcomes, agents can moderate
their behavior to achieve greater social equality. \end{abstract}"
1,0,Learning Personalized Page Content Ranking Using Customer Representation,"On E-commerce stores (Amazon, eBay etc.) there are rich recommendation
content to help shoppers shopping more efficiently. However given numerous
products, it's crucial to select most relevant content to reduce the burden of
information overload. We introduced a content ranking service powered by a
linear causal bandit algorithm to rank and select content for each shopper
under each context. The algorithm mainly leverages aggregated customer behavior
features, and ignores single shopper level past activities. We study the
problem of inferring shoppers interest from historical activities. We propose a
deep learning based bandit algorithm that incorporates historical shopping
behavior, customer latent shopping goals, and the correlation between customers
and content categories. This model produces more personalized content ranking
measured by 12.08% nDCG lift. In the online A/B test setting, the model
improved 0.02% annualized commercial impact measured by our business metric,
validating its effectiveness."
0,0,Latent Interactive A2C for Improved RL in Open Many-Agent Systems,"There is a prevalence of multiagent reinforcement learning (MARL) methods
that engage in centralized training. But, these methods involve obtaining
various types of information from the other agents, which may not be feasible
in competitive or adversarial settings. A recent method, the interactive
advantage actor critic (IA2C), engages in decentralized training coupled with
decentralized execution, aiming to predict the other agents' actions from
possibly noisy observations. In this paper, we present the latent IA2C that
utilizes an encoder-decoder architecture to learn a latent representation of
the hidden state and other agents' actions. Our experiments in two domains --
each populated by many agents -- reveal that the latent IA2C significantly
improves sample efficiency by reducing variance and converging faster.
Additionally, we introduce open versions of these domains where the agent
population may change over time, and evaluate on these instances as well."
0,0,Local Optimization Achieves Global Optimality in Multi-Agent Reinforcement Learning,"Policy optimization methods with function approximation are widely used in
multi-agent reinforcement learning. However, it remains elusive how to design
such algorithms with statistical guarantees. Leveraging a multi-agent
performance difference lemma that characterizes the landscape of multi-agent
policy optimization, we find that the localized action value function serves as
an ideal descent direction for each local policy. Motivated by the observation,
we present a multi-agent PPO algorithm in which the local policy of each agent
is updated similarly to vanilla PPO. We prove that with standard regularity
conditions on the Markov game and problem-dependent quantities, our algorithm
converges to the globally optimal policy at a sublinear rate. We extend our
algorithm to the off-policy setting and introduce pessimism to policy
evaluation, which aligns with experiments. To our knowledge, this is the first
provably convergent multi-agent PPO algorithm in cooperative Markov games."
1,0,Global Update Tracking: A Decentralized Learning Algorithm for Heterogeneous Data,"Decentralized learning enables the training of deep learning models over
large distributed datasets generated at different locations, without the need
for a central server. However, in practical scenarios, the data distribution
across these devices can be significantly different, leading to a degradation
in model performance. In this paper, we focus on designing a decentralized
learning algorithm that is less susceptible to variations in data distribution
across devices. We propose Global Update Tracking (GUT), a novel tracking-based
method that aims to mitigate the impact of heterogeneous data in decentralized
learning without introducing any communication overhead. We demonstrate the
effectiveness of the proposed technique through an exhaustive set of
experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion
MNIST, and ImageNette), model architectures, and network topologies. Our
experiments show that the proposed method achieves state-of-the-art performance
for decentralized learning on heterogeneous data via a $1-6\%$ improvement in
test accuracy compared to other existing techniques."
0,0,Federated Learning in Wireless Networks via Over-the-Air Computations,"In a multi-agent system, agents can cooperatively learn a model from data by
exchanging their estimated model parameters, without the need to exchange the
locally available data used by the agents. This strategy, often called
federated learning, is mainly employed for two reasons: (i) improving
resource-efficiency by avoiding to share potentially large datasets and (ii)
guaranteeing privacy of local agents' data. Efficiency can be further increased
by adopting a beyond-5G communication strategy that goes under the name of
Over-the-Air Computation. This strategy exploits the interference property of
the wireless channel. Standard communication schemes prevent interference by
enabling transmissions of signals from different agents at distinct time or
frequency slots, which is not required with Over-the-Air Computation, thus
saving resources. In this case, the received signal is a weighted sum of
transmitted signals, with unknown weights (fading channel coefficients). State
of the art papers in the field aim at reconstructing those unknown
coefficients. In contrast, the approach presented here does not require
reconstructing channel coefficients by complex encoding-decoding schemes. This
improves both efficiency and privacy."
0,0,Optimal Scheduling of Agents in ADTrees: Specialised Algorithm and Declarative Models,"Expressing attack-defence trees in a multi-agent setting allows for studying
a new aspect of security scenarios, namely how the number of agents and their
task assignment impact the performance, e.g. attack time, of strategies
executed by opposing coalitions. Optimal scheduling of agents' actions, a
non-trivial problem, is thus vital. We discuss associated caveats and propose
an algorithm that synthesises such an assignment, targeting minimal attack time
and using the minimal number of agents for a given attack-defence tree. We also
investigate an alternative approach for the same problem using Rewriting Logic,
starting with a simple and elegant declarative model, whose correctness (in
terms of schedule's optimality) is self-evident. We then refine this
specification, inspired by the design of our specialised algorithm, to obtain
an efficient system that can be used as a playground to explore various aspects
of attack-defence trees. We compare the two approaches on different benchmarks."
1,0,Deadlock-Free Collision Avoidance for Nonholonomic Robots,"We present a method for deadlock-free and collision-free navigation in a
multi-robot system with nonholonomic robots. The problem is solved by quadratic
programming and is applicable to most wheeled mobile robots with linear
kinematic constraints. We introduce masked velocity and Masked Cooperative
Collision Avoidance (MCCA) algorithm to encourage a fully decentralized
deadlock avoidance behavior. To verify the method, we provide a detailed
implementation and introduce heading oscillation avoidance for
differential-drive robots. To the best of our knowledge, it is the first method
to give very promising and stable results for deadlock avoidance even in
situations with a large number of robots and narrow passages."
0,0,Multi-Objective Task Assignment and Multiagent Planning with Hybrid GPU-CPU Acceleration,"Allocation and planning with a collection of tasks and a group of agents is
an important problem in multiagent systems. One commonly faced bottleneck is
scalability, as in general the multiagent model increases exponentially in size
with the number of agents. We consider the combination of random task
assignment and multiagent planning under multiple-objective constraints, and
show that this problem can be decentralised to individual agent-task models. We
present an algorithm of point-oriented Pareto computation, which checks whether
a point corresponding to given cost and probability thresholds for our formal
problem is feasible or not. If the given point is infeasible, our algorithm
finds a Pareto-optimal point which is closest to the given point. We provide
the first multi-objective model checking framework that simultaneously uses GPU
and multi-core acceleration. Our framework manages CPU and GPU devices as a
load balancing problem for parallel computation. Our experiments demonstrate
that parallelisation achieves significant run time speed-up over sequential
computation."
0,0,We Need to Talk: Identifying and Overcoming Communication-Critical Scenarios for Self-Driving,"In this work, we consider the task of collision-free trajectory planning for
connected self-driving vehicles. We specifically consider
communication-critical situations--situations where single-agent systems have
blindspots that require multi-agent collaboration. To identify such situations,
we propose a method which (1) simulates multi-agent perspectives from real
self-driving datasets, (2) finds scenarios that are challenging for isolated
agents, and (3) augments scenarios with adversarial obstructions. To overcome
these challenges, we propose to extend costmap-based trajectory evaluation to a
distributed multi-agent setting. We demonstrate that our bandwidth-efficient,
uncertainty-aware method reduces collision rates by up to 62.5% compared to
single agent baselines."
1,0,a general model of vehicle routing guidance systems based on distributive learning scheme,"Dynamic traffic assignment and vehicle route guidance have been important
problems in ITS for some time. This paper proposes a new model for VRGS, which
takes into consideration of the information propagation, user selection and
information reaction. Parameter p is then defined as the updating weight for
computing cost of traffic based on a distributive learning scheme. p is
calculated through a function which denotes information propagation over time
and space and the function needs further optimization. Comparison to static
traffic assignment, DTA and feasible strategies are given, and future work is
also stated."
1,0,A Game of Pawns,"We introduce and study pawn games, a class of two-player zero-sum turn-based
graph games. A turn-based graph game proceeds by placing a token on an initial
vertex, and whoever controls the vertex on which the token is located, chooses
its next location. This leads to a path in the graph, which determines the
winner. Traditionally, the control of vertices is predetermined and fixed. The
novelty of pawn games is that control of vertices changes dynamically
throughout the game as follows. Each vertex of a pawn game is owned by a pawn.
In each turn, the pawns are partitioned between the two players, and the player
who controls the pawn that owns the vertex on which the token is located,
chooses the next location of the token. Control of pawns changes dynamically
throughout the game according to a fixed mechanism. Specifically, we define
several grabbing-based mechanisms in which control of at most one pawn
transfers at the end of each turn. We study the complexity of solving pawn
games, where we focus on reachability objectives and parameterize the problem
by the mechanism that is being used and by restrictions on pawn ownership of
vertices. On the positive side, even though pawn games are
exponentially-succinct turn-based games, we identify several natural classes
that can be solved in PTIME. On the negative side, we identify several
EXPTIME-complete classes, where our hardness proofs are based on a new class of
games called lock & Key games, which may be of independent interest."
1,0,Decentralised Semi-supervised Onboard Learning for Scene Classification in Low-Earth Orbit,"Onboard machine learning on the latest satellite hardware offers the
potential for significant savings in communication and operational costs. We
showcase the training of a machine learning model on a satellite constellation
for scene classification using semi-supervised learning while accounting for
operational constraints such as temperature and limited power budgets based on
satellite processor benchmarks of the neural network. We evaluate mission
scenarios employing both decentralised and federated learning approaches. All
scenarios achieve convergence to high accuracy (around 91% on EuroSAT RGB
dataset) within a one-day mission timeframe."
0,0,Improving LaCAM for Scalable Eventually Optimal Multi-Agent Pathfinding,"This study extends the recently-developed LaCAM algorithm for multi-agent
pathfinding (MAPF). LaCAM is a sub-optimal search-based algorithm that uses
lazy successor generation to dramatically reduce the planning effort. We
present two enhancements. First, we propose its anytime version, called LaCAM*,
which eventually converges to optima, provided that solution costs are
accumulated transition costs. Second, we improve the successor generation to
quickly obtain initial solutions. Exhaustive experiments demonstrate their
utility. For instance, LaCAM* sub-optimally solved 99% of the instances
retrieved from the MAPF benchmark, where the number of agents varied up to a
thousand, within ten seconds on a standard desktop PC, while ensuring eventual
convergence to optima; developing a new horizon of MAPF algorithms."
3,0,Flash: An Asynchronous Payment System with Good-Case Linear Communication Complexity,"While the original purpose of blockchains was to realize a payment system, it
has been shown that, in fact, such systems do not require consensus and can be
implemented deterministically in asynchronous networks. State-of-the-art
payment systems employ Reliable Broadcast to disseminate payments and prevent
double spending, which entails O(n^2) communication complexity per payment even
if Byzantine behavior is scarce or non-existent.
  Here we present Flash, the first payment system to achieve $O(n)$
communication complexity per payment in the good case and $O(n^2)$ complexity
in the worst-case, matching the lower bound. This is made possible by
sidestepping Reliable Broadcast and instead using the blocklace -- a DAG-like
partially-ordered generalization of the blockchain -- for the tasks of
recording transaction dependencies, block dissemination, and equivocation
exclusion, which in turn prevents doublespending.
  Flash has two variants: for high congestion when multiple blocks that contain
multiple payments are issued concurrently; and for low congestion when payments
are infrequent."
1,0,Cooperative Driving of Connected Autonomous Vehicles in Heterogeneous Mixed Traffic: A Game Theoretic Approach,"High-density, unsignalized intersection has always been a bottleneck of
efficiency and safety. The emergence of Connected Autonomous Vehicles (CAVs)
results in a mixed traffic condition, further increasing the complexity of the
transportation system. Against this background, this paper aims to study the
intricate and heterogeneous interaction of vehicles and conflict resolution at
the high-density, mixed, unsignalized intersection. Theoretical insights about
the interaction between CAVs and Human-driven Vehicles (HVs) and the
cooperation of CAVs are synthesized, based on which a novel cooperative
decision-making framework in heterogeneous mixed traffic is proposed.
Normalized Cooperative game is concatenated with Level-k game (NCL game) to
generate a system optimal solution. Then Lattice planner generates the optimal
and collision-free trajectories for CAVs. To reproduce HVs in mixed traffic,
interactions from naturalistic human driving data are extracted as prior
knowledge. Non-cooperative game and Inverse Reinforcement Learning (IRL) are
integrated to mimic the decision making of heterogeneous HVs. Finally, three
cases are conducted to verify the performance of the proposed algorithm,
including the comparative analysis with different methods, the case study under
different Rates of Penetration (ROP) and the interaction analysis with
heterogeneous HVs. It is found that the proposed cooperative decision-making
framework is beneficial to the driving conflict resolution and the traffic
efficiency improvement of the mixed unsignalized intersection. Besides, due to
the consideration of driving heterogeneity, better human-machine interaction
and cooperation can be realized in this paper."
1,0,Decentralized diffusion-based learning under non-parametric limited prior knowledge,"We study the problem of diffusion-based network learning of a nonlinear
phenomenon, $m$, from local agents' measurements collected in a noisy
environment. For a decentralized network and information spreading merely
between directly neighboring nodes, we propose a non-parametric learning
algorithm, that avoids raw data exchange and requires only mild \textit{a
priori} knowledge about $m$. Non-asymptotic estimation error bounds are derived
for the proposed method. Its potential applications are illustrated through
simulation experiments."
0,0,Stackelberg Games for Learning Emergent Behaviors During Competitive Autocurricula,"Autocurricular training is an important sub-area of multi-agent reinforcement
learning~(MARL) that allows multiple agents to learn emergent skills in an
unsupervised co-evolving scheme. The robotics community has experimented
autocurricular training with physically grounded problems, such as robust
control and interactive manipulation tasks. However, the asymmetric nature of
these tasks makes the generation of sophisticated policies challenging. Indeed,
the asymmetry in the environment may implicitly or explicitly provide an
advantage to a subset of agents which could, in turn, lead to a low-quality
equilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg
Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a
two-player MARL problem as a Stackelberg game with one player as the `leader'
and the other as the `follower' in a hierarchical interaction structure wherein
the leader has an advantage. We first demonstrate that the leader's advantage
from ST-MADDPG can be used to alleviate the inherent asymmetry in the
environment. By exploiting the leader's advantage, ST-MADDPG improves the
quality of a co-evolution process and results in more sophisticated and complex
strategies that work well even against an unseen strong opponent."
1,0,A computational framework of human values for ethical AI,"In the diverse array of work investigating the nature of human values from
psychology, philosophy and social sciences, there is a clear consensus that
values guide behaviour. More recently, a recognition that values provide a
means to engineer ethical AI has emerged. Indeed, Stuart Russell proposed
shifting AI's focus away from simply ``intelligence'' towards intelligence
``provably aligned with human values''. This challenge -- the value alignment
problem -- with others including an AI's learning of human values, aggregating
individual values to groups, and designing computational mechanisms to reason
over values, has energised a sustained research effort. Despite this, no
formal, computational definition of values has yet been proposed. We address
this through a formal conceptual framework rooted in the social sciences, that
provides a foundation for the systematic, integrated and interdisciplinary
investigation into how human values can support designing ethical AI."
1,0,Human Values in Multiagent Systems,"One of the major challenges we face with ethical AI today is developing
computational systems whose reasoning and behaviour are provably aligned with
human values. Human values, however, are notorious for being ambiguous,
contradictory and ever-changing. In order to bridge this gap, and get us closer
to the situation where we can formally reason about implementing values into
AI, this paper presents a formal representation of values, grounded in the
social sciences. We use this formal representation to articulate the key
challenges for achieving value-aligned behaviour in multiagent systems (MAS)
and a research roadmap for addressing them."
0,0,System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning,"Evolutionary science provides evidence that diversity confers resilience.
Yet, traditional multi-agent reinforcement learning techniques commonly enforce
homogeneity to increase training sample efficiency. When a system of learning
agents is not constrained to homogeneous policies, individual agents may
develop diverse behaviors, resulting in emergent complementarity that benefits
the system. Despite this feat, there is a surprising lack of tools that measure
behavioral diversity in systems of learning agents. Such techniques would pave
the way towards understanding the impact of diversity in collective resilience
and performance. In this paper, we introduce System Neural Diversity (SND): a
measure of behavioral heterogeneity for multi-agent systems where agents have
stochastic policies. %over a continuous state space. We discuss and prove its
theoretical properties, and compare it with alternate, state-of-the-art
behavioral diversity metrics used in cross-disciplinary domains. Through
simulations of a variety of multi-agent tasks, we show how our metric
constitutes an important diagnostic tool to analyze latent properties of
behavioral heterogeneity. By comparing SND with task reward in static tasks,
where the problem does not change during training, we show that it is key to
understanding the effectiveness of heterogeneous vs homogeneous agents. In
dynamic tasks, where the problem is affected by repeated disturbances during
training, we show that heterogeneous agents are first able to learn specialized
roles that allow them to cope with the disturbance, and then retain these roles
when the disturbance is removed. SND allows a direct measurement of this latent
resilience, while other proxies such as task performance (reward) fail to."
0,0,Attention Based Feature Fusion For Multi-Agent Collaborative Perception,"In the domain of intelligent transportation systems (ITS), collaborative
perception has emerged as a promising approach to overcome the limitations of
individual perception by enabling multiple agents to exchange information, thus
enhancing their situational awareness. Collaborative perception overcomes the
limitations of individual sensors, allowing connected agents to perceive
environments beyond their line-of-sight and field of view. However, the
reliability of collaborative perception heavily depends on the data aggregation
strategy and communication bandwidth, which must overcome the challenges posed
by limited network resources. To improve the precision of object detection and
alleviate limited network resources, we propose an intermediate collaborative
perception solution in the form of a graph attention network (GAT). The
proposed approach develops an attention-based aggregation strategy to fuse
intermediate representations exchanged among multiple connected agents. This
approach adaptively highlights important regions in the intermediate feature
maps at both the channel and spatial levels, resulting in improved object
detection precision. We propose a feature fusion scheme using attention-based
architectures and evaluate the results quantitatively in comparison to other
state-of-the-art collaborative perception approaches. Our proposed approach is
validated using the V2XSim dataset. The results of this work demonstrate the
efficacy of the proposed approach for intermediate collaborative perception in
improving object detection average precision while reducing network resource
usage."
0,0,Human Machine Co-adaption Interface via Cooperation Markov Decision Process System,"This paper aims to develop a new human-machine interface to improve
rehabilitation performance from the perspective of both the user (patient) and
the machine (robot) by introducing the co-adaption techniques via model-based
reinforcement learning. Previous studies focus more on robot assistance, i.e.,
to improve the control strategy so as to fulfill the objective of
Assist-As-Needed. In this study, we treat the full process of robot-assisted
rehabilitation as a co-adaptive or mutual learning process and emphasize the
adaptation of the user to the machine. To this end, we proposed a Co-adaptive
MDPs (CaMDPs) model to quantify the learning rates based on cooperative
multi-agent reinforcement learning (MARL) in the high abstraction layer of the
systems. We proposed several approaches to cooperatively adjust the Policy
Improvement among the two agents in the framework of Policy Iteration. Based on
the proposed co-adaptive MDPs, the simulation study indicates the
non-stationary problem can be mitigated using various proposed Policy
Improvement approaches."
0,0,Decentralised Active Perception in Continuous Action Spaces for the Coordinated Escort Problem,"We consider the coordinated escort problem, where a decentralised team of
supporting robots implicitly assist the mission of higher-value principal
robots. The defining challenge is how to evaluate the effect of supporting
robots' actions on the principal robots' mission. To capture this effect, we
define two novel auxiliary reward functions for supporting robots called
satisfaction improvement and satisfaction entropy, which computes the
improvement in probability of mission success, or the uncertainty thereof.
Given these reward functions, we coordinate the entire team of principal and
supporting robots using decentralised cross entropy method (Dec-CEM), a new
extension of CEM to multi-agent systems based on the product distribution
approximation. In a simulated object avoidance scenario, our planning framework
demonstrates up to two-fold improvement in task satisfaction against
conventional decoupled information gathering.The significance of our results is
to introduce a new family of algorithmic problems that will enable important
new practical applications of heterogeneous multi-robot systems."
1,0,Achieving Realistic Cyclist Behavior in SUMO using the SimRa Dataset,"Increasing the modal share of bicycle traffic to reduce carbon emissions,
reduce urban car traffic, and to improve the health of citizens, requires a
shift away from car-centric city planning. For this, traffic planners often
rely on simulation tools such as SUMO which allow them to study the effects of
construction changes before implementing them. Similarly, studies of vulnerable
road users, here cyclists, also use such models to assess the performance of
communication-based road traffic safety systems. The cyclist model in SUMO,
however, is very imprecise as SUMO cyclists behave either like slow cars or
fast pedestrians, thus, casting doubt on simulation results for bicycle
traffic. In this paper, we analyze acceleration, deceleration, velocity, and
intersection left-turn behavior of cyclists in a large dataset of real world
cycle tracks. We use the results to improve the existing cyclist model in SUMO
and add three more detailed cyclist models and implement them in SUMO."
0,0,Heterogeneous Social Value Orientation Leads to Meaningful Diversity in Sequential Social Dilemmas,"In social psychology, Social Value Orientation (SVO) describes an
individual's propensity to allocate resources between themself and others. In
reinforcement learning, SVO has been instantiated as an intrinsic motivation
that remaps an agent's rewards based on particular target distributions of
group reward. Prior studies show that groups of agents endowed with
heterogeneous SVO learn diverse policies in settings that resemble the
incentive structure of Prisoner's dilemma. Our work extends this body of
results and demonstrates that (1) heterogeneous SVO leads to meaningfully
diverse policies across a range of incentive structures in sequential social
dilemmas, as measured by task-specific diversity metrics; and (2) learning a
best response to such policy diversity leads to better zero-shot generalization
in some situations. We show that these best-response agents learn policies that
are conditioned on their co-players, which we posit is the reason for improved
zero-shot generalization results."
0,0,On the Complexity of Multi-Agent Decision Making: From Learning in Games to Partial Monitoring,"A central problem in the theory of multi-agent reinforcement learning (MARL)
is to understand what structural conditions and algorithmic principles lead to
sample-efficient learning guarantees, and how these considerations change as we
move from few to many agents. We study this question in a general framework for
interactive decision making with multiple agents, encompassing Markov games
with function approximation and normal-form games with bandit feedback. We
focus on equilibrium computation, in which a centralized learning algorithm
aims to compute an equilibrium by controlling multiple agents that interact
with an unknown environment. Our main contributions are:
  - We provide upper and lower bounds on the optimal sample complexity for
multi-agent decision making based on a multi-agent generalization of the
Decision-Estimation Coefficient, a complexity measure introduced by Foster et
al. (2021) in the single-agent counterpart to our setting. Compared to the best
results for the single-agent setting, our bounds have additional gaps. We show
that no ""reasonable"" complexity measure can close these gaps, highlighting a
striking separation between single and multiple agents.
  - We show that characterizing the statistical complexity for multi-agent
decision making is equivalent to characterizing the statistical complexity of
single-agent decision making, but with hidden (unobserved) rewards, a framework
that subsumes variants of the partial monitoring problem. As a consequence, we
characterize the statistical complexity for hidden-reward interactive decision
making to the best extent possible.
  Building on this development, we provide several new structural results,
including 1) conditions under which the statistical complexity of multi-agent
decision making can be reduced to that of single-agent, and 2) conditions under
which the so-called curse of multiple agents can be avoided."
1,0,AI-Assisted Ethics? Considerations of AI Simulation for the Ethical Assessment and Design of Assistive Technologies,"Current ethical debates on the use of artificial intelligence (AI) in health
care treat AI as a product of technology in three ways: First, by assessing
risks and potential benefits of currently developed AI-enabled products with
ethical checklists; second, by proposing ex ante lists of ethical values seen
as relevant for the design and development of assisting technology, and third,
by promoting AI technology to use moral reasoning as part of the automation
process. Subsequently, we propose a fourth approach to AI, namely as a
methodological tool to assist ethical reflection. We provide a concept of an
AI-simulation informed by three separate elements: 1) stochastic human behavior
models based on behavioral data for simulating realistic settings, 2)
qualitative empirical data on value statements regarding internal policy, and
3) visualization components that aid in understanding the impact of changes in
these variables. The potential of this approach is to inform an
interdisciplinary field about anticipated ethical challenges or ethical
trade-offs in concrete settings and, hence, to spark a re-evaluation of design
and implementation plans. This may be particularly useful for applications that
deal with extremely complex values and behavior or with limitations on the
communication resources of affected persons (e.g., persons with dementia care
or for care of persons with cognitive impairment). Simulation does not replace
ethical reflection but does allow for detailed, context-sensitive analysis
during the design process and prior to implementation. Finally, we discuss the
inherently quantitative methods of analysis afforded by stochastic simulations
as well as the potential for ethical discussions and how simulations with AI
can improve traditional forms of thought experiments and future-oriented
technology assessment."
0,0,Model-free Motion Planning of Autonomous Agents for Complex Tasks in Partially Observable Environments,"Motion planning of autonomous agents in partially known environments with
incomplete information is a challenging problem, particularly for complex
tasks. This paper proposes a model-free reinforcement learning approach to
address this problem. We formulate motion planning as a probabilistic-labeled
partially observable Markov decision process (PL-POMDP) problem and use linear
temporal logic (LTL) to express the complex task. The LTL formula is then
converted to a limit-deterministic generalized B\""uchi automaton (LDGBA). The
problem is redefined as finding an optimal policy on the product of PL-POMDP
with LDGBA based on model-checking techniques to satisfy the complex task. We
implement deep Q learning with long short-term memory (LSTM) to process the
observation history and task recognition. Our contributions include the
proposed method, the utilization of LTL and LDGBA, and the LSTM-enhanced deep Q
learning. We demonstrate the applicability of the proposed method by conducting
simulations in various environments, including grid worlds, a virtual office,
and a multi-agent warehouse. The simulation results demonstrate that our
proposed method effectively addresses environment, action, and observation
uncertainties. This indicates its potential for real-world applications,
including the control of unmanned aerial vehicles (UAVs)."
1,0,Guaranteed Evader Detection in Multi-Agent Search Tasks using Pincer Trajectories,"Assume that inside an initial planar area there are smart mobile evaders
attempting to avoid detection by a team of sweeping searching agents. All
sweepers detect evaders with fan-shaped sensors, modeling the field of view of
real cameras. Detection of all evaders is guaranteed with cooperative sweeping
strategies, by setting requirements on sweepers' speed, and by carefully
designing their trajectories. Assume the smart evaders have an upper limit on
their speed which is a-priori known to the sweeping team. An easier task for
the team of sweepers is to confine evaders to the domain in which they are
initially located. The sweepers accomplish the confinement task if they move
sufficiently fast and detect evaders by applying an appropriate search
strategy. Any given search strategy results in a minimal sweeper's speed in
order to be able to detect all evaders. The minimal speed guarantees the
ability of the sweeping team to confine evaders to their original domain, and
if the sweepers move faster they are able to detect all evaders that are
present in the region. We present results on the total search time for a novel
pincer-movement based search protocol that utilizes complementary trajectories
along with adaptive sensor geometries for any even number of pursuers."
0,0,ICQ: A Quantization Scheme for Best-Arm Identification Over Bit-Constrained Channels,"We study the problem of best-arm identification in a distributed variant of
the multi-armed bandit setting, with a central learner and multiple agents.
Each agent is associated with an arm of the bandit, generating stochastic
rewards following an unknown distribution. Further, each agent can communicate
the observed rewards with the learner over a bit-constrained channel. We
propose a novel quantization scheme called Inflating Confidence for
Quantization (ICQ) that can be applied to existing confidence-bound based
learning algorithms such as Successive Elimination. We analyze the performance
of ICQ applied to Successive Elimination and show that the overall algorithm,
named ICQ-SE, has the order-optimal sample complexity as that of the
(unquantized) SE algorithm. Moreover, it requires only an exponentially sparse
frequency of communication between the learner and the agents, thus requiring
considerably fewer bits than existing quantization schemes to successfully
identify the best arm. We validate the performance improvement offered by ICQ
with other quantization methods through numerical experiments."
0,0,Learning to Seek: Multi-Agent Online Source Seeking Against Non-Stochastic Disturbances,"This paper proposes to leverage the emerging~learning techniques and devise a
multi-agent online source {seeking} algorithm under unknown environment. Of
particular significance in our problem setups are: i) the underlying
environment is not only unknown, but dynamically changing and also perturbed by
two types of non-stochastic disturbances; and ii) a group of agents is deployed
and expected to cooperatively seek as many sources as possible.
Correspondingly, a new technique of discounted Kalman filter is developed to
tackle with the non-stochastic disturbances, and a notion of confidence bound
in polytope nature is utilized~to aid the computation-efficient cooperation
among~multiple agents. With standard assumptions on the unknown environment as
well as the disturbances, our algorithm is shown to achieve sub-linear regrets
under the two~types of non-stochastic disturbances; both results are comparable
to the state-of-the-art. Numerical examples on a real-world pollution
monitoring application are provided to demonstrate the effectiveness of our
algorithm."
1,0,An Integrated System Dynamics and Discrete Event Supply Chain Simulation Framework for Supply Chain Resilience with Non-Stationary Pandemic Demand,"COVID-19 resulted in some of the largest supply chain disruptions in recent
history. To mitigate the impact of future disruptions, we propose an integrated
hybrid simulation framework to couple nonstationary demand signals from an
event like COVID-19 with a model of an end-to-end supply chain. We first create
a system dynamics susceptible-infected-recovered (SIR) model, augmenting a
classic epidemiological model to create a realistic portrayal of demand
patterns for oxygen concentrators (OC). Informed by this granular demand
signal, we then create a supply chain discrete event simulation model of OC
sourcing, manufacturing, and distribution to test production augmentation
policies to satisfy this increased demand. This model utilizes publicly
available data, engineering teardowns of OCs, and a supply chain illumination
to identify suppliers. Our findings indicate that this coupled approach can use
realistic demand during a disruptive event to enable rapid recommendations of
policies for increased supply chain resilience with controlled cost."
0,0,From Explicit Communication to Tacit Cooperation:A Novel Paradigm for Cooperative MARL,"Centralized training with decentralized execution (CTDE) is a widely-used
learning paradigm that has achieved significant success in complex tasks.
However, partial observability issues and the absence of effectively shared
signals between agents often limit its effectiveness in fostering cooperation.
While communication can address this challenge, it simultaneously reduces the
algorithm's practicality. Drawing inspiration from human team cooperative
learning, we propose a novel paradigm that facilitates a gradual shift from
explicit communication to tacit cooperation. In the initial training stage, we
promote cooperation by sharing relevant information among agents and
concurrently reconstructing this information using each agent's local
trajectory. We then combine the explicitly communicated information with the
reconstructed information to obtain mixed information. Throughout the training
process, we progressively reduce the proportion of explicitly communicated
information, facilitating a seamless transition to fully decentralized
execution without communication. Experimental results in various scenarios
demonstrate that the performance of our method without communication can
approaches or even surpasses that of QMIX and communication-based methods."
0,0,Double-Deck Multi-Agent Pickup and Delivery: Multi-Robot Rearrangement in Large-Scale Warehouses,"We introduce a new problem formulation, Double-Deck Multi-Agent Pickup and
Delivery (DD-MAPD), which models the multi-robot shelf rearrangement problem in
automated warehouses. DD-MAPD extends both Multi-Agent Pickup and Delivery
(MAPD) and Multi-Agent Path Finding (MAPF) by allowing agents to move beneath
shelves or lift and deliver a shelf to an arbitrary location, thereby changing
the warehouse layout. We show that solving DD-MAPD is NP-hard. To tackle
DD-MAPD, we propose MAPF-DECOMP, an algorithmic framework that decomposes a
DD-MAPD instance into a MAPF instance for coordinating shelf trajectories and a
subsequent MAPD instance with task dependencies for computing paths for agents.
We also present an optimization technique to improve the performance of
MAPF-DECOMP and demonstrate how to make MAPF-DECOMP complete for well-formed
DD-MAPD instances, a realistic subclass of DD-MAPD instances. Our experimental
results demonstrate the efficiency and effectiveness of MAPF-DECOMP, with the
ability to compute high-quality solutions for large-scale instances with over
one thousand shelves and hundreds of agents in just minutes of runtime."
0,0,Preference Inference from Demonstration in Multi-objective Multi-agent Decision Making,"It is challenging to quantify numerical preferences for different objectives
in a multi-objective decision-making problem. However, the demonstrations of a
user are often accessible. We propose an algorithm to infer linear preference
weights from either optimal or near-optimal demonstrations. The algorithm is
evaluated in three environments with two baseline methods. Empirical results
demonstrate significant improvements compared to the baseline algorithms, in
terms of both time requirements and accuracy of the inferred preferences. In
future work, we plan to evaluate the algorithm's effectiveness in a multi-agent
system, where one of the agents is enabled to infer the preferences of an
opponent using our preference inference algorithm."
0,0,SocNavGym: A Reinforcement Learning Gym for Social Navigation,"It is essential for autonomous robots to be socially compliant while
navigating in human-populated environments. Machine Learning and, especially,
Deep Reinforcement Learning have recently gained considerable traction in the
field of Social Navigation. This can be partially attributed to the resulting
policies not being bound by human limitations in terms of code complexity or
the number of variables that are handled. Unfortunately, the lack of safety
guarantees and the large data requirements by DRL algorithms make learning in
the real world unfeasible. To bridge this gap, simulation environments are
frequently used. We propose SocNavGym, an advanced simulation environment for
social navigation that can generate a wide variety of social navigation
scenarios and facilitates the development of intelligent social agents.
SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly
configured to generate different types of social navigation scenarios. It can
also be configured to work with different hand-crafted and data-driven social
reward signals and to yield a variety of evaluation metrics to benchmark
agents' performance. Further, we also provide a case study where a Dueling-DQN
agent is trained to learn social-navigation policies using SocNavGym. The
results provides evidence that SocNavGym can be used to train an agent from
scratch to navigate in simple as well as complex social scenarios. Our
experiments also show that the agents trained using the data-driven reward
function displays more advanced social compliance in comparison to the
heuristic-based reward function."
0,0,Decentralized Inference via Capability Type Structures in Cooperative Multi-Agent Systems,"This work studies the problem of ad hoc teamwork in teams composed of agents
with differing computational capabilities. We consider cooperative multi-player
games in which each agent's policy is constrained by a private capability
parameter, and agents with higher capabilities are able to simulate the
behavior of agents with lower capabilities (but not vice-versa). To address
this challenge, we propose an algorithm that maintains a belief over the other
agents' capabilities and incorporates this belief into the planning process.
Our primary innovation is a novel framework based on capability type
structures, which ensures that the belief updates remain consistent and
informative without constructing the infinite hierarchy of beliefs. We also
extend our techniques to settings where the agents' observations are subject to
noise. We provide examples of games in which deviations in capability between
oblivious agents can lead to arbitrarily poor outcomes, and experimentally
validate that our capability-aware algorithm avoids the anti-cooperative
behavior of the naive approach in these toy settings as well as a more complex
cooperative checkers environment."
1,0,Ensoul: A framework for the creation of self organizing intelligent ultra low power systems (SOULS) through evolutionary enerstatic networks,"Ensoul is a framework proposed for the purpose of creating technologies that
create more technologies through the combined use of networks, and nests, of
energy homeostatic (enerstatic) loops and open-ended evolutionary techniques.
Generative technologies developed by such an approach serve as both simple, yet
insightful models of thermodynamically driven complex systems and as powerful
sources of novel technologies. ""Self Organizing intelligent Ultra Low power
Systems"" (SOULS) is a term that well describes the technologies produced by
such a generative technology, as well as the generative technology itself. The
term is meant to capture the abstract nature of such technologies as being
independent of the substrate in which they are embedded. In other words, SOULS
can be biological, artificial or hybrid in form."
0,0,N$\text{A}^\text{2}$Q: Neural Attention Additive Model for Interpretable Multi-Agent Q-Learning,"Value decomposition is widely used in cooperative multi-agent reinforcement
learning, however, its implicit credit assignment mechanism is not yet fully
understood due to black-box networks. In this work, we study an interpretable
value decomposition framework via the family of generalized additive models. We
present a novel method, named Neural Attention Additive
Q-learning~(N$\text{A}^\text{2}$Q), providing inherent intelligibility of
collaboration behavior. N$\text{A}^\text{2}$Q can explicitly factorize the
optimal joint policy induced by enriching shape functions to model all possible
coalitions of agents into individual policies. Moreover, we construct identity
semantics to promote estimating credits together with the global state and
individual value functions, where local semantic masks help us diagnose whether
each agent captures relevant-task information. Extensive experiments show that
N$\text{A}^\text{2}$Q consistently achieves superior performance compared to
different state-of-the-art methods on all challenging tasks, while yielding
human-like interpretability."
0,0,Centralized control for multi-agent RL in a complex Real-Time-Strategy game,"Multi-agent Reinforcement learning (MARL) studies the behaviour of multiple
learning agents that coexist in a shared environment. MARL is more challenging
than single-agent RL because it involves more complex learning dynamics: the
observations and rewards of each agent are functions of all other agents. In
the context of MARL, Real-Time Strategy (RTS) games represent very challenging
environments where multiple players interact simultaneously and control many
units of different natures all at once. In fact, RTS games are so challenging
for the current RL methods, that just being able to tackle them with RL is
interesting. This project provides the end-to-end experience of applying RL in
the Lux AI v2 Kaggle competition, where competitors design agents to control
variable-sized fleets of units and tackle a multi-variable optimization,
resource gathering, and allocation problem in a 1v1 scenario against other
competitors. We use a centralized approach for training the RL agents, and
report multiple design decisions along the process. We provide the source code
of the project: https://github.com/roger-creus/centralized-control-lux."
1,0,Focusing on Information Context for ITS using a Spatial Age of Information Model,"New technologies for sensing and communication act as enablers for
cooperative driving applications. Sensors are able to detect objects in the
surrounding environment and information such as their current location is
exchanged among vehicles. In order to cope with the vehicles' mobility, such
information is required to be as fresh as possible for proper operation of
cooperative driving applications. The age of information (AoI) has been
proposed as a metric for evaluating freshness of information; recently also
within the context of intelligent transportation systems (ITS). We investigate
mechanisms to reduce the AoI of data transported in form of beacon messages
while controlling their emission rate. We aim to balance packet collision
probability and beacon frequency using the average peak age of information
(PAoI) as a metric. This metric, however, only accounts for the generation time
of the data but not for application-specific aspects, such as the location of
the transmitting vehicle. We thus propose a new way of interpreting the AoI by
considering information context; thereby incorporating vehicles' locations. As
an example, we characterize such importance using the orientation and the
distance of the involved vehicles. In particular, we introduce a weighting
coefficient used in combination with the PAoI to evaluate the information
freshness; emphasizing on information from more important neighbors. We further
design the beaconing approach in a way to meet a given AoI requirement, thus,
saving resources on the wireless channel while keeping the AoI minimal. We
illustrate the effectiveness of our approach in Manhattan-like urban scenarios;
reaching pre-specified targets for the AoI of beacon messages."
0,0,SEA: A Spatially Explicit Architecture for Multi-Agent Reinforcement Learning,"Spatial information is essential in various fields. How to explicitly model
according to the spatial location of agents is also very important for the
multi-agent problem, especially when the number of agents is changing and the
scale is enormous. Inspired by the point cloud task in computer vision, we
propose a spatial information extraction structure for multi-agent
reinforcement learning in this paper. Agents can effectively share the
neighborhood and global information through a spatially encoder-decoder
structure. Our method follows the centralized training with decentralized
execution (CTDE) paradigm. In addition, our structure can be applied to various
existing mainstream reinforcement learning algorithms with minor modifications
and can deal with the problem with a variable number of agents. The experiments
in several multi-agent scenarios show that the existing methods can get
convincing results by adding our spatially explicit architecture."
0,0,PID-inspired modifications in response threshold models in swarm intelligent systems,"In this study, we investigate the effectiveness of using the PID
(Proportional - Integral - Derivative) control loop factors for modifying
response thresholds in a decentralized, non-communicating, threshold-based
swarm. Each agent in our swarm has a set of four thresholds, each corresponding
to a task the agent is capable of performing. The agent will act on a
particular task if the stimulus is higher than its corresponding threshold. The
ability to modify their thresholds allows the agents to specialize dynamically
in response to task demands. Current approaches to dynamic thresholds typically
use a learning and forgetting process to adjust thresholds. These methods are
able to effectively specialize once, but can have difficulty re-specializing if
the task demands change. Our approach, inspired by the PID control loop, alters
the threshold values based on the current task demand value, the change in task
demand, and the cumulative sum of previous task demands. We show that our
PID-inspired method is scalable and outperforms fixed and current learning and
forgetting response thresholds with non-changing, constant, and abrupt changes
in task demand. This superior performance is due to the ability of our method
to re-specialize repeatedly in response to changing task demands."
0,0,Stubborn: An Environment for Evaluating Stubbornness between Agents with Aligned Incentives,"Recent research in multi-agent reinforcement learning (MARL) has shown
success in learning social behavior and cooperation. Social dilemmas between
agents in mixed-sum settings have been studied extensively, but there is little
research into social dilemmas in fullycooperative settings, where agents have
no prospect of gaining reward at another agent's expense.
  While fully-aligned interests are conducive to cooperation between agents,
they do not guarantee it. We propose a measure of ""stubbornness"" between agents
that aims to capture the human social behavior from which it takes its name: a
disagreement that is gradually escalating and potentially disastrous. We would
like to promote research into the tendency of agents to be stubborn, the
reactions of counterpart agents, and the resulting social dynamics.
  In this paper we present Stubborn, an environment for evaluating stubbornness
between agents with fully-aligned incentives. In our preliminary results, the
agents learn to use their partner's stubbornness as a signal for improving the
choices that they make in the environment."
0,0,A Spatial Calibration Method for Robust Cooperative Perception,"Cooperative perception is a promising technique for enhancing the perception
capabilities of automated vehicles through vehicle-to-everything (V2X)
cooperation, provided that accurate relative pose transforms are available.
Nevertheless, obtaining precise positioning information often entails high
costs associated with navigation systems. Moreover, signal drift resulting from
factors such as occlusion and multipath effects can compromise the stability of
the positioning information. Hence, a low-cost and robust method is required to
calibrate relative pose information for multi-agent cooperative perception. In
this paper, we propose a simple but effective inter-agent object association
approach (CBM), which constructs contexts using the detected bounding boxes,
followed by local context matching and global consensus maximization. Based on
the matched correspondences, optimal relative pose transform is estimated,
followed by cooperative perception fusion. Extensive experimental studies are
conducted on both the simulated and real-world datasets, high object
association precision and decimeter level relative pose calibration accuracy is
achieved among the cooperating agents even with larger inter-agent localization
errors. Furthermore, the proposed approach outperforms the state-of-the-art
methods in terms of object association and relative pose estimation accuracy,
as well as the robustness of cooperative perception against the pose errors of
the connected agents. The code will be available at
https://github.com/zhyingS/CBM."
0,0,Studying the Impact of Semi-Cooperative Drivers on Overall Highway Flow,"Semi-cooperative behaviors are intrinsic properties of human drivers and
should be considered for autonomous driving. In addition, new autonomous
planners can consider the social value orientation (SVO) of human drivers to
generate socially-compliant trajectories. Yet the overall impact on traffic
flow for this new class of planners remain to be understood. In this work, we
present study of implicit semi-cooperative driving where agents deploy a
game-theoretic version of iterative best response assuming knowledge of the
SVOs of other agents. We simulate nominal traffic flow and investigate whether
the proportion of prosocial agents on the road impact individual or system-wide
driving performance. Experiments show that the proportion of prosocial agents
has a minor impact on overall traffic flow and that benefits of
semi-cooperation disproportionally affect egoistic and high-speed drivers."
0,0,Recomputing Solutions to Perturbed Multi-Commodity Pickup and Delivery Vehicle Routing Problems using Monte Carlo Tree Search,"The Multi-Commodity Pickup and Delivery Vehicle Routing Problem aims to
optimize the pickup and delivery of multiple unique commodities using a fleet
of several agents with limited payload capacities. This paper addresses the
challenge of quickly recomputing the solution to this NP-hard problem when
there are unexpected perturbations to the nominal task definitions, likely to
occur under real-world operating conditions. The proposed method first
decomposes the nominal problem by constructing a search tree using Monte Carlo
Tree Search for task assignment, and uses a rapid heuristic for routing each
agent. When changes to the problem are revealed, the nominal search tree is
rapidly updated with new costs under the updated problem parameters, generating
solutions quicker and with a reduced optimality gap, as compared to recomputing
the solution as an entirely new problem. Computational experiments are
conducted by varying the locations of the nominal problem and the payload
capacity of an agent to demonstrate the effectiveness of utilizing the nominal
search tree to handle perturbations for real-time implementation."
1,1,Statistical Estimation for Covariance Structures with Tail Estimates using Nodewise Quantile Predictive Regression Models,"This paper considers the specification of covariance structures with tail
estimates. We focus on two aspects: (i) the estimation of the VaR-CoVaR risk
matrix in the case of larger number of time series observations than assets in
a portfolio using quantile predictive regression models without assuming the
presence of nonstationary regressors and; (ii) the construction of a novel
variable selection algorithm, so-called, Feature Ordering by Centrality
Exclusion (FOCE), which is based on an assumption-lean regression framework,
has no tuning parameters and is proved to be consistent under general sparsity
assumptions. We illustrate the usefulness of our proposed methodology with
numerical studies of real and simulated datasets when modelling systemic risk
in a network."
1,1,Context-Dependent Heterogeneous Preferences: A Comment on Barseghyan and Molinari (2023),"Barseghyan and Molinari (2023) give sufficient conditions for
semi-nonparametric point identification of parameters of interest in a mixture
model of decision-making under risk, allowing for unobserved heterogeneity in
utility functions and limited consideration. A key assumption in the model is
that the heterogeneity of risk preferences is unobservable but
context-independent. In this comment, we build on their insights and present
identification results in a setting where the risk preferences are allowed to
be context-dependent."
1,1,Modeling Interference Using Experiment Roll-out,"Experiments on online marketplaces and social networks suffer from
interference, where the outcome of a unit is impacted by the treatment status
of other units. We propose a framework for modeling interference using a
ubiquitous deployment mechanism for experiments, staggered roll-out designs,
which slowly increase the fraction of units exposed to the treatment to
mitigate any unanticipated adverse side effects. Our main idea is to leverage
the temporal variations in treatment assignments introduced by roll-outs to
model the interference structure. We first present a set of model
identification conditions under which the estimation of common estimands is
possible and show how these conditions are aided by roll-out designs. Since
there are often multiple competing models of interference in practice, we then
develop a model selection method that evaluates models based on their ability
to explain outcome variation observed along the roll-out. Through simulations,
we show that our heuristic model selection method, Leave-One-Period-Out,
outperforms other baselines. We conclude with a set of considerations,
robustness checks, and potential limitations for practitioners wishing to use
our framework."
1,1,Nowcasting with signature methods,"Key economic variables are often published with a significant delay of over a
month. The nowcasting literature has arisen to provide fast, reliable estimates
of delayed economic indicators and is closely related to filtering methods in
signal processing. The path signature is a mathematical object which captures
geometric properties of sequential data; it naturally handles missing data from
mixed frequency and/or irregular sampling -- issues often encountered when
merging multiple data sources -- by embedding the observed data in continuous
time. Calculating path signatures and using them as features in models has
achieved state-of-the-art results in fields such as finance, medicine, and
cyber security. We look at the nowcasting problem by applying regression on
signatures, a simple linear model on these nonlinear objects that we show
subsumes the popular Kalman filter. We quantify the performance via a
simulation exercise, and through application to nowcasting US GDP growth, where
we see a lower error than a dynamic factor model based on the New York Fed
staff nowcasting model. Finally we demonstrate the flexibility of this method
by applying regression on signatures to nowcast weekly fuel prices using daily
data. Regression on signatures is an easy-to-apply approach that allows great
flexibility for data with complex sampling patterns."
1,1,Monitoring multicountry macroeconomic risk,"We propose a multicountry quantile factor augmeneted vector autoregression
(QFAVAR) to model heterogeneities both across countries and across
characteristics of the distributions of macroeconomic time series. The presence
of quantile factors allows for summarizing these two heterogeneities in a
parsimonious way. We develop two algorithms for posterior inference that
feature varying level of trade-off between estimation precision and
computational speed. Using monthly data for the euro area, we establish the
good empirical properties of the QFAVAR as a tool for assessing the effects of
global shocks on country-level macroeconomic risks. In particular, QFAVAR
short-run tail forecasts are more accurate compared to a FAVAR with symmetric
Gaussian errors, as well as univariate quantile autoregressions that ignore
comovements among quantiles of macroeconomic variables. We also illustrate how
quantile impulse response functions and quantile connectedness measures,
resulting from the new model, can be used to implement joint risk scenario
analysis."
1,1,Grenander-type Density Estimation under Myerson Regularity,"This study presents a novel approach to the density estimation of private
values from second-price auctions, diverging from the conventional use of
smoothing-based estimators. We introduce a Grenander-type estimator,
constructed based on a shape restriction in the form of a convexity constraint.
This constraint corresponds to the renowned Myerson regularity condition in
auction theory, which is equivalent to the concavity of the revenue function
for selling the auction item. Our estimator is nonparametric and does not
require any tuning parameters. Under mild assumptions, we establish the
cube-root consistency and show that the estimator asymptotically follows the
scaled Chernoff's distribution. Moreover, we demonstrate that the estimator
achieves the minimax optimal convergence rate."
1,1,Designing Discontinuities,"Discontinuities can be fairly arbitrary but also cause a significant impact
on outcomes in social systems. Indeed, their arbitrariness is why they have
been used to infer causal relationships among variables in numerous settings.
Regression discontinuity from econometrics assumes the existence of a
discontinuous variable that splits the population into distinct partitions to
estimate the causal effects of a given phenomenon. Here we consider the design
of partitions for a given discontinuous variable to optimize a certain effect
previously studied using regression discontinuity. To do so, we propose a
quantization-theoretic approach to optimize the effect of interest, first
learning the causal effect size of a given discontinuous variable and then
applying dynamic programming for optimal quantization design of discontinuities
that balance the gain and loss in the effect size. We also develop a
computationally-efficient reinforcement learning algorithm for the dynamic
programming formulation of optimal quantization. We demonstrate our approach by
designing optimal time zone borders for counterfactuals of social capital,
social mobility, and health. This is based on regression discontinuity analyses
we perform on novel data, which may be of independent empirical interest in
showing a causal relationship between sunset time and social capital."
1,1,Hierarchical DCC-HEAVY Model for High-Dimensional Covariance Matrices,"We introduce a new HD DCC-HEAVY class of hierarchical-type factor models for
conditional covariance matrices of high-dimensional returns, employing the
corresponding realized measures built from higher-frequency data. The modelling
approach features sophisticated asymmetric dynamics in covariances coupled with
straightforward estimation and forecasting schemes, independent of the
cross-sectional dimension of the assets under consideration. Empirical analyses
suggest the HD DCC-HEAVY models have a better in-sample fit, and deliver
statistically and economically significant out-of-sample gains relative to the
standard benchmarks and existing hierarchical factor models. The results are
robust under different market conditions."
1,1,Efficient Semiparametric Estimation of Average Treatment Effects Under Covariate Adaptive Randomization,"Experiments that use covariate adaptive randomization (CAR) are commonplace
in applied economics and other fields. In such experiments, the experimenter
first stratifies the sample according to observed baseline covariates and then
assigns treatment randomly within these strata so as to achieve balance
according to pre-specified stratum-specific target assignment proportions. In
this paper, we compute the semiparametric efficiency bound for estimating the
average treatment effect (ATE) in such experiments with binary treatments
allowing for the class of CAR procedures considered in Bugni, Canay, and Shaikh
(2018, 2019). This is a broad class of procedures and is motivated by those
used in practice. The stratum-specific target proportions play the role of the
propensity score conditional on all baseline covariates (and not just the
strata) in these experiments. Thus, the efficiency bound is a special case of
the bound in Hahn (1998), but conditional on all baseline covariates.
Additionally, this efficiency bound is shown to be achievable under the same
conditions as those used to derive the bound by using a cross-fitted
Nadaraya-Watson kernel estimator to form nonparametric regression adjustments."
1,1,The Newsvendor with Advice,"The standard newsvendor model assumes a stochastic demand distribution as
well as costs for overages and underages. The celebrated critical fractile
formula can be used to determine the optimal inventory levels. While the model
has been leveraged in numerous applications, often in practice more
characteristics and features of the problem are known. Using these features, it
is common to employ machine learning to predict inventory levels over the
classic newsvendor approach.
  An emerging line of work has shown how to use incorporate machine learned
predictions into models to circumvent lower bounds and give improved
performance. This paper develops the first newsvendor model that incorporates
machine learned predictions. The paper considers a repeated newsvendor setting
with nonstationary demand. There is a prediction is for each period's demand
and, as is the case in machine learning, the prediction can be noisy. The goal
is for an inventory management algorithm to take advantage of the prediction
when it is high quality and to have performance bounded by the best possible
algorithm without a prediction when the prediction is highly inaccurate.
  This paper proposes a generic model of a nonstationary newsvendor without
predictions and develops optimal upper and lower bounds on the regret. The
paper then propose an algorithm that takes a prediction as advice which,
without a priori knowledge of the accuracy of the advice, achieves the nearly
optimal minimax regret. The perforamce mataches the best possible had the
accuracy been known in advance. We show the theory is predictive of practice on
real data and demonstrtate emprically that our algorithm has a 14% to 19% lower
cost than a clairvoyant who knows the quality of the advice beforehand."
1,1,Semiparametrically Optimal Cointegration Test,"This paper aims to address the issue of semiparametric efficiency for
cointegration rank testing in finite-order vector autoregressive models, where
the innovation distribution is considered an infinite-dimensional nuisance
parameter. Our asymptotic analysis relies on Le Cam's theory of limit
experiment, which in this context takes the form of Locally Asymptotically
Brownian Functional (LABF). By leveraging the structural version of LABF, an
Ornstein-Uhlenbeck experiment, we develop the asymptotic power envelopes of
asymptotically invariant tests for both cases with and without a time trend. We
propose feasible tests based on a nonparametrically estimated density and
demonstrate that their power can achieve the semiparametric power envelopes,
making them semiparametrically optimal. We validate the theoretical results
through large-sample simulations and illustrate satisfactory size control and
excellent power performance of our tests under small samples. In both cases
with and without time trend, we show that a remarkable amount of additional
power can be obtained from non-Gaussian distributions."
1,1,Band-Pass Filtering with High-Dimensional Time Series,"The paper deals with the construction of a synthetic indicator of economic
growth, obtained by projecting a quarterly measure of aggregate economic
activity, namely gross domestic product (GDP), into the space spanned by a
finite number of smooth principal components, representative of the
medium-to-long-run component of economic growth of a high-dimensional time
series, available at the monthly frequency. The smooth principal components
result from applying a cross-sectional filter distilling the low-pass component
of growth in real time. The outcome of the projection is a monthly nowcast of
the medium-to-long-run component of GDP growth. After discussing the
theoretical properties of the indicator, we deal with the assessment of its
reliability and predictive validity with reference to a panel of macroeconomic
U.S. time series."
1,1,The price elasticity of Gleevec in patients with Chronic Myeloid Leukemia enrolled in Medicare Part D: Evidence from a regression discontinuity design,"Objective To assess the price elasticity of branded imatinib in chronic
myeloid leukemia (CML) patients on Medicare Part D to determine if high
out-of-pocket payments (OOP) are driving the substantial levels of
non-adherence observed in this population.
  Data sources and study setting We use data from the TriNetX Diamond Network
(TDN) United States database for the period from first availability in 2011
through the end of patent exclusivity following the introduction of generic
imatinib in early 2016.
  Study design We implement a fuzzy regression discontinuity design to
separately estimate the effect of Medicare Part D enrollment at age 65 on
adherence and OOP in newly-diagnosed CML patients initiating branded imatinib.
The corresponding price elasticity of demand (PED) is estimated and results are
assessed across a variety of specifications and robustness checks.
  Data collection/extraction methods Data from eligible patients following the
application of inclusion and exclusion criteria were analyzed.
  Principal findings Our analysis suggests that there is a significant increase
in initial OOP of $232 (95% Confidence interval (CI): $102 to $362) for
individuals that enrolled in Part D due to expanded eligibility at age 65. The
relatively smaller and non-significant decrease in adherence of only 6
percentage points (95% CI: -0.21 to 0.08) led to a PED of -0.02 (95% CI:
-0.056, 0.015).
  Conclusion This study provides evidence regarding the financial impact of
coinsurance-based benefit designs on Medicare-age patients with CML initiating
branded imatinib. Results indicate that factors besides high OOP are driving
the substantial non-adherence observed in this population and add to the
growing literature on PED for specialty drugs."
1,1,On the Time-Varying Structure of the Arbitrage Pricing Theory using the Japanese Sector Indices,"This paper is the first study to examine the time instability of the APT in
the Japanese stock market. In particular, we measure how changes in each risk
factor affect the stock risk premiums to investigate the validity of the APT
over time, applying the rolling window method to Fama and MacBeth's (1973)
two-step regression and Kamstra and Shi's (2023) generalized GRS test. We
summarize our empirical results as follows: (1) the APT is supported over the
entire sample period but not at all times, (2) the changes in monetary policy
greatly affect the validity of the APT in Japan, and (3) the time-varying
estimates of the risk premiums for each factor are also unstable over time, and
they are affected by the business cycle and economic crises. Therefore, we
conclude that the validity of the APT as an appropriate model to explain the
Japanese sector index is not stable over time."
1,1,Does Principal Component Analysis Preserve the Sparsity in Sparse Weak Factor Models?,"This paper studies the principal component (PC) method-based estimation of
weak factor models with sparse loadings. We uncover an intrinsic near-sparsity
preservation property for the PC estimators of loadings, which comes from the
approximately upper triangular (block) structure of the rotation matrix. It
implies an asymmetric relationship among factors: the rotated loadings for a
stronger factor can be contaminated by those from a weaker one, but the
loadings for a weaker factor is almost free of the impact of those from a
stronger one. More importantly, the finding implies that there is no need to
use complicated penalties to sparsify the loading estimators. Instead, we adopt
a simple screening method to recover the sparsity and construct estimators for
various factor strengths. In addition, for sparse weak factor models, we
provide a singular value thresholding-based approach to determine the number of
factors and establish uniform convergence rates for PC estimators, which
complement Bai and Ng (2023). The accuracy and efficiency of the proposed
estimators are investigated via Monte Carlo simulations. The application to the
FRED-QD dataset reveals the underlying factor strengths and loading sparsity as
well as their dynamic features."
1,1,Volatility of Volatility and Leverage Effect from Options,"We propose model-free (nonparametric) estimators of the volatility of
volatility and leverage effect using high-frequency observations of short-dated
options. At each point in time, we integrate available options into estimates
of the conditional characteristic function of the price increment until the
options' expiration and we use these estimates to recover spot volatility. Our
volatility of volatility estimator is then formed from the sample variance and
first-order autocovariance of the spot volatility increments, with the latter
correcting for the bias in the former due to option observation errors. The
leverage effect estimator is the sample covariance between price increments and
the estimated volatility increments. The rate of convergence of the estimators
depends on the diffusive innovations in the latent volatility process as well
as on the observation error in the options with strikes in the vicinity of the
current spot price. Feasible inference is developed in a way that does not
require prior knowledge of the source of estimation error that is
asymptotically dominating."
0,1,Risk management in the use of published statistical results for policy decisions,"Statistical inferential results generally come with a measure of reliability
for decision-making purposes. For a policy implementer, the value of
implementing published policy research depends critically upon this
reliability. For a policy researcher, the value of policy implementation may
depend weakly or not at all upon the policy's outcome. Some researchers might
find it advantageous to overstate the reliability of statistical results.
Implementers may find it difficult or impossible to determine whether
researchers are overstating reliability. This information asymmetry between
researchers and implementers can lead to an adverse selection problem where, at
best, the full benefits of a policy are not realized or, at worst, a policy is
deemed too risky to implement at any scale. Researchers can remedy this by
guaranteeing the policy outcome. Researchers can overcome their own risk
aversion and wealth constraints by exchanging risks with other researchers or
offering only partial insurance. The problem and remedy are illustrated using a
confidence interval for the success probability of a binomial policy outcome."
1,1,Debiased inference for dynamic nonlinear models with two-way fixed effects,"Panel data models often use fixed effects to account for unobserved
heterogeneities. These fixed effects are typically incidental parameters and
their estimators converge slowly relative to the square root of the sample
size. In the maximum likelihood context, this induces an asymptotic bias of the
likelihood function. Test statistics derived from the asymptotically biased
likelihood, therefore, no longer follow their standard limiting distributions.
This causes severe distortions in test sizes. We consider a generic class of
dynamic nonlinear models with two-way fixed effects and propose an analytical
bias correction method for the likelihood function. We formally show that the
likelihood ratio, the Lagrange-multiplier, and the Wald test statistics derived
from the corrected likelihood follow their standard asymptotic distributions.
As a by-product, a bias-corrected estimator of the structural parameter can
also be derived from the corrected likelihood function. We evaluate the
performance of our bias correction procedure through simulations."
1,1,Doubly Robust Uniform Confidence Bands for Group-Time Conditional Average Treatment Effects in Difference-in-Differences,"This study considers a panel data analysis to examine the heterogeneity in
treatment effects with respect to a pre-treatment covariate of interest in the
staggered difference-in-differences setting in Callaway and Sant'Anna (2021).
Under a set of standard identification conditions, a doubly robust estimand
conditional on the covariate identifies the group-time conditional average
treatment effect given the covariate. Given this identification result, we
propose a three-step estimation procedure based on nonparametric local linear
regressions and parametric estimation methods, and develop a doubly robust
inference method to construct a uniform confidence band of the group-time
conditional average treatment effect function."
1,1,Large Global Volatility Matrix Analysis Based on Structural Information,"In this paper, we develop a novel large volatility matrix estimation
procedure for analyzing global financial markets. Practitioners often use
lower-frequency data, such as weekly or monthly returns, to address the issue
of different trading hours in the international financial market. However, this
approach can lead to inefficiency due to information loss. To mitigate this
problem, our proposed method, called Structured Principal Orthogonal complEment
Thresholding (Structured-POET), incorporates structural information for both
global and national factor models. We establish the asymptotic properties of
the Structured-POET estimator, and also demonstrate the drawbacks of
conventional covariance matrix estimation procedures when using lower-frequency
data. Finally, we apply the Structured-POET estimator to an out-of-sample
portfolio allocation study using international stock market data."
1,1,Transfer Estimates for Causal Effects across Heterogeneous Sites,"We consider the problem of extrapolating treatment effects across
heterogeneous populations (``sites""/``contexts""). We consider an idealized
scenario in which the researcher observes cross-sectional data for a large
number of units across several ``experimental"" sites in which an intervention
has already been implemented to a new ``target"" site for which a baseline
survey of unit-specific, pre-treatment outcomes and relevant attributes is
available. We propose a transfer estimator that exploits cross-sectional
variation between individuals and sites to predict treatment outcomes using
baseline outcome data for the target location. We consider the problem of
obtaining a predictor of conditional average treatment effects at the target
site that is MSE optimal within a certain class and subject to data
constraints. Our approach is design-based in the sense that the performance of
the predictor is evaluated given the specific, finite selection of experimental
and target sites. Our approach is nonparametric, and our formal results concern
the construction of an optimal basis of predictors as well as convergence rates
for the estimated conditional average treatment effect relative to the
constrained-optimal population predictor for the target site. We illustrate our
approach using a combined data set of five multi-site randomized controlled
trials (RCTs) to evaluate the effect of conditional cash transfers on school
attendance."
1,1,Estimating Input Coefficients for Regional Input-Output Tables Using Deep Learning with Mixup,"An input-output table is an important data for analyzing the economic
situation of a region. Generally, the input-output table for each region
(regional input-output table) in Japan is not always publicly available, so it
is necessary to estimate the table. In particular, various methods have been
developed for estimating input coefficients, which are an important part of the
input-output table. Currently, non-survey methods are often used to estimate
input coefficients because they require less data and computation, but these
methods have some problems, such as discarding information and requiring
additional data for estimation.
  In this study, the input coefficients are estimated by approximating the
generation process with an artificial neural network (ANN) to mitigate the
problems of the non-survey methods and to estimate the input coefficients with
higher precision. To avoid over-fitting due to the small data used, data
augmentation, called mixup, is introduced to increase the data size by
generating virtual regions through region composition and scaling.
  By comparing the estimates of the input coefficients with those of Japan as a
whole, it is shown that the accuracy of the method of this research is higher
and more stable than that of the conventional non-survey methods. In addition,
the estimated input coefficients for the three cities in Japan are generally
close to the published values for each city."
1,1,Estimation and Inference in Threshold Predictive Regression Models with Locally Explosive Regressors,"In this paper, we study the estimation of the threshold predictive regression
model with hybrid stochastic local unit root predictors. We demonstrate the
estimation procedure and derive the asymptotic distribution of the least square
estimator and the IV based estimator proposed by Magdalinos and Phillips
(2009), under the null hypothesis of a diminishing threshold effect. Simulation
experiments focus on the finite sample performance of our proposed estimators
and the corresponding predictability tests as in Gonzalo and Pitarakis (2012),
under the presence of threshold effects with stochastic local unit roots. An
empirical application to stock return equity indices, illustrate the usefulness
of our framework in uncovering regimes of predictability during certain
periods. In particular, we focus on an aspect not previously examined in the
predictability literature, that is, the effect of economic policy uncertainty."
1,1,Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control,"Motivated by a recent literature on the double-descent phenomenon in machine
learning, we consider highly over-parametrized models in causal inference,
including synthetic control with many control units. In such models, there may
be so many free parameters that the model fits the training data perfectly. As
a motivating example, we first investigate high-dimensional linear regression
for imputing wage data, where we find that models with many more covariates
than sample size can outperform simple ones. As our main contribution, we
document the performance of high-dimensional synthetic control estimators with
many control units. We find that adding control units can help improve
imputation performance even beyond the point where the pre-treatment fit is
perfect. We then provide a unified theoretical perspective on the performance
of these high-dimensional models. Specifically, we show that more complex
models can be interpreted as model-averaging estimators over simpler ones,
which we link to an improvement in average performance. This perspective yields
concrete insights into the use of synthetic control when control units are many
relative to the number of pre-treatment periods."
1,1,Optimal tests following sequential experiments,"Recent years have seen tremendous advances in the theory and application of
sequential experiments. While these experiments are not always designed with
hypothesis testing in mind, researchers may still be interested in performing
tests after the experiment is completed. The purpose of this paper is to aid in
the development of optimal tests for sequential experiments by analyzing their
asymptotic properties. Our key finding is that the asymptotic power function of
any test can be matched by a test in a limit experiment where a Gaussian
process is observed for each treatment, and inference is made for the drifts of
these processes. This result has important implications, including a powerful
sufficiency result: any candidate test only needs to rely on a fixed set of
statistics, regardless of the type of sequential experiment. These statistics
are the number of times each treatment has been sampled by the end of the
experiment, along with final value of the score (for parametric models) or
efficient influence function (for non-parametric models) process for each
treatment. We then characterize asymptotically optimal tests under various
restrictions such as unbiasedness, \alpha-spending constraints etc. Finally, we
apply our our results to three key classes of sequential experiments: costly
sampling, group sequential trials, and bandit experiments, and show how optimal
inference can be conducted in these scenarios."
1,1,Augmented balancing weights as linear regression,"We provide a novel characterization of augmented balancing weights, also
known as Automatic Debiased Machine Learning (AutoDML). These estimators
combine outcome modeling with balancing weights, which estimate inverse
propensity score weights directly. When the outcome and weighting models are
both linear in some (possibly infinite) basis, we show that the augmented
estimator is equivalent to a single linear model with coefficients that combine
the original outcome model coefficients and OLS; in many settings, the
augmented estimator collapses to OLS alone. We then extend these results to
specific choices of outcome and weighting models. We first show that the
combined estimator that uses (kernel) ridge regression for both outcome and
weighting models is equivalent to a single, undersmoothed (kernel) ridge
regression; this also holds when considering asymptotic rates. When the
weighting model is instead lasso regression, we give closed-form expressions
for special cases and demonstrate a ``double selection'' property. Finally, we
generalize these results to linear estimands via the Riesz representer. Our
framework ``opens the black box'' on these increasingly popular estimators and
provides important insights into estimation choices for augmented balancing
weights."
1,1,Assessing Text Mining and Technical Analyses on Forecasting Financial Time Series,"Forecasting financial time series (FTS) is an essential field in finance and
economics that anticipates market movements in financial markets. This paper
investigates the accuracy of text mining and technical analyses in forecasting
financial time series. It focuses on the S&P500 stock market index during the
pandemic, which tracks the performance of the largest publicly traded companies
in the US. The study compares two methods of forecasting the future price of
the S&P500: text mining, which uses NLP techniques to extract meaningful
insights from financial news, and technical analysis, which uses historical
price and volume data to make predictions. The study examines the advantages
and limitations of both methods and analyze their performance in predicting the
S&P500. The FinBERT model outperforms other models in terms of S&P500 price
prediction, as evidenced by its lower RMSE value, and has the potential to
revolutionize financial analysis and prediction using financial news data.
Keywords: ARIMA, BERT, FinBERT, Forecasting Financial Time Series, GARCH, LSTM,
Technical Analysis, Text Mining JEL classifications: G4, C8"
1,1,Convexity Not Required: Estimation of Smooth Moment Condition Models,"Generalized and Simulated Method of Moments are often used to estimate
structural Economic models. Yet, it is commonly reported that optimization is
challenging because the corresponding objective function is non-convex. For
smooth problems, this paper shows that convexity is not required: under a
global rank condition involving the Jacobian of the sample moments, certain
algorithms are globally convergent. These include a gradient-descent and a
Gauss-Newton algorithm with appropriate choice of tuning parameters. The
results are robust to 1) non-convexity, 2) one-to-one non-linear
reparameterizations, and 3) moderate misspecification. In contrast,
Newton-Raphson and quasi-Newton methods can fail to converge for the same
estimation because of non-convexity. A simple example illustrates a non-convex
GMM estimation problem that satisfies the aforementioned rank condition.
Empirical applications to random coefficient demand estimation and impulse
response matching further illustrate the results."
1,1,A universal model for the Lorenz curve with novel applications for datasets containing zeros and/or exhibiting extreme inequality,"Given that the existing parametric functional forms for the Lorenz curve do
not fit all possible size distributions, a universal parametric functional form
is introduced. By using the empirical data from different scientific
disciplines and also the hypothetical data, this study shows that, the proposed
model fits not only the data whose actual Lorenz plots have a typical convex
segment but also the data whose actual Lorenz plots have both horizontal and
convex segments practically well. It also perfectly fits the data whose
observation is larger in size while the rest of observations are smaller and
equal in size as characterized by 2 positive-slope linear segments. In
addition, the proposed model has a closed-form expression for the Gini index,
making it computationally convenient to calculate. Considering that the Lorenz
curve and the Gini index are widely used in various disciplines of sciences,
the proposed model and the closed-form expression for the Gini index could be
used as alternative tools to analyze size distributions of non-negative
quantities and examine their inequalities or unevennesses."
1,1,Difference-in-Differences with Compositional Changes,"This paper studies difference-in-differences (DiD) setups with repeated
cross-sectional data and potential compositional changes across time periods.
We begin our analysis by deriving the efficient influence function and the
semiparametric efficiency bound for the average treatment effect on the treated
(ATT). We introduce nonparametric estimators that attain the semiparametric
efficiency bound under mild rate conditions on the estimators of the nuisance
functions, exhibiting a type of rate doubly-robust (DR) property. Additionally,
we document a trade-off related to compositional changes: We derive the
asymptotic bias of DR DiD estimators that erroneously exclude compositional
changes and the efficiency loss when one fails to correctly rule out
compositional changes. We propose a nonparametric Hausman-type test for
compositional changes based on these trade-offs. The finite sample performance
of the proposed DiD tools is evaluated through Monte Carlo experiments and an
empirical application. As a by-product of our analysis, we present a new
uniform stochastic expansion of the local polynomial multinomial logit
estimator, which may be of independent interest."
1,1,Estimation of Characteristics-based Quantile Factor Models,"This paper studies the estimation of characteristic-based quantile factor
models where the factor loadings are unknown functions of observed individual
characteristics while the idiosyncratic error terms are subject to conditional
quantile restrictions. We propose a three-stage estimation procedure that is
easily implementable in practice and has nice properties. The convergence
rates, the limiting distributions of the estimated factors and loading
functions, and a consistent selection criterion for the number of factors at
each quantile are derived under general conditions. The proposed estimation
methodology is shown to work satisfactorily when: (i) the idiosyncratic errors
have heavy tails, (ii) the time dimension of the panel dataset is not large,
and (iii) the number of factors exceeds the number of characteristics. Finite
sample simulations and an empirical application aimed at estimating the loading
functions of the daily returns of a large panel of S\&P500 index securities
help illustrate these properties."
1,1,Common Correlated Effects Estimation of Nonlinear Panel Data Models,"This paper focuses on estimating the coefficients and average partial effects
of observed regressors in nonlinear panel data models with interactive fixed
effects, using the common correlated effects (CCE) framework. The proposed
two-step estimation method involves applying principal component analysis to
estimate latent factors based on cross-sectional averages of the regressors in
the first step, and jointly estimating the coefficients of the regressors and
factor loadings in the second step. The asymptotic distributions of the
proposed estimators are derived under general conditions, assuming that the
number of time-series observations is comparable to the number of
cross-sectional observations. To correct for asymptotic biases of the
estimators, we introduce both analytical and split-panel jackknife methods, and
confirm their good performance in finite samples using Monte Carlo simulations.
An empirical application utilizes the proposed method to study the arbitrage
behaviour of nonfinancial firms across different security markets."
1,1,Enhanced multilayer perceptron with feature selection and grid search for travel mode choice prediction,"Accurate and reliable prediction of individual travel mode choices is crucial
for developing multi-mode urban transportation systems, conducting
transportation planning and formulating traffic demand management strategies.
Traditional discrete choice models have dominated the modelling methods for
decades yet suffer from strict model assumptions and low prediction accuracy.
In recent years, machine learning (ML) models, such as neural networks and
boosting models, are widely used by researchers for travel mode choice
prediction and have yielded promising results. However, despite the superior
prediction performance, a large body of ML methods, especially the branch of
neural network models, is also limited by overfitting and tedious model
structure determination process. To bridge this gap, this study proposes an
enhanced multilayer perceptron (MLP; a neural network) with two hidden layers
for travel mode choice prediction; this MLP is enhanced by XGBoost (a boosting
method) for feature selection and a grid search method for optimal hidden
neurone determination of each hidden layer. The proposed method was trained and
tested on a real resident travel diary dataset collected in Chengdu, China."
1,1,The Ordinary Least Eigenvalues Estimator,"We propose a rate optimal estimator for the linear regression model on
network data with interacted (unobservable) individual effects. The estimator
achieves a faster rate of convergence $N$ compared to the standard estimators'
$\sqrt{N}$ rate and is efficient in cases that we discuss. We observe that the
individual effects alter the eigenvalue distribution of the data's matrix
representation in significant and distinctive ways. We subsequently offer a
correction for the \textit{ordinary least squares}' objective function to
attenuate the statistical noise that arises due to the individual effects, and
in some cases, completely eliminate it. The new estimator is asymptotically
normal and we provide a valid estimator for its asymptotic covariance matrix.
While this paper only considers models accounting for first-order interactions
between individual effects, our estimation procedure is naturally extendable to
higher-order interactions and more general specifications of the error terms."
3,1,Determination of the effective cointegration rank in high-dimensional time-series predictive regressions,"This paper proposes a new approach to identifying the effective cointegration
rank in high-dimensional unit-root (HDUR) time series from a prediction
perspective using reduced-rank regression. For a HDUR process $\mathbf{x}_t\in
\mathbb{R}^N$ and a stationary series $\mathbf{y}_t\in \mathbb{R}^p$ of
interest, our goal is to predict future values of $\mathbf{y}_t$ using
$\mathbf{x}_t$ and lagged values of $\mathbf{y}_t$. The proposed framework
consists of a two-step estimation procedure. First, the Principal Component
Analysis is used to identify all cointegrating vectors of $\mathbf{x}_t$.
Second, the co-integrated stationary series are used as regressors, together
with some lagged variables of $\mathbf{y}_t$, to predict $\mathbf{y}_t$. The
estimated reduced rank is then defined as the effective cointegration rank of
$\mathbf{x}_t$. Under the scenario that the autoregressive coefficient matrices
are sparse (or of low-rank), we apply the Least Absolute Shrinkage and
Selection Operator (or the reduced-rank techniques) to estimate the
autoregressive coefficients when the dimension involved is high. Theoretical
properties of the estimators are established under the assumptions that the
dimensions $p$ and $N$ and the sample size $T \to \infty$. Both simulated and
real examples are used to illustrate the proposed framework, and the empirical
application suggests that the proposed procedure fares well in predicting stock
returns."
1,1,Policy Learning under Biased Sample Selection,"Practitioners often use data from a randomized controlled trial to learn a
treatment assignment policy that can be deployed on a target population. A
recurring concern in doing so is that, even if the randomized trial was
well-executed (i.e., internal validity holds), the study participants may not
represent a random sample of the target population (i.e., external validity
fails)--and this may lead to policies that perform suboptimally on the target
population. We consider a model where observable attributes can impact sample
selection probabilities arbitrarily but the effect of unobservable attributes
is bounded by a constant, and we aim to learn policies with the best possible
performance guarantees that hold under any sampling bias of this type. In
particular, we derive the partial identification result for the worst-case
welfare in the presence of sampling bias and show that the optimal max-min,
max-min gain, and minimax regret policies depend on both the conditional
average treatment effect (CATE) and the conditional value-at-risk (CVaR) of
potential outcomes given covariates. To avoid finite-sample inefficiencies of
plug-in estimates, we further provide an end-to-end procedure for learning the
optimal max-min and max-min gain policies that does not require the separate
estimation of nuisance parameters."
1,1,The Impact of Industrial Zone:Evidence from China's National High-tech Zone Policy,"Based on the statistical yearbook data and related patent data of 287 cities
in China from 2000 to 2020, this study regards the policy of establishing the
national high-tech zones as a quasi-natural experiment. Using this experiment,
this study firstly estimated the treatment effect of the policy and checked the
robustness of the estimation. Then the study examined the heterogeneity in
different geographic demarcation of China and in different city level of China.
After that, this study explored the possible influence mechanism of the policy.
It shows that the possible mechanism of the policy is financial support,
industrial agglomeration of secondary industry and the spillovers. In the end,
this study examined the spillovers deeply and showed the distribution of
spillover effect."
1,1,A hybrid model for day-ahead electricity price forecasting: Combining fundamental and stochastic modelling,"The accurate prediction of short-term electricity prices is vital for
effective trading strategies, power plant scheduling, profit maximisation and
efficient system operation. However, uncertainties in supply and demand make
such predictions challenging. We propose a hybrid model that combines a
techno-economic energy system model with stochastic models to address this
challenge. The techno-economic model in our hybrid approach provides a deep
understanding of the market. It captures the underlying factors and their
impacts on electricity prices, which is impossible with statistical models
alone. The statistical models incorporate non-techno-economic aspects, such as
the expectations and speculative behaviour of market participants, through the
interpretation of prices. The hybrid model generates both conventional point
predictions and probabilistic forecasts, providing a comprehensive
understanding of the market landscape. Probabilistic forecasts are particularly
valuable because they account for market uncertainty, facilitating informed
decision-making and risk management. Our model delivers state-of-the-art
results, helping market participants to make informed decisions and operate
their systems more efficiently."
1,1,Club coefficients in the UEFA Champions League: Time for the shift to an Elo-based formula,"One of the most popular club football tournaments, the UEFA Champions League,
will see a fundamental reform from the 2024/25 season: the traditional group
stage will be replaced by one league where each of the 36 teams plays eight
matches. To guarantee that the opponents of the clubs are of the same strength
in the new design, it is crucial to forecast the performance of the teams
before the tournament as well as possible. This paper investigates whether the
currently used rating of the teams, the UEFA club coefficient, can be improved
by taking the games played in the national leagues into account. According to
our logistic regression models, a variant of the Elo method provides a higher
accuracy in terms of explanatory power in the Champions League matches. The
Union of European Football Associations (UEFA) is encouraged to follow the
example of the FIFA World Ranking and reform the calculation of the club
coefficients in order to avoid unbalanced schedules in the novel tournament
format of the Champions League."
1,1,Doubly Robust Estimators with Weak Overlap,"In this paper, we derive a new class of doubly robust estimators for
treatment effect estimands that is also robust against weak covariate overlap.
Our proposed estimator relies on trimming observations with extreme propensity
scores and uses a bias correction device for trimming bias. Our framework
accommodates many research designs, such as unconfoundedness, local treatment
effects, and difference-in-differences. Simulation exercises illustrate that
our proposed tools indeed have attractive finite sample properties, which are
aligned with our theoretical asymptotic results."
1,1,Adjustment with Many Regressors Under Covariate-Adaptive Randomizations,"Our paper identifies a trade-off when using regression adjustments (RAs) in
causal inference under covariate-adaptive randomizations (CARs). On one hand,
RAs can improve the efficiency of causal estimators by incorporating
information from covariates that are not used in the randomization. On the
other hand, RAs can degrade estimation efficiency due to their estimation
errors, which are not asymptotically negligible when the number of regressors
is of the same order as the sample size. Failure to account for the cost of RAs
can result in over-rejection of causal inference under the null hypothesis. To
address this issue, we develop a unified inference theory for the
regression-adjusted average treatment effect (ATE) estimator under CARs. Our
theory has two key features: (1) it ensures the exact asymptotic size under the
null hypothesis, regardless of whether the number of covariates is fixed or
diverges at most at the rate of the sample size, and (2) it guarantees weak
efficiency improvement over the ATE estimator with no adjustments."
1,1,Coarsened Bayesian VARs -- Correcting BVARs for Incorrect Specification,"Model mis-specification in multivariate econometric models can strongly
influence quantities of interest such as structural parameters, forecast
distributions or responses to structural shocks, even more so if higher-order
forecasts or responses are considered, due to parameter convolution. We propose
a simple method for addressing these specification issues in the context of
Bayesian VARs. Our method, called coarsened Bayesian VARs (cBVARs), replaces
the exact likelihood with a coarsened likelihood that takes into account that
the model might be mis-specified along important but unknown dimensions.
Coupled with a conjugate prior, this results in a computationally simple model.
As opposed to more flexible specifications, our approach avoids overfitting, is
simple to implement and estimation is fast. The resulting cBVAR performs well
in simulations for several types of mis-specification. Applied to US data,
cBVARs improve point and density forecasts compared to standard BVARs, and lead
to milder but more persistent negative effects of uncertainty shocks on output."
1,1,Penalized Likelihood Inference with Survey Data,"This paper extends three Lasso inferential methods, Debiased Lasso,
$C(\alpha)$ and Selective Inference to a survey environment. We establish the
asymptotic validity of the inference procedures in generalized linear models
with survey weights and/or heteroskedasticity. Moreover, we generalize the
methods to inference on nonlinear parameter functions e.g. the average marginal
effect in survey logit models. We illustrate the effectiveness of the approach
in simulated data and Canadian Internet Use Survey 2020 data."
1,1,Gini-stable Lorenz curves and their relation to the generalised Pareto distribution,"We study an iterative discrete information production process (IPP) where we
can extend ordered normalised vectors by new elements based on a simple affine
transformation, while preserving the predefined level of inequality, G, as
measured by the Gini index. Then, we derive the family of Lorenz curves of the
corresponding vectors and prove that it is stochastically ordered with respect
to both the sample size and G which plays the role of the uncertainty
parameter. A case study of family income data in nine countries shows a very
good fit of our model. Moreover, we show that asymptotically, we obtain all,
and only, Lorenz curves generated by a new, intuitive parametrisation of the
finite-mean Generalised Pareto Distribution (GPD) that unifies three other
families, namely: the Pareto Type II, exponential, and scaled beta ones. The
family is not only ordered with respect to the parameter G, but also, thanks to
our derivations, has a nice underlying interpretation. Our result may thus shed
new light on the genesis of this family of distributions."
1,1,Equivalence of inequality indices: Three dimensions of impact revisited,"Inequality is an inherent part of our lives: we see it in the distribution of
incomes, talents, resources, and citations, amongst many others. Its intensity
varies across different environments: from relatively evenly distributed ones,
to where a small group of stakeholders controls the majority of the available
resources. We would like to understand why inequality naturally arises as a
consequence of the natural evolution of any system. Studying simple
mathematical models governed by intuitive assumptions can bring many insights
into this problem. In particular, we recently observed (Siudem et al., PNAS
117:13896-13900, 2020) that impact distribution might be modelled accurately by
a time-dependent agent-based model involving a mixture of the rich-get-richer
and sheer chance components. Here we point out its relationship to an iterative
process that generates rank distributions of any length and a predefined level
of inequality, as measured by the Gini index.
  Many indices quantifying the degree of inequality have been proposed. Which
of them is the most informative? We show that, under our model, indices such as
the Bonferroni, De Vergottini, and Hoover ones are equivalent. Given one of
them, we can recreate the value of any other measure using the derived
functional relationships. Also, thanks to the obtained formulae, we can
understand how they depend on the sample size. An empirical analysis of a large
sample of citation records in economics (RePEc) as well as countrywise family
income data, confirms our theoretical observations. Therefore, we can safely
and effectively remain faithful to the simplest measure: the Gini index."
1,1,Generalized Automatic Least Squares: Efficiency Gains from Misspecified Heteroscedasticity Models,"It is well known that in the presence of heteroscedasticity ordinary least
squares estimator is not efficient. I propose a generalized automatic least
squares estimator (GALS) that makes partial correction of heteroscedasticity
based on a (potentially) misspecified model without a pretest. Such an
estimator is guaranteed to be at least as efficient as either OLS or WLS but
can provide some asymptotic efficiency gains over OLS if the misspecified model
is approximately correct. If the heteroscedasticity model is correct, the
proposed estimator achieves full asymptotic efficiency. The idea is to frame
moment conditions corresponding to OLS and WLS squares based on miss-specified
heteroscedasticity as a joint generalized method of moments estimation problem.
The resulting optimal GMM estimator is equivalent to a feasible GLS with
estimated weight matrix. I also propose an optimal GMM variance-covariance
estimator for GALS to account for any remaining heteroscedasticity in the
residuals."
1,1,Detection and Estimation of Structural Breaks in High-Dimensional Functional Time Series,"In this paper, we consider detecting and estimating breaks in heterogeneous
mean functions of high-dimensional functional time series which are allowed to
be cross-sectionally correlated and temporally dependent. A new test statistic
combining the functional CUSUM statistic and power enhancement component is
proposed with asymptotic null distribution theory comparable to the
conventional CUSUM theory derived for a single functional time series. In
particular, the extra power enhancement component enlarges the region where the
proposed test has power, and results in stable power performance when breaks
are sparse in the alternative hypothesis. Furthermore, we impose a latent group
structure on the subjects with heterogeneous break points and introduce an
easy-to-implement clustering algorithm with an information criterion to
consistently estimate the unknown group number and membership. The estimated
group structure can subsequently improve the convergence property of the
post-clustering break point estimate. Monte-Carlo simulation studies and
empirical applications show that the proposed estimation and testing techniques
have satisfactory performance in finite samples."
1,1,Predictive Incrementality by Experimentation (PIE) for Ad Measurement,"We present a novel approach to causal measurement for advertising, namely to
use exogenous variation in advertising exposure (RCTs) for a subset of ad
campaigns to build a model that can predict the causal effect of ad campaigns
that were run without RCTs. This approach -- Predictive Incrementality by
Experimentation (PIE) -- frames the task of estimating the causal effect of an
ad campaign as a prediction problem, with the unit of observation being an RCT
itself. In contrast, traditional causal inference approaches with observational
data seek to adjust covariate imbalance at the user level. A key insight is to
use post-campaign features, such as last-click conversion counts, that do not
require an RCT, as features in our predictive model. We find that our PIE model
recovers RCT-derived incremental conversions per dollar (ICPD) much better than
the program evaluation approaches analyzed in Gordon et al. (forthcoming). The
prediction errors from the best PIE model are 48%, 42%, and 62% of the
RCT-based average ICPD for upper-, mid-, and lower-funnel conversion outcomes,
respectively. In contrast, across the same data, the average prediction error
of stratified propensity score matching exceeds 491%, and that of
double/debiased machine learning exceeds 2,904%. Using a decision-making
framework inspired by industry, we show that PIE leads to different decisions
compared to RCTs for only 6% of upper-funnel, 7% of mid-funnel, and 13% of
lower-funnel outcomes. We conclude that PIE could enable advertising platforms
to scale causal ad measurement by extrapolating from a limited number of RCTs
to a large set of non-experimental ad campaigns."
1,1,GDP nowcasting with artificial neural networks: How much does long-term memory matter?,"In our study, we apply different statistical models to nowcast quarterly GDP
growth for the US economy. Using the monthly FRED-MD database, we compare the
nowcasting performance of the dynamic factor model (DFM) and four artificial
neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional
convolutional neural network (1D CNN), the long short-term memory network
(LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the
results from two distinctively different evaluation periods. The first (2010:Q1
-- 2019:Q4) is characterized by balanced economic growth, while the second
(2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According
to our results, longer input sequences result in more accurate nowcasts in
periods of balanced economic growth. However, this effect ceases above a
relatively low threshold value of around six quarters (eighteen months). During
periods of economic turbulence (e.g., during the COVID-19 recession), longer
training sequences do not help the models' predictive performance; instead,
they seem to weaken their generalization capability. Our results show that 1D
CNN, with the same parameters, generates accurate nowcasts in both of our
evaluation periods. Consequently, first in the literature, we propose the use
of this specific neural network architecture for economic nowcasting."
1,1,Financial Time Series Forecasting using CNN and Transformer,"Time series forecasting is important across various domains for
decision-making. In particular, financial time series such as stock prices can
be hard to predict as it is difficult to model short-term and long-term
temporal dependencies between data points. Convolutional Neural Networks (CNN)
are good at capturing local patterns for modeling short-term dependencies.
However, CNNs cannot learn long-term dependencies due to the limited receptive
field. Transformers on the other hand are capable of learning global context
and long-term dependencies. In this paper, we propose to harness the power of
CNNs and Transformers to model both short-term and long-term dependencies
within a time series, and forecast if the price would go up, down or remain the
same (flat) in the future. In our experiments, we demonstrated the success of
the proposed method in comparison to commonly adopted statistical and deep
learning methods on forecasting intraday stock price change of S&P 500
constituents."
3,1,Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series,"The real life time series are usually nonstationary, bringing a difficult
question of model adaptation. Classical approaches like ARMA-ARCH assume
arbitrary type of dependence. To avoid such bias, we will focus on recently
proposed agnostic philosophy of moving estimator: in time $t$ finding
parameters optimizing e.g. $F_t=\sum_{\tau<t} (1-\eta)^{t-\tau} \ln(\rho_\theta
(x_\tau))$ moving log-likelihood, evolving in time. It allows for example to
estimate parameters using inexpensive exponential moving averages (EMA), like
absolute central moments $E[|x-\mu|^p]$ evolving for one or multiple powers
$p\in\mathbb{R}^+$ using $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$.
Application of such general adaptive methods of moments will be presented on
Student's t-distribution, popular especially in economical applications, here
applied to log-returns of DJIA companies. While standard ARMA-ARCH approaches
provide evolution of $\mu$ and $\sigma$, here we also get evolution of $\nu$
describing $\rho(x)\sim |x|^{-\nu-1}$ tail shape, probability of extreme events
- which might turn out catastrophic, destabilizing the market."
1,1,Faster estimation of dynamic discrete choice models using index sufficiency,"Many estimators of dynamic discrete choice models with permanent unobserved
heterogeneity have desirable statistical properties but may be computationally
intensive. In this paper we propose a method to quicken estimation for a broad
class of dynamic discrete choice problems by exploiting index sufficiency.
Index sufficiency implies a set of equality constraints which restrict the
structural parameter of interest to belong in a subspace of the parameter
space. We propose an estimator that uses the equality constraints, and show it
is asymptotically equivalent to the unconstrained, computationally heavy
estimator. Since the computational gains of our proposed estimator are due to
the restriction of the parameter space to the subspace satisfying the equality
constraints, we provide a series of results on the dimension of this subspace.
Finally, we demonstrate the advantages of our approach by estimating a dynamic
model of the U.K. fast food market."
1,1,"Individual Welfare Analysis: Random Quasilinear Utility, Independence, and Confidence Bounds","We introduce a novel framework for individual-level welfare analysis. It
builds on a parametric model for continuous demand with a quasilinear utility
function, allowing for unobserved individual-product-level preference shocks.
We obtain bounds on the individual-level consumer welfare loss at any
confidence level due to a hypothetical price increase, solving a scalable
optimization problem constrained by a new confidence set under an independence
restriction. This confidence set is computationally simple, robust to weak
instruments and nonlinearity, and may have applications beyond welfare
analysis. Monte Carlo simulations and two empirical applications on gasoline
and food demand demonstrate the effectiveness of our method."
1,1,Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python,"The $\texttt{torch-choice}$ is an open-source library for flexible, fast
choice modeling with Python and PyTorch. $\texttt{torch-choice}$ provides a
$\texttt{ChoiceDataset}$ data structure to manage databases flexibly and
memory-efficiently. The paper demonstrates constructing a
$\texttt{ChoiceDataset}$ from databases of various formats and functionalities
of $\texttt{ChoiceDataset}$. The package implements two widely used models,
namely the multinomial logit and nested logit models, and supports
regularization during model estimation. The package incorporates the option to
take advantage of GPUs for estimation, allowing it to scale to massive datasets
while being computationally efficient. Models can be initialized using either
R-style formula strings or Python dictionaries. We conclude with a comparison
of the computational efficiencies of $\texttt{torch-choice}$ and
$\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the
number of covariates increases, and (3) the expansion of item sets. Finally, we
demonstrate the scalability of $\texttt{torch-choice}$ on large-scale datasets."
1,1,Heterogeneity-robust granular instruments,"Granular instrumental variables have experienced sharp growth in empirical
macro-finance. Their attraction lies in their applicability to a wide set of
economic environments like demand systems and the estimation of spillovers. I
propose a new estimator$\unicode{x2014}$called robust granular instrumental
variables (RGIV)$\unicode{x2014}$that, unlike GIV, allows for heterogeneous
responses across units to the aggregate variable, unknown shock variances, and
does not rely on skewness of the size distribution of units. Its generality
allows researchers to account for and study unit-level heterogeneity. I also
develop an overidentification test that evaluates the RGIV's compatibility with
the data and a parameter restriction test that evaluates the appropriateness of
the homogeneous coefficient assumption. In simulations, I show that RGIV
produces reliable and informative confidence intervals."
1,1,Testing for idiosyncratic Treatment Effect Heterogeneity,"This paper provides asymptotically valid tests for the null hypothesis of no
treatment effect heterogeneity. Importantly, I consider the presence of
heterogeneity that is not explained by observed characteristics, or so-called
idiosyncratic heterogeneity. When examining this heterogeneity, common
statistical tests encounter a nuisance parameter problem in the average
treatment effect which renders the asymptotic distribution of the test
statistic dependent on that parameter. I propose an asymptotically valid test
that circumvents the estimation of that parameter using the empirical
characteristic function. A simulation study illustrates not only the test's
validity but its higher power in rejecting a false null as compared to current
tests. Furthermore, I show the method's usefulness through its application to a
microfinance experiment in Bosnia and Herzegovina. In this experiment and for
outcomes related to loan take-up and self-employment, the tests suggest that
treatment effect heterogeneity does not seem to be completely accounted for by
baseline characteristics. For those outcomes, researchers could potentially try
to collect more baseline characteristics to inspect the remaining treatment
effect heterogeneity, and potentially, improve treatment targeting."
1,1,Artificial neural networks and time series of counts: A class of nonlinear INGARCH models,"Time series of counts are frequently analyzed using generalized
integer-valued autoregressive models with conditional heteroskedasticity
(INGARCH). These models employ response functions to map a vector of past
observations and past conditional expectations to the conditional expectation
of the present observation. In this paper, it is shown how INGARCH models can
be combined with artificial neural network (ANN) response functions to obtain a
class of nonlinear INGARCH models. The ANN framework allows for the
interpretation of many existing INGARCH models as a degenerate version of a
corresponding neural model. Details on maximum likelihood estimation, marginal
effects and confidence intervals are given. The empirical analysis of time
series of bounded and unbounded counts reveals that the neural INGARCH models
are able to outperform reasonable degenerate competitor models in terms of the
information loss."
1,1,Testing and Identifying Substitution and Complementarity Patterns,"This paper studies semiparametric identification of substitution and
complementarity patterns between two goods using a panel multinomial choice
model with bundles. The model allows the two goods to be either substitutes or
complements and admits heterogeneous complementarity through observed
characteristics. I first provide testable implications for the complementarity
relationship between goods. I then characterize the sharp identified set for
the model parameters and provide sufficient conditions for point
identification. The identification analysis accommodates endogenous covariates
through flexible dependence structures between observed characteristics and
fixed effects while placing no distributional assumptions on unobserved
preference shocks. My method is shown to perform more robustly than the
parametric method through Monte Carlo simulations. As an extension, I allow for
unobserved heterogeneity in the complementarity, investigate scenarios
involving more than two goods, and study a class of nonseparable utility
functions."
1,1,Endogenous Linear Regressions with Included Instrumental Variables,"We show that endogenous linear regression models can be identified without
excluded instrumental variables, based on the standard mean independence
condition and a no-multicollinearity condition on the conditional expectations
of endogenous covariates given the included exogenous covariates. Based on the
identification results, we propose two semiparametric estimators as well as a
discretization-based estimator that does not require any nonparametric
regressions. We establish their asymptotic normality, provide corresponding
variance estimators, and demonstrate via simulations the good finite-sample
performances of our proposed estimation and inference procedures. In
particular, we find that the discretization-based estimator performs remarkably
well in finite samples, while being very simple and fast to compute."
1,1,Inference on eigenvectors of non-symmetric matrices,"This paper argues that the symmetrisability condition in Tyler (1981) is not
necessary to establish asymptotic inference procedures for eigenvectors. We
establish distribution theory for a Wald and t-test for full-vector and
individual coefficient hypotheses, respectively. Our test statistics originate
from eigenprojections of non-symmetric matrices. Representing projections as a
mapping from the underlying matrix to its spectral data, we find derivatives
through analytic perturbation theory. These results demonstrate how the
analytic perturbation theory of Sun (1991) is a useful tool in multivariate
statistics and are of independent interest. As an application, we define
confidence sets for Bonacich centralities estimated from adjacency matrices
induced by directed graphs."
1,1,Under-Identification of Structural Models Based on Timing and Information Set Assumptions,"We revisit identification based on timing and information set assumptions in
structural models, which have been used in the context of production functions,
demand equations, and hedonic pricing models (e.g. Olley and Pakes (1996),
Blundell and Bond (2000)). First, we demonstrate a general under-identification
problem using these assumptions in a simple version of the Blundell-Bond
dynamic panel model. In particular, the basic moment conditions can yield
multiple discrete solutions: one at the persistence parameter in the main
equation and another at the persistence parameter governing the regressor. We
then show that the problem can persist in a broader set of models but
disappears in models under stronger timing assumptions. We then propose
possible solutions in the simple setting by enforcing an assumed sign
restriction and conclude by using lessons from our basic identification
approach to propose more general practical advice for empirical researchers."
1,1,Sensitivity Analysis in Unconditional Quantile Effects,"This paper proposes a framework to analyze the effects of counterfactual
policies on the unconditional quantiles of an outcome variable. For a given
counterfactual policy, we obtain identified sets for the effect of both
marginal and global changes in the proportion of treated individuals. To
conduct a sensitivity analysis, we introduce the quantile breakdown frontier, a
curve that (i) indicates whether a sensitivity analysis if possible or not, and
(ii) when a sensitivity analysis is possible, quantifies the amount of
selection bias consistent with a given conclusion of interest across different
quantiles. To illustrate our method, we perform a sensitivity analysis on the
effect of unionizing low income workers on the quantiles of the distribution of
(log) wages."
3,1,Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions,"We consider a setting with $N$ heterogeneous units and $p$ interventions. Our
goal is to learn unit-specific potential outcomes for any combination of these
$p$ interventions, i.e., $N \times 2^p$ causal parameters. Choosing
combinations of interventions is a problem that naturally arises in many
applications such as factorial design experiments, recommendation engines
(e.g., showing a set of movies that maximizes engagement for users),
combination therapies in medicine, selecting important features for ML models,
etc. Running $N \times 2^p$ experiments to estimate the various parameters is
infeasible as $N$ and $p$ grow. Further, with observational data there is
likely confounding, i.e., whether or not a unit is seen under a combination is
correlated with its potential outcome under that combination. To address these
challenges, we propose a novel model that imposes latent structure across both
units and combinations. We assume latent similarity across units (i.e., the
potential outcomes matrix is rank $r$) and regularity in how combinations
interact (i.e., the coefficients in the Fourier expansion of the potential
outcomes is $s$ sparse). We establish identification for all causal parameters
despite unobserved confounding. We propose an estimation procedure, Synthetic
Combinations, and establish finite-sample consistency under precise conditions
on the observation pattern. Our results imply Synthetic Combinations
consistently estimates unit-specific potential outcomes given $\text{poly}(r)
\times (N + s^2p)$ observations. In comparison, previous methods that do not
exploit structure across both units and combinations have sample complexity
scaling as $\min(N \times s^2p, \ \ r \times (N + 2^p))$. We use Synthetic
Combinations to propose a data-efficient experimental design mechanism for
combinatorial causal inference. We corroborate our theoretical findings with
numerical simulations."
3,1,On the failure of the bootstrap for Chatterjee's rank correlation,"While researchers commonly use the bootstrap for statistical inference, many
of us have realized that the standard bootstrap, in general, does not work for
Chatterjee's rank correlation. In this paper, we provide proof of this issue
under an additional independence assumption, and complement our theory with
simulation evidence for general settings. Chatterjee's rank correlation thus
falls into a category of statistics that are asymptotically normal but
bootstrap inconsistent. Valid inferential methods in this case are Chatterjee's
original proposal (for testing independence) and Lin and Han (2022)'s analytic
asymptotic variance estimator (for more general purposes)."
1,1,Point Identification of LATE with Two Imperfect Instruments,"This paper characterizes point identification results of the local average
treatment effect (LATE) using two imperfect instruments. The classical approach
(Imbens and Angrist (1994)) establishes the identification of LATE via an
instrument that satisfies exclusion, monotonicity, and independence. However,
it may be challenging to find a single instrument that satisfies all these
assumptions simultaneously. My paper uses two instruments but imposes weaker
assumptions on both instruments. The first instrument is allowed to violate the
exclusion restriction and the second instrument does not need to satisfy
monotonicity. Therefore, the first instrument can affect the outcome via both
direct effects and a shift in the treatment status. The direct effects can be
identified via exogenous variation in the second instrument and therefore the
local average treatment effect is identified. An estimator is proposed, and
using Monte Carlo simulations, it is shown to perform more robustly than the
instrumental variable estimand."
1,1,Bootstrap-Assisted Inference for Generalized Grenander-type Estimators,"Westling and Carone (2020) proposed a framework for studying the large sample
distributional properties of generalized Grenander-type estimators, a versatile
class of nonparametric estimators of monotone functions. The limiting
distribution of those estimators is representable as the left derivative of the
greatest convex minorant of a Gaussian process whose covariance kernel can be
complicated and whose monomial mean can be of unknown order (when the degree of
flatness of the function of interest is unknown). The standard nonparametric
bootstrap is unable to consistently approximate the large sample distribution
of the generalized Grenander-type estimators even if the monomial order of the
mean is known, making statistical inference a challenging endeavour in
applications. To address this inferential problem, we present a
bootstrap-assisted inference procedure for generalized Grenander-type
estimators. The procedure relies on a carefully crafted, yet automatic,
transformation of the estimator. Moreover, our proposed method can be made
``flatness robust"" in the sense that it can be made adaptive to the (possibly
unknown) degree of flatness of the function of interest. The method requires
only the consistent estimation of a single scalar quantity, for which we
propose an automatic procedure based on numerical derivative estimation and the
generalized jackknife. Under random sampling, our inference method can be
implemented using a computationally attractive exchangeable bootstrap
procedure. We illustrate our methods with examples and we also provide a small
simulation study. The development of formal results is made possible by some
technical results that may be of independent interest."
1,1,Sequential Cauchy Combination Test for Multiple Testing Problems with Financial Applications,"We introduce a simple tool to control for false discoveries and identify
individual signals when there are many tests, the test statistics are
correlated, and the signals are potentially sparse. The tool applies the Cauchy
combination test recursively on a sequence of expanding subsets of $p$-values
and is referred to as the sequential Cauchy combination test. While the
original Cauchy combination test aims for a global statement over a set of null
hypotheses by summing transformed $p$-values, the sequential version determines
which $p$-values trigger the rejection of the global null. The test achieves
strong familywise error rate control and is less conservative than existing
controlling procedures when the test statistics are dependent, leading to
higher global powers and successful detection rates. As illustrations, we
consider two popular financial econometric applications for which the test
statistics have either serial dependence or cross-sectional dependence:
monitoring drift bursts in asset prices and searching for assets with a nonzero
alpha. The sequential Cauchy combination test is a preferable alternative in
both cases in simulation settings and leads to higher detection rates than
benchmark procedures in empirics."
1,1,Uncertain Prior Economic Knowledge and Statistically Identified Structural Vector Autoregressions,"This study proposes an estimator that combines statistical identification
with economically motivated restrictions on the interactions. The estimator is
identified by (mean) independent non-Gaussian shocks and allows for
incorporation of uncertain prior economic knowledge through an adaptive ridge
penalty. The estimator shrinks towards economically motivated restrictions when
the data is consistent with them and stops shrinkage when the data provides
evidence against the restriction. The estimator is applied to analyze the
interaction between the stock and oil market. The results suggest that what is
usually identified as oil-specific demand shocks can actually be attributed to
information shocks extracted from the stock market, which explain about 30-40%
of the oil price variation."
1,1,Functional-Coefficient Quantile Regression for Panel Data with Latent Group Structure,"This paper considers estimating functional-coefficient models in panel
quantile regression with individual effects, allowing the cross-sectional and
temporal dependence for large panel observations. A latent group structure is
imposed on the heterogenous quantile regression models so that the number of
nonparametric functional coefficients to be estimated can be reduced
considerably. With the preliminary local linear quantile estimates of the
subject-specific functional coefficients, a classic agglomerative clustering
algorithm is used to estimate the unknown group structure and an
easy-to-implement ratio criterion is proposed to determine the group number.
The estimated group number and structure are shown to be consistent.
Furthermore, a post-grouping local linear smoothing method is introduced to
estimate the group-specific functional coefficients, and the relevant
asymptotic normal distribution theory is derived with a normalisation rate
comparable to that in the literature. The developed methodologies and theory
are verified through a simulation study and showcased with an application to
house price data from UK local authority districts, which reveals different
homogeneity structures at different quantile levels."
1,1,sparseDFM: An R Package to Estimate Dynamic Factor Models with Sparse Loadings,"sparseDFM is an R package for the implementation of popular estimation
methods for dynamic factor models (DFMs) including the novel Sparse DFM
approach of Mosley et al. (2023). The Sparse DFM ameliorates interpretability
issues of factor structure in classic DFMs by constraining the loading matrices
to have few non-zero entries (i.e. are sparse). Mosley et al. (2023) construct
an efficient expectation maximisation (EM) algorithm to enable estimation of
model parameters using a regularised quasi-maximum likelihood. We provide
detail on the estimation strategy in this paper and show how we implement this
in a computationally efficient way. We then provide two real-data case studies
to act as tutorials on how one may use the sparseDFM package. The first case
study focuses on summarising the structure of a small subset of quarterly CPI
(consumer price inflation) index data for the UK, while the second applies the
package onto a large-scale set of monthly time series for the purpose of
nowcasting nine of the main trade commodities the UK exports worldwide."
1,1,Forecasting Large Realized Covariance Matrices: The Benefits of Factor Models and Shrinkage,"We propose a model to forecast large realized covariance matrices of returns,
applying it to the constituents of the S\&P 500 daily. To address the curse of
dimensionality, we decompose the return covariance matrix using standard
firm-level factors (e.g., size, value, and profitability) and use sectoral
restrictions in the residual covariance matrix. This restricted model is then
estimated using vector heterogeneous autoregressive (VHAR) models with the
least absolute shrinkage and selection operator (LASSO). Our methodology
improves forecasting precision relative to standard benchmarks and leads to
better estimates of minimum variance portfolios."
1,1,"Don't (fully) exclude me, it's not necessary! Identification with semi-IVs","This paper proposes a novel approach to identify models with a discrete
endogenous variable, that I study in the general context of nonseparable models
with continuous potential outcomes. I show that nonparametric identification of
the potential outcome and selection equations, and thus of the individual
treatment effects, can be obtained with semi-instrumental variables (semi-IVs),
which are relevant but only partially excluded from the potential outcomes,
i.e., excluded from one or more potential outcome equations, but not
necessarily all. This contrasts with the full exclusion restriction imposed on
standard instrumental variables (IVs), which is stronger than necessary for
identification: IVs are only a special case of valid semi-IVs. In practice,
there is a trade-off between imposing stronger exclusion restrictions, and
finding semi-IVs with a larger support and stronger relevance assumptions.
Since, in empirical work, the main obstacle for finding a valid IV is often the
full exclusion restriction, tackling the endogeneity problem with semi-IVs
instead should be an attractive alternative."
1,1,Quasi Maximum Likelihood Estimation of High-Dimensional Factor Models: A Critical Review,"We review Quasi Maximum Likelihood estimation of factor models for
high-dimensional panels of time series. We consider two cases: (1) estimation
when no dynamic model for the factors is specified (Bai and Li, 2016); (2)
estimation based on the Kalman smoother and the Expectation Maximization
algorithm thus allowing to model explicitly the factor dynamics (Doz et al.,
2012). Our interest is in approximate factor models, i.e., when we allow for
the idiosyncratic components to be mildly cross-sectionally, as well as
serially, correlated. Although such setting apparently makes estimation harder,
we show, in fact, that factor models do not suffer of the curse of
dimensionality problem, but instead they enjoy a blessing of dimensionality
property. In particular, given an approximate factor structure, if the
cross-sectional dimension of the data, $N$, grows to infinity, we show that:
(i) identification of the model is still possible, (ii) the mis-specification
error due to the use of an exact factor model log-likelihood vanishes.
Moreover, if we let also the sample size, $T$, grow to infinity, we can also
consistently estimate all parameters of the model and make inference. The same
is true for estimation of the latent factors which can be carried out by
weighted least-squares, linear projection, or Kalman filtering/smoothing. We
also compare the approaches presented with: Principal Component analysis and
the classical, fixed $N$, exact Maximum Likelihood approach. We conclude with a
discussion on efficiency of the considered estimators."
1,1,Using Forests in Multivariate Regression Discontinuity Designs,"We discuss estimating conditional treatment effects in regression
discontinuity designs with multiple scores. While local linear regressions have
been popular in settings where the treatment status is completely described by
one running variable, they do not easily generalize to empirical applications
involving multiple treatment assignment rules. In practice, the multivariate
problem is usually reduced to a univariate one where using local linear
regressions is suitable. Instead, we propose a forest-based estimator that can
flexibly model multivariate scores, where we build two honest forests in the
sense of Wager and Athey (2018) on both sides of the treatment boundary. This
estimator is asymptotically normal and sidesteps the pitfalls of running local
linear regressions in higher dimensions. In simulations, we find our proposed
estimator outperforms local linear regressions in multivariate designs and is
competitive against the minimax-optimal estimator of Imbens and Wager (2019).
The implementation of this estimator is simple, can readily accommodate any
(fixed) number of running variables, and does not require estimating any
nuisance parameters of the data generating process."
1,1,On the Existence and Information of Orthogonal Moments For Inference,"Locally Robust (LR)/Orthogonal/Debiased moments have been proved useful with
machine learning or high dimensional first steps, but their existence has not
been investigated for general models and parameters. In this paper, we provide
a necessary and sufficient condition, referred to as Restricted Local
Non-surjectivity (RLN), for the existence of such orthogonal moments to conduct
robust inference on parameters of interest in regular semiparametric models.
Importantly, RLN does not require identification of the parameters of interest
or identification of the nuisance parameters. Thus, orthogonal moments exist
under rather general conditions. However, for orthogonal moments to be
informative for inference, the efficient Fisher Information matrix for the
parameter must be non-zero (though possibly singular). We use these results to
characterize the existence of orthogonal moments in a class of models with
Unobserved Heterogeneity (UH), and to clarify the important role played by the
support of UH for such characterization. Our results deliver functional
differencing moments as a special case, and they also extend them to general
functionals of UH. We also investigate existence of orthogonal moments and
their relevance for models defined by moment restrictions with possibly
different conditioning variables, and we characterize orthogonal moments for
heterogenous parameters in treatment effects, for sample selection models, and
for popular models of demand of differentiated products."
1,1,How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on Over 60 Replicated Studies,"Instrumental variable (IV) strategies are widely used in political science to
establish causal relationships. However, the identifying assumptions required
by an IV design are demanding, and it remains challenging for researchers to
assess their validity. In this paper, we replicate 67 papers published in three
top journals in political science during 2010-2022 and identify several
troubling patterns. First, researchers often overestimate the strength of their
IVs due to non-i.i.d. errors, such as a clustering structure. Second, the most
commonly used t-test for the two-stage-least-squares (2SLS) estimates often
severely underestimates uncertainty. Using more robust inferential methods, we
find that around 19-30% of the 2SLS estimates in our sample are underpowered.
Third, in the majority of the replicated studies, the 2SLS estimates are much
larger than the ordinary-least-squares estimates, and their ratio is negatively
correlated with the strength of the IVs in studies where the IVs are not
experimentally generated, suggesting potential violations of unconfoundedness
or the exclusion restriction. To help researchers avoid these pitfalls, we
provide a checklist for better practice."
1,1,Network log-ARCH models for forecasting stock market volatility,"This paper presents a novel dynamic network autoregressive conditional
heteroscedasticity (ARCH) model based on spatiotemporal ARCH models to forecast
volatility in the US stock market. To improve the forecasting accuracy, the
model integrates temporally lagged volatility information and information from
adjacent nodes, which may instantaneously spill across the entire network. The
model is also suitable for high-dimensional cases where multivariate ARCH
models are typically no longer applicable. We adopt the theoretical foundations
from spatiotemporal statistics and transfer the dynamic ARCH model for
processes to networks. This new approach is compared with independent
univariate log-ARCH models. We could quantify the improvements due to the
instantaneous network ARCH effects, which are studied for the first time in
this paper. The edges are determined based on various distance and correlation
measures between the time series. The performances of the alternative networks'
definitions are compared in terms of out-of-sample accuracy. Furthermore, we
consider ensemble forecasts based on different network definitions."
1,1,Standard errors when a regressor is randomly assigned,"We examine asymptotic properties of the OLS estimator when the values of the
regressor of interest are assigned randomly and independently of other
regressors. We find that the OLS variance formula in this case is often
simplified, sometimes substantially. In particular, when the regressor of
interest is independent not only of other regressors but also of the error
term, the textbook homoskedastic variance formula is valid even if the error
term and auxiliary regressors exhibit a general dependence structure. In the
context of randomized controlled trials, this conclusion holds in completely
randomized experiments with constant treatment effects. When the error term is
heteroscedastic with respect to the regressor of interest, the variance formula
has to be adjusted not only for heteroscedasticity but also for correlation
structure of the error term. However, even in the latter case, some
simplifications are possible as only a part of the correlation structure of the
error term should be taken into account. In the context of randomized control
trials, this implies that the textbook homoscedastic variance formula is
typically not valid if treatment effects are heterogenous but
heteroscedasticity-robust variance formulas are valid if treatment effects are
independent across units, even if the error term exhibits a general dependence
structure. In addition, we extend the results to the case when the regressor of
interest is assigned randomly at a group level, such as in randomized control
trials with treatment assignment determined at a group (e.g., school/village)
level."
1,1,Inference of Grouped Time-Varying Network Vector Autoregression Models,"This paper considers statistical inference of time-varying network vector
autoregression models for large-scale time series. A latent group structure is
imposed on the heterogeneous and node-specific time-varying momentum and
network spillover effects so that the number of unknown time-varying
coefficients to be estimated can be reduced considerably. A classic
agglomerative clustering algorithm with normalized distance matrix estimates is
combined with a generalized information criterion to consistently estimate the
latent group number and membership. A post-grouping local linear smoothing
method is proposed to estimate the group-specific time-varying momentum and
network effects, substantially improving the convergence rates of the
preliminary estimates which ignore the latent structure. In addition, a
post-grouping specification test is conducted to verify the validity of the
parametric model assumption for group-specific time-varying coefficient
functions, and the asymptotic theory is derived for the test statistic
constructed via a kernel weighted quadratic form under the null and alternative
hypotheses. Numerical studies including Monte-Carlo simulation and an empirical
application to the global trade flow data are presented to examine the
finite-sample performance of the developed model and methodology."
1,1,Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices,"This paper presents a new method for combining (or aggregating or ensembling)
multivariate probabilistic forecasts, taking into account dependencies between
quantiles and covariates through a smoothing procedure that allows for online
learning. Two smoothing methods are discussed: dimensionality reduction using
Basis matrices and penalized smoothing. The new online learning algorithm
generalizes the standard CRPS learning framework into multivariate dimensions.
It is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic
learning properties. We provide an in-depth discussion on possible extensions
of the algorithm and several nested cases related to the existing literature on
online forecast combination. The methodology is applied to forecasting
day-ahead electricity prices, which are 24-dimensional distributional
forecasts. The proposed method yields significant improvements over uniform
combination in terms of continuous ranked probability score (CRPS). We discuss
the temporal evolution of the weights and hyperparameters and present the
results of reduced versions of the preferred model. A fast C++ implementation
of all discussed methods is provided in the R-Package profoc."
1,1,Bootstrap based asymptotic refinements for high-dimensional nonlinear models,"We consider penalized extremum estimation of a high-dimensional, possibly
nonlinear model that is sparse in the sense that most of its parameters are
zero but some are not. We use the SCAD penalty function, which provides model
selection consistent and oracle efficient estimates under suitable conditions.
However, asymptotic approximations based on the oracle model can be inaccurate
with the sample sizes found in many applications. This paper gives conditions
under which the bootstrap, based on estimates obtained through SCAD
penalization with thresholding, provides asymptotic refinements of size \(O
\left( n^{- 2} \right)\) for the error in the rejection (coverage) probability
of a symmetric hypothesis test (confidence interval) and \(O \left( n^{- 1}
\right)\) for the error in rejection (coverage) probability of a one-sided or
equal tailed test (confidence interval). The results of Monte Carlo experiments
show that the bootstrap can provide large reductions in errors in coverage
probabilities. The bootstrap is consistent, though it does not necessarily
provide asymptotic refinements, even if some parameters are close but not equal
to zero. Random-coefficients logit and probit models and nonlinear moment
models are examples of models to which the procedure applies."
3,1,Mean-variance constrained priors have finite maximum Bayes risk in the normal location model,"Consider a normal location model $X \mid \theta \sim N(\theta, \sigma^2)$
with known $\sigma^2$. Suppose $\theta \sim G_0$, where the prior $G_0$ has
zero mean and unit variance. Let $G_1$ be a possibly misspecified prior with
zero mean and unit variance. We show that the squared error Bayes risk of the
posterior mean under $G_1$ is bounded, uniformly over $G_0, G_1, \sigma^2 > 0$."
1,1,Identifying an Earnings Process With Dependent Contemporaneous Income Shocks,"This paper proposes a novel approach for identifying coefficients in an
earnings dynamics model with arbitrarily dependent contemporaneous income
shocks. Traditional methods relying on second moments fail to identify these
coefficients, emphasizing the need for nongaussianity assumptions that capture
information from higher moments. Our results contribute to the literature on
earnings dynamics by allowing models of earnings to have, for example, the
permanent income shock of a job change to be linked to the contemporaneous
transitory income shock of a relocation bonus."
1,1,Identification- and many instrument-robust inference via invariant moment conditions,"Identification-robust hypothesis tests are commonly based on the continuous
updating objective function or its score. When the number of moment conditions
grows proportionally with the sample size, the large-dimensional weighting
matrix prohibits the use of conventional asymptotic approximations and the
behavior of these tests remains unknown. We show that the structure of the
weighting matrix opens up an alternative route to asymptotic results when,
under the null hypothesis, the distribution of the moment conditions is
reflection invariant. In a heteroskedastic linear instrumental variables model,
we then establish asymptotic normality of conventional tests statistics under
many instrument sequences. A key result is that the additional terms that
appear in the variance are negative. Revisiting a study on the elasticity of
substitution between immigrant and native workers where the number of
instruments is over a quarter of the sample size, the many instrument-robust
approximation indeed leads to substantially narrower confidence intervals."
1,1,Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm,"In non-asymptotic statistical inferences, variance-type parameters of
sub-Gaussian distributions play a crucial role. However, direct estimation of
these parameters based on the empirical moment generating function (MGF) is
infeasible. To this end, we recommend using a sub-Gaussian intrinsic moment
norm [Buldygin and Kozachenko (2000), Theorem 1.3] through maximizing a series
of normalized moments. Importantly, the recommended norm can not only recover
the exponential moment bounds for the corresponding MGFs, but also lead to
tighter Hoeffding's sub-Gaussian concentration inequalities. In practice,
{\color{black} we propose an intuitive way of checking sub-Gaussian data with a
finite sample size by the sub-Gaussian plot}. Intrinsic moment norm can be
robustly estimated via a simple plug-in approach. Our theoretical results are
applied to non-asymptotic analysis, including the multi-armed bandit."
1,1,Inflation forecasting with attention based transformer neural networks,"Inflation is a major determinant for allocation decisions and its forecast is
a fundamental aim of governments and central banks. However, forecasting
inflation is not a trivial task, as its prediction relies on low frequency,
highly fluctuating data with unclear explanatory variables. While classical
models show some possibility of predicting inflation, reliably beating the
random walk benchmark remains difficult. Recently, (deep) neural networks have
shown impressive results in a multitude of applications, increasingly setting
the new state-of-the-art. This paper investigates the potential of the
transformer deep neural network architecture to forecast different inflation
rates. The results are compared to a study on classical time series and machine
learning models. We show that our adapted transformer, on average, outperforms
the baseline in 6 out of 16 experiments, showing best scores in two out of four
investigated inflation rates. Our results demonstrate that a transformer based
neural network can outperform classical regression and machine learning models
in certain inflation rates and forecasting horizons."
1,1,Counterfactual Copula and Its Application to the Effects of College Education on Intergenerational Mobility,"This paper proposes a nonparametric estimator of the counterfactual copula of
two outcome variables that would be affected by a policy intervention. The
proposed estimator allows policymakers to conduct ex-ante evaluations by
comparing the estimated counterfactual and actual copulas as well as their
corresponding measures of association. Asymptotic properties of the
counterfactual copula estimator are established under regularity conditions.
These conditions are also used to validate the nonparametric bootstrap for
inference on counterfactual quantities. Simulation results indicate that our
estimation and inference procedures perform well in moderately sized samples.
Applying the proposed method to studying the effects of college education on
intergenerational income mobility under two counterfactual scenarios, we find
that while providing some college education to all children is unlikely to
promote mobility, offering a college degree to children from less educated
families can significantly reduce income persistence across generations."
1,1,Distributional Vector Autoregression: Eliciting Macro and Financial Dependence,"Vector autoregression is an essential tool in empirical macroeconomics and
finance for understanding the dynamic interdependencies among multivariate time
series. In this study, we expand the scope of vector autoregression by
incorporating a multivariate distributional regression framework and
introducing a distributional impulse response function, providing a
comprehensive view of dynamic heterogeneity. We propose a straightforward yet
flexible estimation method and establish its asymptotic properties under weak
dependence assumptions. Our empirical analysis examines the conditional joint
distribution of GDP growth and financial conditions in the United States, with
a focus on the global financial crisis. Our results show that tight financial
conditions lead to a multimodal conditional joint distribution of GDP growth
and financial conditions, and easing financial conditions significantly impacts
long-term GDP growth, while improving the GDP growth during the global
financial crisis has limited effects on financial conditions."
1,1,Inference on Optimal Dynamic Policies via Softmax Approximation,"Estimating optimal dynamic policies from offline data is a fundamental
problem in dynamic decision making. In the context of causal inference, the
problem is known as estimating the optimal dynamic treatment regime. Even
though there exists a plethora of methods for estimation, constructing
confidence intervals for the value of the optimal regime and structural
parameters associated with it is inherently harder, as it involves non-linear
and non-differentiable functionals of un-known quantities that need to be
estimated. Prior work resorted to sub-sample approaches that can deteriorate
the quality of the estimate. We show that a simple soft-max approximation to
the optimal treatment regime, for an appropriately fast growing temperature
parameter, can achieve valid inference on the truly optimal regime. We
illustrate our result for a two-period optimal dynamic regime, though our
approach should directly extend to the finite horizon case. Our work combines
techniques from semi-parametric inference and $g$-estimation, together with an
appropriate triangular array central limit theorem, as well as a novel analysis
of the asymptotic influence and asymptotic bias of softmax approximations."
1,1,Identification of Ex Ante Returns Using Elicited Choice Probabilities,"This paper studies the identification of perceived ex ante returns in the
context of binary human capital investment decisions. The environment is
characterised by uncertainty about future outcomes, with some uncertainty being
resolved over time. In this context, each individual holds a probability
distribution over different levels of returns. The paper uses the hypothetical
choice methodology to identify nonparametrically the population distribution of
several individual-specific distribution parameters, which are crucial for
counterfactual policy analyses. The empirical application estimates perceived
returns on overstaying for Afghan asylum seekers in Germany and evaluates the
effect of assisted voluntary return policies."
1,1,EnsembleIV: Creating Instrumental Variables from Ensemble Learners for Robust Statistical Inference,"Despite increasing popularity in empirical studies, the integration of
machine learning generated variables into regression models for statistical
inference suffers from the measurement error problem, which can bias estimation
and threaten the validity of inferences. In this paper, we develop a novel
approach to alleviate associated estimation biases. Our proposed approach,
EnsembleIV, creates valid and strong instrumental variables from weak learners
in an ensemble model, and uses them to obtain consistent estimates that are
robust against the measurement error problem. Our empirical evaluations, using
both synthetic and real-world datasets, show that EnsembleIV can effectively
reduce estimation biases across several common regression specifications, and
can be combined with modern deep learning techniques when dealing with
unstructured data."
1,1,Censored Quantile Regression with Many Controls,"This paper develops estimation and inference methods for censored quantile
regression models with high-dimensional controls. The methods are based on the
application of double/debiased machine learning (DML) framework to the censored
quantile regression estimator of Buchinsky and Hahn (1998). I provide valid
inference for low-dimensional parameters of interest in the presence of
high-dimensional nuisance parameters when implementing machine learning
estimators. The proposed estimator is shown to be consistent and asymptotically
normal. The performance of the estimator with high-dimensional controls is
illustrated with numerical simulation and an empirical application that
examines the effect of 401(k) eligibility on savings."
1,1,"Deterministic, quenched or annealed? Differences in the parameter estimation of heterogeneous network models","Analysing weighted networks requires modelling the binary and weighted
properties simultaneously. We highlight three approaches for estimating the
parameters responsible for them: econometric techniques treating topology as
deterministic and statistical techniques either ensemble-averaging parameters
or maximising an averaged likelihood over the topological randomness. In
homogeneous models, equivalence holds; in heterogeneous network models, the
local disorder breaks it, in a way reminiscent of the difference between
`quenched' and `annealed' averages in the physics of disordered systems."
1,1,Fast Forecasting of Unstable Data Streams for On-Demand Service Platforms,"On-demand service platforms face a challenging problem of forecasting a large
collection of high-frequency regional demand data streams that exhibit
instabilities. This paper develops a novel forecast framework that is fast and
scalable, and automatically assesses changing environments without human
intervention. We empirically test our framework on a large-scale demand data
set from a leading on-demand delivery platform in Europe, and find strong
performance gains from using our framework against several industry benchmarks,
across all geographical regions, loss functions, and both pre- and post-Covid
periods. We translate forecast gains to economic impacts for this on-demand
service platform by computing financial gains and reductions in computing
costs."
1,1,Constructing High Frequency Economic Indicators by Imputation,"Monthly and weekly economic indicators are often taken to be the largest
common factor estimated from high and low frequency data, either separately or
jointly. To incorporate mixed frequency information without directly modeling
them, we target a low frequency diffusion index that is already available, and
treat high frequency values as missing. We impute these values using multiple
factors estimated from the high frequency data. In the empirical examples
considered, static matrix completion that does not account for serial
correlation in the idiosyncratic errors yields imprecise estimates of the
missing values irrespective of how the factors are estimated. Single equation
and systems-based dynamic procedures yield imputed values that are closer to
the observed ones. This is the case in the counterfactual exercise that imputes
the monthly values of consumer sentiment series before 1978 when the data was
released only on a quarterly basis. This is also the case for a weekly version
of the CFNAI index of economic activity that is imputed using seasonally
unadjusted data. The imputed series reveals episodes of increased variability
of weekly economic information that are masked by the monthly data, notably
around the 2014-15 collapse in oil prices."
1,1,Adaptive Estimation of Intersection Bounds: a Classification Approach,"This paper studies averages of intersection bounds -- the bounds defined by
the infimum of a collection of regression functions -- and other similar
functionals of these bounds, such as averages of saddle values. Examples of
such parameters are Frechet-Hoeffding bounds, Makarov (1981) bounds on
distributional effects. The proposed estimator classifies covariate values into
the regions corresponding to the identity of the binding regression function
and takes the sample average. The paper shows that the proposed moment function
is insensitive to first-order classification mistakes, enabling various
nonparametric and regularized/machine learning classifiers in the first
(classification) step. The result is generalized to cover bounds on the values
of linear programming problems and best linear predictor of intersection
bounds."
1,1,$21^{st}$ Century Statistical Disclosure Limitation: Motivations and Challenges,"This chapter examines the motivations and imperatives for modernizing how
statistical agencies approach statistical disclosure limitation for official
data product releases. It discusses the implications for agencies' broader data
governance and decision-making, and it identifies challenges that agencies will
likely face along the way. In conclusion, the chapter proposes some principles
and best practices that we believe can help guide agencies in navigating the
transformation of their confidentiality programs."
1,1,Generalized Cumulative Shrinkage Process Priors with Applications to Sparse Bayesian Factor Analysis,"The paper discusses shrinkage priors which impose increasing shrinkage in a
sequence of parameters. We review the cumulative shrinkage process (CUSP) prior
of Legramanti et al. (2020), which is a spike-and-slab shrinkage prior where
the spike probability is stochastically increasing and constructed from the
stick-breaking representation of a Dirichlet process prior. As a first
contribution, this CUSP prior is extended by involving arbitrary stick-breaking
representations arising from beta distributions. As a second contribution, we
prove that exchangeable spike-and-slab priors, which are popular and widely
used in sparse Bayesian factor analysis, can be represented as a finite
generalized CUSP prior, which is easily obtained from the decreasing order
statistics of the slab probabilities. Hence, exchangeable spike-and-slab
shrinkage priors imply increasing shrinkage as the column index in the loading
matrix increases, without imposing explicit order constraints on the slab
probabilities. An application to sparse Bayesian factor analysis illustrates
the usefulness of the findings of this paper. A new exchangeable spike-and-slab
shrinkage prior based on the triple gamma prior of Cadonna et al. (2020) is
introduced and shown to be helpful for estimating the unknown number of factors
in a simulation study."
0,1,Price Changes and Welfare Analysis: Measurement under Individual Heterogeneity,"Measuring the welfare impact of price changes on consumers is pivotal in
economic analyses. Researchers often measure these impacts with cross sectional
data, where every consumer is observed only once. The representative agent (RA)
approach, which assumes all observations stem from a single agent, may lead to
biased estimates when agents preferences are heterogeneous. We show how to use
the higher moments of demand to improve these estimates. In fact, the variance
alone captures much of the bias in the RA approach. Our approach also enables
inference on the distribution of welfare changes. We then leverage our approach
to obtain conditions moments of demand must satisfy to arise from a population
of utility maximizers and deliver a characterization of rationality for the two
good case. Using the UK Household Budget Survey, we apply our methodology to
estimate the welfare impact of a 10 percent transport price increase and find
that the RA approach understates the welfare impact by 27 percent."
1,1,Disentangling Structural Breaks in High Dimensional Factor Models,"We disentangle structural breaks in dynamic factor models by establishing a
projection based equivalent representation theorem which decomposes any break
into a rotational change and orthogonal shift. Our decomposition leads to the
natural interpretation of these changes as a change in the factor variance and
loadings respectively, which allows us to formulate two separate tests to
differentiate between these two cases, unlike the pre-existing literature at
large. We derive the asymptotic distributions of the two tests, and demonstrate
their good finite sample performance. We apply the tests to the FRED-MD dataset
focusing on the Great Moderation and Global Financial Crisis as candidate
breaks, and find evidence that the Great Moderation may be better characterised
as a break in the factor variance as opposed to a break in the loadings,
whereas the Global Financial Crisis is a break in both. Our empirical results
highlight how distinguishing between the breaks can nuance the interpretation
attributed to them by existing methods."
3,2,Local vs. global Lipschitz geometry,"In this article, we prove that for a definable set in an o-minimal structure
with connected link (at 0 or infinity), the inner distance of the link is
equivalent to the inner distance of the set restrict to the link. As
consequences, we obtain: (1) a definable set, with connected link at infinity,
is LNE at infinity if and only if it is LLNE at infinity; (2) a definable set
is LNE at infinity if and only if its stereographic modification is LNE at the
North Pole; (3) a connected definable set is LNE if and only if its
stereographic modification is LNE; and under certain extra conditions we prove
that: (4) two definable sets are definably inner (resp. outer) lipeomorphic if
and only if their stereographic modifications are definably inner (resp. outer)
lipeomorphic if and only if their inversions are definably inner (resp. outer)
lipeomorphic. Moreover, we also prove that two sets in Euclidean spaces, not
necessarily definables in an o-minimal structure, are outer lipeomorphic if and
only if their stereographic modifications are outer lipeomorphic if and only if
their inversions are outer lipeomorphic."
3,2,The strongly robust simplicial complex of monomial curves,"To every simple toric ideal $I_T$ one can associate the strongly robust
simplicial complex $\Delta _T$, which determines the strongly robust property
for all ideals that have $I_T$ as their bouquet ideal. We show that for the
simple toric ideals of monomial curves in $\mathbb{A}^{s}$, the strongly robust
simplicial complex $\Delta _T$ is either $\{\emptyset \}$ or contains exactly
one 0-dimensional face. In the case of monomial curves in $\mathbb{A}^{3}$, the
strongly robust simplicial complex $\Delta _T$ contains one 0-dimensional face
if and only if the toric ideal $I_T$ is a complete intersection ideal with
exactly two Betti degrees. Finally, we provide a construction to produce
infinitely many strongly robust ideals with bouquet ideal the ideal of a
monomial curve and show that they are all produced this way."
3,2,Higher nearby cycles and central sheaves on affine flag varieties,"In this paper we generalize and study a notion of (unipotent) nearby cycles
over a higher dimensional base based on Be\u{i}linson's description of
unipotent nearby cycles, following an idea of Gaitsgory. This generalization,
in the setting of affine Grassmannians, is required in recent work of
Bezrukavnikov-Braverman-Finkelberg-Kazhdan."
3,2,Finiteness of cohomology for pro-locally proper maps,"We introduce a notion of proper morphism for schematic finite spaces and
prove the analogue of Grothendieck's finiteness theorem for it by means of the
classic result for schemes and general descent arguments. This result also
generalizes the class of morphisms of schemes for which the conclusion of the
aforementioned Theorem holds. The key is giving a weaker definition of locally
finitely presented morphisms."
3,2,On the geometry of the anti-canonical bundle of the Bott-Samelson-Demazure-Hansen varieties,"Let $G$ be a semi-simple simply connected algebraic group over the field
$\mathbb{C}$ of complex numbers. Let $T$ be a maximal torus of $G,$ and let $W$
be the Weyl group of $G$ with respect to $T$. Let $Z(w,\, \underline{i})$ be
the Bott-Samelson-Demazure-Hansen variety corresponding to a tuple
$\underline{i}$ associated to a reduced expression of an element $w \,\in\, W.$
We prove that for the tuple $\underline{i}$ associated to any reduced
expression of a minuscule Weyl group element $w,$ the anti-canonical line
bundle on $Z(w,\,\underline{i})$ is globally generated. As consequence, we
prove that $Z(w,\,\underline{i})$ is weak Fano.
  Assume that $G$ is a simple algebraic group whose type is different from
$A_2.$ Let $S\,=\,\{\alpha_{1},\,\cdots,\,\alpha_{n}\}$ be the set of simple
roots. Let $w$ be such that support of $w$ is equal to $S.$ We prove that
$Z(w,\,\underline{i})$ is Fano for the tuple $\underline{i}$ associated to any
reduced expression of $w$ if and only if $w$ is a Coxeter element and
$w^{-1}(\sum_{t=1}^{n}\alpha_{t})\,\in\, -S$."
3,2,Geometric local systems on the projective line minus four points,"Let $J(m)$ be an $m\times m$ Jordan block with eigenvalue $1$. For
$\lambda\in \mathbb{C}\setminus\{0,1\}$, we explicitly construct all rank $2$
local systems of geometric origin on $\mathbb{P}^1\setminus\{0,1,\lambda,
\infty\}$, with local monodromy conjugate to $J(2)$ at $0,1,\lambda$ and
conjugate to $-J(2)$ at $\infty$. The construction relies on Katz's middle
convolution operation. We use our construction to prove two conjectures of
Sun-Yang-Zuo (one of which was proven earlier by Lin-Sheng-Wang; the other was
proven independently from us by Yang-Zuo)."
3,2,Torus orbit closures and 1-strip-less tableaux,"We compare two formulas for the class of a generic torus orbit closure on the
Grassmannian, due to Klyachko and Berget-Fink. The naturally emerging
combinatorial objects are semi-standard fillings we call 1-strip-less tableaux."
3,2,On Euler-homogeneity for free divisors,"In 2002, it was conjectured that a free divisor satisfying the so-called
Logarithmic Comparison Theorem must be strongly Euler-homogeneous and it was
proved for the two-dimensional case. Later, in 2006, it was shown that the
conjecture is also true in dimension three, but, today, the answer for the
general case remains unknown. In this paper, we use the decomposition of a
singular derivation as the sum of a semisimple and a topologically nilpotent
derivation that commute in order to deal with this problem. By showing that
this decomposition preserves the property of being logarithmic, we are able to
give alternative proofs for the low-dimensional known cases."
3,2,Mittag-Leffler modules and Frobenius,"We systematically study the intersection flatness and Ohm-Rush properties for
modules over a commutative ring, drawing inspiration from the work of Ohm and
Rush and of Hochster and Jeffries. We then use our newfound understanding of
these properties to deduce consequences for the Frobenius map. We establish new
structural results for modules that are intersection flat/Ohm-Rush by
exhibiting intimate connections between these notions and the seminal work of
Raynaud and Gruson on Mittag-Leffler modules. In particular, we develop a
theory of Ohm-Rush modules that is parallel to the theory of Mittag-Leffler
modules. Our motivation for doing so is that the Ohm-Rush condition is weaker
than intersection flatness, and it will be shown in future work that one only
needs the Ohm-Rush property of Frobenius on an excellent regular ring to prove
the existence of test elements for homomorphic images of the ring. We also
obtain descent and local-to-global results for intersection flat/Ohm-Rush
modules. Our investigations reveal a pleasing picture for flat modules over a
complete local ring, in which case many otherwise distinct properties coincide.
Using these results, we characterize when a regular local ring has intersection
flat Frobenius in terms of a purity condition on the relative Frobenius of the
completion map, we provide a new characterization of excellence for DVRs in
prime characteristic, we prove new cases of intersection flatness of Frobenius,
we prove descent and ascent results for intersection flatness and Ohm-Rush
properties for Frobenius, and we show that proving the existence of test
elements for homomorphic images of excellent regular rings reduces to
exhibiting openness of pure loci for certain simple module maps. We study this
openness of loci problem in a special case and show that the Frobenius of any
excellent regular ring of prime characteristic is close to being Ohm-Rush."
3,2,Curve counting on the Enriques surface and the Klemm-Mario formula,"We determine the Gromov-Witten invariants of the local Enriques surfaces for
all genera and curve classes and prove the Klemm-Mari\~{n}o formula. In
particular, we show that the generating series of genus $1$ invariants of the
Enriques surface is the Fourier expansion of a certain power of Borcherds
automorphic form on the moduli space of Enriques surfaces. We also determine
all Vafa-Witten invariants of the Enriques surface.
  The proof uses the correspondence between Gromov-Witten and
Pandharipande-Thomas theory. On the Gromov-Witten side we prove the relative
Gromov-Witten potentials of an elliptic Enriques surfaces are quasi-Jacobi
forms and satisfy a holomorphic anomaly equation. On the sheaf side, we relate
the Pandharipande-Thomas invariants of the Enriques-Calabi-Yau threefold in
fiber classes to the $2$-dimensional Donaldson-Thomas invariants by a version
of Toda's formula for local K3 surfaces. Altogether, we obtain sufficient
modular constraints to determine all invariants from basic geometric
computations."
3,2,Automorphisms of K3 surfaces and cyclotomic polynomials,"Let X be a complex projective K3 surface, and let T(X) be its transcendental
lattice; the characteristic polynomials of the isometries of T(X) induced by
automorphisms of X are powers of cyclotomic polynomials. Which powers of
cyclotomic polynomials occur ? The aim of this note is to answer this question,
as well as related ones, and give an alternative approach to some results of
Kondo, Machida, Oguiso, Vorontsov, Xiao and Zhang."
3,2,Mixed Hodge structures and Siegel operators,"In this paper we study mixed Hodge structures on the cohomology of locally
symmetric varieties and give an application to modular forms. We first prove
vanishing of various Hodge numbers around the roof of the Hodge triangle. In
the middle degree, this especially implies that the length of the weight
filtration on the first Hodge subspace reduces to the Q-rank of the group. We
prove that this weight filtration coincides with the corank filtration on the
space of modular forms of canonical weight defined by the Siegel operators, and
calculate its graded quotients. As a consequence, we deduce surjectivity of the
total Siegel operators in many cases."
3,2,On the Hessian of cubic hypersurfaces,"In this paper, we analyze the Hessian locus associated to a general cubic
hypersurface, by describing for every $n$ its singular locus and its
desingularization. The strategy is based on strong connections between the
Hessian and the quadrics defined as partial derivatives of the cubic
polynomial. In particular, we focus our attention on the singularities of the
Hessian hypersurface associated to the general cubic fourfold. It turns out to
be a minimal surface of general type: its analysis is developed by exploiting
the nature of this surface as a degeneracy locus of a symmetric vector bundle
map and by describing an unramified double cover, which is constructed in a
more general setting."
1,2,Two-step Newton's method for deflation-one singular zeros of analytic systems,"We propose a two-step Newton's method for refining an approximation of a
singular zero whose deflation process terminates after one step, also known as
a deflation-one singularity. Given an isolated singular zero of a square
analytic system, our algorithm exploits an invertible linear operator obtained
by combining the Jacobian and a projection of the Hessian in the direction of
the kernel of the Jacobian. We prove the quadratic convergence of the two-step
Newton method when it is applied to an approximation of a deflation-one
singular zero. Also, the algorithm requires a smaller size of matrices than the
existing methods, making it more efficient. We demonstrate examples and
experiments to show the efficiency of the method."
3,2,Moduli spaces of stable objects in Enriques categories,"We study moduli spaces of stable objects in Enriques categories by exploiting
their relation to moduli spaces of stable objects in associated K3 categories.
In particular, we settle the nonemptiness problem for moduli spaces of stable
objects in the Kuznetsov components of several interesting classes of Fano
varieties, and deduce the nonemptiness of fixed loci of certain antisymplectic
involutions on modular hyperk\""{a}hler varieties."
3,2,Moduli of objects in finite length abelian categories,"We construct moduli spaces of objects in an abelian categories satisfying
some finiteness hypotheses. Our approach is based on the work of Artin-Zhang
and the intrinsic construction of moduli spaces for stacks developed by
Alper-Halpern-Leistner-Heinloth."
3,2,Construction of Arithmetic Teichmuller Spaces II$\frac{1}{2}$: Deformations of number fields,"This paper lays the foundation of the \textit{Theory of Arithmetic
Teichmuller Spaces of Number Fields} by explicitly constructing many
arithmetically inequivalent avatars of a fixed number field. This paper also
constructs a topological space of such avatars and describes its symmetries.
Notably amongst these symmetries is a global Frobenius morphism which changes
the avatar of the number field! The existence of such avatars has been
suggested and used by Shinichi Mochizuki in his work on the arithmetic Vojta
and Szpiro conjectures. The key advantage of my approach is that one can
quantify the difference between two inequivalent avatars and this renders my
theory fundamentally and quantitatively more precise than Mochizuki's approach.
In the appendix, I provide a discussion of the proofs of the geometric Szpiro
Conjectures due \cite{bogomolov00} and \cite{zhang01} from the point of view
this paper."
3,2,The cyclic Deligne conjecture and Calabi-Yau structures,"The Deligne conjecture (many times a theorem) endows Hochschild cochains of a
linear category with the structure of an $E_2$-algebra, that is, of an algebra
over the little 2-disks operad. In this paper, we prove the cyclic Deligne
conjecture, stating that for a linear category equipped with a Calabi-Yau
structure (a kind of non-commutative orientation), the Hochschild cochains is
endowed with the finer structure of a framed $E_2$-algebra, that is, of a
circle-equivariant algebra over the little 2-disks operad. Our approach applies
simultaneously to both smooth and proper linear categories, as well as to
linear functors equipped with a relative Calabi-Yau structure, and works for a
very general notion of linear category, including any dualizable presentable
$\infty$-category. As a particular application, given a compact oriented
manifold with boundary $\partial M \subset M$, our construction gives
chain-level genus zero string topology operations on the relative loop homology
$H_{*}(LM,L\partial M)$."
3,2,Addition-deletion theorems for the Solomon-Terao polynomials and $B$-sequences of hyperplane arrangements,"We prove the addition-deletion theorems for the Solomon-Terao polynomials,
which have two important specializations. Namely, one is to the characteristic
polynomials of hyperplane arangements, and the other to the Poincar\`{e}
polynomials of the regular nilpotent Hessenberg varieties. One of the main
tools to show them is the free surjection theorem which confirms the right
exactness of several important exact sequences among logarithmic modules.
Moreover, we introduce a generalized polynomial $B$-theory to the higher order
logarithmic modules, whose origin was due to Terao."
3,2,On the Manin-Mumford Theorem for Algebraic Groups,"We describe the Zariski-closure of sets of torsion points in connected
algebraic groups. This is a generalization of the Manin-Mumford conjecture for
commutative algebraic groups proved by Hindry. He proved that every subset with
Zariski-dense torsion points is the finite union of torsion-translates of
algebraic subgroups. We formulate and prove an analogous theorem for arbitrary
connected algebraic groups.
  We also define a canonical height on connected algebraic groups that
coincides with a N\'eron-Tate height if $G$ is a (semi-) abelian variety. This
motivates a generalization of the Bogomolov conjecture to arbitrary connected
algebraic groups defined over a number field. We prove such a generalization as
well."
3,2,Higher Genus Quantum $K$--theory,"We prove genus $g$ invariants in quantum $K$-theory are determined by genus
zero invariants of a smooth stack in the spirit of K.~Costello's result in
Gromov--Witten theory."
3,2,On Grothendieck's section conjecture for curves of index $1$,"We prove that every hyperbolic curve with a faithful action of a non-cyclic
$p$-group (with a few exceptions if $p=2$) has a twisted form of index $1$
which satisfies Grothendieck's section conjecture. Furthermore, we prove that
for every hyperbolic curve $S$ over a field $k$ finitely generated over
$\mathbb{Q}$ there exists a finite extension $K/k$ and a finite \'etale cover
$C\to S_{K}$ such that $C$ satisfies the conjecture."
3,2,Bloch's conjecture for (anti-)autoequivalences on K3 surfaces,"In this paper, we study Bloch's conjecture for zero cycles on K3 surfaces and
hyper-K\""ahler varieties. We prove Bloch's conjecture for reflexive
autoequivalences on K3 surfaces. This confirms Bloch's conjecture for all
(anti)-symplectic autoequivalences of a K3 surface with Picard number $>2$. As
a consequence, we prove Bloch's conjecture for (anti)-symplectic birational
automorphisms on Bridgeland moduli space of a K3 surface with Picard number $>
2$. Furthermore, we generalize Huybrechts' work in \cite{Huy12} to twisted K3
surfaces which asserts that the action of autoequivalences of a twisted K3
surface on the Chow group is determined by its action on the cohomology group.
This allows us to prove Bloch's conjecture for symplectic birational
automorphisms on arbitrary $K3^{[n]}$-type hyper-K\""ahler varieties preserving
a birational Lagrangian fibration. We also establish an anti-symplectic version
provided Voisin's filtration is the same as the filtration introduced in
\cite{SYZ20}. Finally, we prove the constant cycle property for the fixed loci
of anti-symplectic involutions on hyper-K\""ahler variety of $K3^{[n]}$-type if
$n\leq 2$ or the N\'eron-Severi invariant sublattice has rank $>1$."
3,2,Kernels of linear maps: A generalization of Duistermaat and Van der Kallens theorem,"The theorem of Duistermaat and Van der Kallen from 1998 proved the first case
of the Mathieu conjecture. Using the theory of Mathieu-Zhao spaces, we can
reformulate this theorem as $\operatorname{Ker} L$ is a Mathieu-Zhao space
where $L$ is the linear map
  \begin{align*} L\colon {\bf C}[X_1,\ldots,X_n,X_1^{-1},\ldots,X_n^{-1}] \to
C,\ f \mapsto f_0\end{align*}. In this paper, we generalize this result (for $n
= 1$) to all non-trivial linear maps $L\colon C[X,X^{-1}] \to C$ such that
$\{X^n \mid |n|\geq N\} \subset \operatorname{Ker} L$ for some $N \geq 1$."
3,2,On rank 3 instanton bundles on $\mathbb{P}^3$,"We investigate rank $3$ instanton vector bundles on $\mathbb{P}^3$ of charge
$n$ and its correspondence with rational curves of degree $n+3$. For $n=2$ we
present a correspondence between stable rank $3$ instanton bundles and stable
rank $2$ reflexive linear sheaves of Chern classes $(c_1,c_2,c_3)=(-1,3,3)$ and
we use this correspondence to compute the dimension of the family of stable
rank $3$ instanton bundles of charge $2$. Finally, we use the results above to
prove that the moduli space of rank $3$ instanton bundles on $\mathbb{P}^3$ of
charge $2$ coincides with the moduli space of rank $3$ stable locally free
sheaves on $\mathbb{P}^3$ of Chern classes $(c_1,c_2,c_3)=(0,2,0)$. This moduli
space is irreducible, has dimension 16 and its generic point corresponds to a
\textcolor{black}{generalized} t`Hooft instanton bundle."
3,2,Non-abelian cohomology of universal curves in positive characteristic,"In this paper, we will compute the non-abelian cohomology of the universal
complete curve in positive characteristic. This extends Hain's result on the
non-abelian cohomology of generic curves in characteristic zero to positive
characteristics. Furthermore, we will prove that the exact sequence of etale
fundamental groups of the universal n-punctured curve in positive
characteristic does not split."
3,2,Existence of the Shafarevich morphism for semisimple local systems on quasi-projective varieties,"Let X be a normal connected complex algebraic variety equipped with a
semisimple complex representation of its fundamental group. Then, under a
maximality assumption, we prove that the covering space of X associated to the
kernel of the representation has a proper surjective holomorphic map with
connected fibres onto a normal analytic space with no positive-dimensional
compact analytic subspace."
3,2,The structure of the moduli of gauged maps from a smooth curve,"For a reductive group $G$, Harder-Narasimhan theory gives a structure theorem
for principal $G$ bundles on a smooth projective curve $C$. A bundle is either
semistable, or it admits a canonical parabolic reduction whose associated Levi
bundle is semistable. We extend this structure theorem by constructing a
$\Theta$-stratification of the moduli stack of gauged maps from $C$ to a
projective-over-affine $G$-variety $X$. The open stratum coincides with the
previously studied moduli of Mundet semistable maps, and in special cases
coincides with the moduli of stable quasi-maps. As an application of the
stratification, we provide a formula for K-theoretic gauged Gromov-Witten
invariants when $X$ is an arbitrary linear representation of $G$. This can be
viewed as a generalization of the Verlinde formula for moduli spaces of
decorated principal bundles. We establish our main technical results for smooth
families of curves over an arbitrary Noetherian base. Our proof develops an
infinite-dimensional analog of geometric invariant theory and applies the
theory of optimization on degeneration fans."
3,2,The relative Green-Griffiths-Lang conjecture for families of varieties of maximal Albanese dimension,"We propose a generalization of the Green-Griffiths-Lang conjecture to the
relative setting and prove that a strong form of it holds for families of
varieties of maximal Albanese dimension. A key step of the proof consists in a
truncated second main theorem type estimate in Nevanlinna theory for families
of abelian varieties."
3,2,Hecke operators for curves over non-archimedean local fields and related finite rings,"We study Hecke operators associated with curves over a non-archimedean local
field $K$ and over the rings $O/{\mathfrak m}^N$, where $O\subset K$ is the
ring of integers. Our main result is commutativity of a certain ``small"" local
Hecke algebra over $O/{\mathfrak m}^N$, associated with a connected split
reductive group $G$ such that $[G,G]$ is simple and simpy connected. The proof
uses a Hecke algebra associated with $G(K(\!(t)\!))$ and a global argument
involving $G$-bundles on curves."
3,2,Chabauty--Kim and the Section Conjecture for locally geometric sections,"Let $X$ be a smooth projective curve of genus $\geq2$ over a number field. A
natural variant of Grothendieck's Section Conjecture postulates that every
section of the fundamental exact sequence for $X$ which everywhere locally
comes from a point of $X$ in fact globally comes from a point of $X$. We show
that $X/\mathbb{Q}$ satisfies this version of the Section Conjecture if it
satisfies Kim's Conjecture for almost all choices of auxiliary prime $p$, and
give the appropriate generalisation to $S$-integral points on hyperbolic
curves. This gives a new ""computational"" strategy for proving instances of this
variant of the Section Conjecture, which we carry out for the thrice-punctured
line over $\mathbb{Z}[1/2]$."
3,2,On Chern classes of Lagrangian fibered hyper-Khler manifolds,"We study the rank stratification for the differential of a Lagrangian
fibration over a smooth basis. We also introduce and study the notion of
Lagrangian morphism of vector bundles. As a consequence, we prove some of the
vanishing, in the Chow groups of a Lagrangian fibered hyper-K\""ahler variety
$X$, of certain polynomials in the Chern classes of $X$ and the Lagrangian
divisor, predicted by the Beauville-Voisin conjecture. Under some natural
assumptions on the dimensions of the rank strata, we also establish
nonnegativity and positivity results for Chern classes."
3,2,Constancy of the Hilbert-Samuel function,"The Hilbert-Samuel function and the multiplicity function are
fundamentallocally defined invariants on Noetherian schemes. They havebeen
playing an important role in desingularization for many years.Bennett studied
upper semicontinuity of the Hilbert-Samuel functionon schemes and proved that
it is non increasing under permissibleblowing ups. The latter are blowing ups
at regular subschemes alongwhich the singular scheme is normally flat.For a
reduced scheme, the Hilbert-Samuel function is constant ifand only if it is
regular: this translates the question of resolutionof singularities into a
problem of lowering the Hilbert-Samuel function.We show here that, this result
can be extended to non reducedschemes, as follows: Given a locally Noetherian
scheme X such thatthe local rings are excellent for every point, then the
Hilbert-Samuelfunction is constant on X if and only if X is normally flat along
itsreduction and the reduction itself is regular."
3,2,Wall-crossing formula for framed quiver moduli,"We investigate the wall-crossing phenomena for moduli of framed quiver
representations. These spaces are expected to be highly useful in capturing the
representation theoretic essence of special functions in integrable systems.
Within this class of moduli spaces, we focus on the type $A$ flag manifold,
type $A$ affine Laumon spaces, Nakajima quiver variety, and framed moduli of
sheaves on the projective plane and the blow-up as main motivating examples.
Specifically, we examine the wall-crossing formulas for integrals of Euler
classes over these moduli spaces."
3,2,Tensor product of representations of quivers,"In this article, we define the tensor product $V\otimes W$ of a
representation $V$ of a quiver $Q$ with a representation $W$ of an another
quiver $Q'$, and show that the representation $V\otimes W$ is semistable if $V$
and $W$ are semistable. Over the field of complex numbers, we also describe a
relation between the natural line bundles, and between the universal
representations on the fine moduli spaces $N_1, N_2$ and $N_3$ of
representations of $Q, Q'$ and $Q\otimes Q'$ respectively. We then prove that
the internal product $\tilde{Q}\otimes \tilde{Q'}$ of covering quivers is a
sub-quiver of the covering quiver $\widetilde{Q\otimes Q'}$. We deduce the
relation between stability of the representations $\widetilde{V\otimes W}$ and
$\tilde{V} \otimes \tilde{W}$. We also lift the relation between natural line
bundles on the product of moduli spaces $\tilde{N_1} \times \tilde{N_2}$."
3,2,Frobenius splitting of moduli spaces of parabolic bundles,"Let $C$ be a nonsingular projective curve over an algebraically closed field
of characteristic $p>0$ and $I\subset C$ be a finite set. If
$\mathcal{U}_{C,\,\omega}$ denotes the moduli space of semistable parabolic
bundles of rank $r$ and degree $d$ on $C$ with parabolic structures determined
by $\omega=(k,\{\vec n(x),\vec a(x)\}_{x\in I})$, we prove that
$\mathcal{U}_{C,\,\omega}$ is \textit{$F$-split} for generic $C$ and generic
choice of $I$ when $p>3r$."
3,2,On maximally non-factorial nodal Fano threefolds,"We classify non-factorial nodal Fano threefolds with $1$ node and class group
of rank $2$."
3,2,Affine vs. Stein in rigid geometry,"We investigate the relationship between affine and Stein varieties in the
context of rigid geometry. We show that the two concepts are much more closely
related than in complex geometry, e.g. they are equivalent for surfaces. This
rests on the density of algebraic functions in analytic functions. One key
ingredient to prove such a density statement is an extension result for Cartier
divisors."
3,2,On the failure of the integral Hodge/Tate conjecture for products with projective hypersurfaces,"In this paper we show the failure of the integral Hodge/Tate conjecture for
the product of an Enriques surface with a smooth odd-dimensional projective
hypersurface. To do this, we use a specialization argument of
Colliot-Th\'el\`ene applied to Schreieder's refined unramified cohomology. The
results obtained in this way give an interpretation of Shen's result in terms
of refined unramified cohomology. Moreover, using this interpretation, we avoid
the need to work over the complex numbers so that we may conclude that Shen's
result also holds over general algebraically closed fields of characteristic
not 2."
3,2,On projective K3 surfaces $\mathcal{X}$ with $\mathrm{Aut}(\mathcal{X})=(\mathbb{Z}/2\mathbb{Z})^2$,"We prove that every K3 surface with automorphism group
$(\mathbb{Z}/2\mathbb{Z})^2$ admits an explicit birational model as a double
sextic surface. This model is canonical for Picard number greater than 10. For
Picard number greater than 9, the K3 surfaces in question possess a second
birational model, in the form of a projective quartic hypersurface,
generalizing the Inose quartic."
3,2,Blowdowns of the Deligne-Mumford Spaces of Real Rational Curves,"We describe a sequence of smooth quotients of the Deligne-Mumford moduli
space ${\mathbb R}\overline{\mathcal M}_{0,\ell+1}$ of real rational curves
with $\ell\!+\!1$ conjugate pairs of marked points that terminates at ${\mathbb
R}\overline{\mathcal M}_{0,\ell}\!\times\!{\mathbb C}{\mathbb P}^1$. This
produces an analogue of Keel's blowup construction of the Deligne-Mumford
moduli spaces $\overline{\mathcal M}_{\ell+1}$ of rational curves with
$\ell\!+\!1$ marked points, but with an explicit description of the
intermediate spaces and the blowups of three different types. The same
framework readily adapts to the real moduli spaces with real points. In a
sequel, we use this inductive construction of ${\mathbb R}\overline{\mathcal
M}_{0,\ell+1}$ to completely determine the rational (co)homology ring of
${\mathbb R}\overline{\mathcal M}_{0,\ell}$."
3,2,The Cohomology Ring of the Deligne-Mumford Moduli Space of Real Rational Curves with Conjugate Marked Points,"It is a long-established and heavily-used fact that the integral cohomology
ring of the Deligne-Mumford moduli space of (complex) rational curves is the
polynomial ring on the boundary divisors modulo the ideal generated by the
obvious geometric relations between them. We show that the rational cohomology
ring of the Deligne-Mumford moduli space of real rational curves with conjugate
marked points only is the polynomial ring on certain (``complex"") boundary
divisors and real boundary hypersurfaces modulo the ideal generated by the
obvious geometric relations between them and the geometric relation in positive
dimension and codimension identified in a previous paper."
3,2,Intersection cohomology of type-A toric varieties,"Type-A toric varieties may be obtained as GIT quotients with respect to a
torus action with weights corresponding to roots of the group $SL(k)$ for some
$k>1$. These varieties appear in various important applications, in particular,
as normal cones to strata in moduli spaces of vector bundles. In this paper, we
describe the intersection Betti numbers of these varieties, and those of some
associated projective varieties. We present an elegant combinatorial model for
these numbers, and, using the work of Hausel and Sturmfels, we show that the
relevant intersection cohomology groups are endowed with a canonical product
structure."
3,2,Monodromy of double elliptic logarithms,"We determine the relative monodromy group of abelian logarithms with respect
to periods in the cases of fibered products of elliptic schemes. This gives
rise to a result stronger than a theorem due to Y. Andr\'e and implies in
particular the algebraic independence of the logarithm of any non-torsion
section and the periods. We then conjecture an analogous result for the general
case of an abelian scheme of arbitrary relative dimension. This generalizes a
theorem of Corvaja and Zannier which determines the said group in the case of a
single elliptic scheme."
3,2,On varieties whose general surface section has negative Kodaira dimension,"In this paper, inspired by work of Fano, Morin and Campana--Flenner, we give
a full projective classification of (however singular) varieties of dimension 3
whose general hyperplane sections have negative Kodaira dimension, and we
partly extend such a classification to varieties of dimension $n\geq 4$ whose
general surface sections have negative Kodaira dimension. In particular we
prove that a variety of dimension $n\geq 3$ whose general surface sections have
negative Kodaira dimension is birationally equivalent to the product of a
general surface section times $\p^{n-2}$ unless (possibly) if the variety is a
cubic hypersurface."
3,2,Local duality theorems for commutative algebraic groups,"If k is an arbitrary field, we construct a category of k-1-motives in which
every commutative algebraic k-group G has a dual object $G^{\vee}$. When k is a
local field of arbitrary characteristic, we establish Pontryagin duality
theorems that relate the fppf cohomology groups of G to the hypercohomology
groups of the k-1-motive $G^{\vee}$. We also obtain a duality theorem for the
second cohomology group of an arbitrary k-1-motive. These results have
applications (to be discussed elsewhere) to certain extensions of
Lichtenbaum-van Hamel duality to a class of non-smooth proper k-varieties."
1,2,Algebraic Winding Numbers,"In this paper we study in detail the properties of the algebraic winding
number proposed in a paper by M. Eisermann with respect to complex root
counting in rectangles. We also propose a new algebraic winding number which
computes the number of complex roots of a polynomial in a rectangle under no
assumptions, including roots on edges or vertices with appropriate counting. We
extend both winding numbers to rational functions, obtaining then an algebraic
version of the argument principle for rectangles."
3,2,Surfaces defined by pairs of polynomials,"We prove that the Brauer group of the generic diagonal surface of arbitrary
degree is trivial. The same method is applied to surfaces whose equation can be
written as the sum of two bilinear forms. This uses a general criterion for the
triviality of the transcendental Brauer group of an isotrivial variety. We
determine the field of definition of the geometric Picard group of the Fermat
surface of arbitrary degree, thus finishing work of Shioda and Aoki."
3,2,Quantum K-invariants and Gopakumar-Vafa invariants II. Calabi-Yau threefolds at genus zero,"This is the second part of our ongoing project on the relations between
Gopakumar-Vafa BPS invariants (GV) and quantum K-theory (QK) on the Calabi--Yau
threefolds (CY3). We show that on CY3 a genus zero quantum K-invariant can be
written as a linear combination of a finite number of Gopakumar--Vafa
invariants with coefficients from an explicit ``multiple cover formula''.
Conversely, GV can be determined by QK in a similar manner. The technical heart
is a proof of a remarkable conjecture by Hans Jockers and Peter Mayr.
  This result is consistent with the ``virtual Clemens conjecture'' for the
Calabi--Yau threefolds. A heuristic derivation of the relation between QK and
GV via the virtual Clemens conjecture and the multiple cover formula is also
given."
3,2,Analytic and algebraic integrability of quasi-smooth derived foliations,"We study integrability results for derived foliations in the holomorphic
context. We prove a global integrability theorem by flat groupoids, as well as
global algebraic integrability in the presence of a compact leaf with finite
holonomy groups. These results are generalization to the derived (and thus
singular) setting of well know results and constructions: integration of
holomorphic foliations by smooth groupoid and global stability in the
holomorphic situation."
3,2,Vinberg pairs and Higgs bundles,"We explore the role of Vinberg pairs defined by cyclic gradings of a
semisimple complex Lie algebra in the context of Higgs bundle theory."
3,2,How many points can stand on the point of a needle? Superfat points and associated tensors,"We study the 0-dimensional schemes supported at one point in $n$-space which
are $m$-symmetric, i.e. they intersect any curves thru the point with length
$m$. We show that the maximal length for such a scheme is $m^n$ ($m$-superfat
points) and we study properties of such schemes, in particular for $n=2$. We
also study varieties defined by such schemes on Veronese and Segre-veronese
varieties."
3,2,Conics in quintic del Pezzo varieties,"The smooth quintic del Pezzo variety $Y$ is well-known to be obtained as a
linear sections of the Grassmannian variety $\mathrm{Gr}(2,5)$ under the
Pl\""ucker embedding into $\mathbb{P}^{9}$. Through a local computation, we show
the Hilbert scheme of conics in $Y$ for $\text{dim} Y \ge 3$ can be obtained
from a certain Grassmannian bundle by a single blowing up/down transformation."
3,2,On boundedness of indices of minimal pairs -- surfaces,"For given positive integers $d$ and $m$, consider the projective klt pairs
$(X,B)$ of dimension $d$, of Cartier index $m$, and with semi-ample $K_X+B$
defining a contraction $\pi\colon X\to Z$. We prove that it is not possible in
general to write $n(K_X+B)\sim\pi^*A_Z$ for some $n$ depending only on $d$ and
$m$, and some Cartier divisor $A_Z$ on $Z$."
3,2,A calculation of the perfectoidization of semiperfectoid rings,"We show that the perfectoidization can be (almost) calculated by using
uniform completion or $p$-root closure in certain cases, including the
semiperfectoid case. To do this, we focus on the universality of
perfectoidization and uniform completion, as well as the $p$-root closed
property of perfectoid rings. Through this calculation, we establish a
connection between Roberts' classical closure operation ``$p$-root closure''
and Bhatt-Scholze's more recent concept of ``perfectoidization''."
3,2,Restricted shtukas and their character sheaves,"We extend cases where nearby cycles commutes with pushforward from sheaves on
the moduli space of shtukas to a product of curves that includes cases
corresponding to automorphic forms of arbitrary depth in the Moy-Prasad
filtration. We introduce the notion of $\Psi$-factorizability to study nearby
cycles over general bases, which encodes various fusion and braiding
relationships for nearby cycles with respect to arbitrary compositions of
specializations on the base. The Satake sheaves on the moduli spaces of shtukas
and their cohomology on curves are $\Psi$-factorizable. This notion allows us
to adapt a ""Zorro lemma"" argument used by Lafforgue and Xue. As an application,
for certain automorphic forms in depth zero, we characterize the semisimple and
unipotent parts of the image of the tame generator in terms of semisimple
orbits and two-sided cells attached to character sheaves, extending ideas of
Lusztig-Yun and Bezrukavnikov-Finkelberg-Ostrik."
3,2,Thomason filtration via $T(1)$-local $\mathrm{TC}$,"We construct a natural filtration on $T(1)$-local $\mathrm{TC}$ for each
derived schemes satisfying certain finiteness conditions using prismatic
cohomology and descent theory. In the course of the construction, we also study
some general properties of prismatic cohomology complexes over perfect prisms
after inverting distinguished generators. The construction is intrinsic to
$\mathrm{TC}$ and recovers Thomason's spectral sequence for $T(1)$-local
algebraic K-theory via the trace map."
3,2,Integral Picard group of moduli of polarized K3 surfaces,"We compute the integral Picard group of the moduli stack of polarized K3
surfaces of fixed degree whose singularities are at most rational double
points. We also compute the integral Picard group of the stack of
quasi-polarized K3 surfaces, and of the stacky period domain."
3,2,Dimension results for extremal-generic polynomial systems over complete toric varieties,"We study polynomial systems with prescribed monomial supports in the Cox
rings of toric varieties built from complete polyhedral fans. We present
combinatorial formulas for the dimensions of their associated subvarieties
under genericity assumptions on the coefficients of the polynomials. Using
these formulas, we identify at which degrees generic systems in polytopal
algebras form regular sequences. Our motivation comes from sparse elimination
theory, where knowing the expected dimension of these subvarieties leads to
specialized algorithms and to large speed-ups for solving sparse polynomial
systems. As a special case, we classify the degrees at which regular sequences
defined by weighted homogeneous polynomials can be found, answering an open
question in the Gr\""obner bases literature. We also show that deciding whether
a sparse system is generically a regular sequence in a polytopal algebra is
hard from the point of view of theoretical computational complexity."
1,2,Amalgamation of real zero polynomials,"With this article, we hope to launch the investigation of what we call the
real zero amalgamation problem. Whenever a polynomial arises from another
polynomial by substituting zero for some of its variables, we call the second
polynomial an extension of the first one. The real zero amalgamation problem
asks when two (multivariate real) polynomials have a common extension (called
amalgam) that is a real zero polynomial. We show that the obvious necessary
conditions are not sufficient. Our counterexample is derived in several steps
from a counterexample to amalgamation of matroids by Poljak and Turz\'ik. On
the positive side, we show that even a degree-preserving amalgamation is
possible in three very special cases with three completely different
techniques. Finally, we conjecture that amalgamation is always possible in the
case of two shared variables. The analogue in matroid theory is true by another
work of Poljak and Turz\'ik. This would imply a very weak form of the
Generalized Lax Conjecture."
3,2,Algebraic cycles and Fano threefolds of genus 10,"We show that prime Fano threefolds $Y$ of genus 10 have a multiplicative
Chow-K\""unneth decomposition, in the sense of Shen-Vial. As a consequence, a
certain tautological subring of the Chow ring of powers of $Y$ injects into
cohomology."
3,2,"On K-stability, height bounds and the Manin-Peyre conjecture","This note discusses some intriguing connections between height bounds on
complex K-semistable Fano varieties X and Peyre's conjectural formula for the
density of rational points on X. Relations to an upper bound for the smallest
rational point, proposed by Elsenhans-Jahnel, are also explored. These
relations suggest an analog of the height inequalities, adapted to the real
points, which is established for the real projective line and related to
K\""ahler-Einstein metrics."
3,2,On decorated representation spaces associated to spherical surfaces,"We analyse local features of the spaces of representations of the fundamental
group of a punctured surface in $\mathrm{SU}_2$ equipped with a decoration,
namely a choice of a logarithm of the representation at peripheral loops. Such
decorated representations naturally arise as monodromies of spherical surfaces
with conical points. Among other things, in this paper we determine the smooth
locus of such absolute and relative decorated representation spaces: in
particular, in the relative case (with few special exceptions) such smooth
locus is dense, connected, and exactly consists of non-coaxial representations.
The present study sheds some light on the local structure of the moduli space
of spherical surfaces with conical points, which is locally modelled on the
above-mentioned decorated representation spaces."
3,2,Moduli of weighted stable marked del Pezzo surfaces,"For $n \leq 6$, we describe the stable pair compactification
$\overline{Y}_c^n$ of the moduli space $Y_c^n$ of log canonical pairs $(S,cB)$
such that $S$ is a smooth del Pezzo surface of degree $9-n$, $B$ is the
(labeled) sum of its finitely many lines, and $c \in (0,1]$ such that $K_S+cB$
is ample. When $c=1$ or $c$ is minimal, this compactification has been
described previously by work of Hacking-Keel-Tevelev, and
Gallardo-Kerr-Schaffler. We establish the full sequence of wall crossings as
one decreases $c$ from 1 to the minimal weight. Our results include a complete
description of the fibers of the universal families."
1,2,"On the derived category of IGr(3, 9)","We construct a minimal Lefschetz decomposition of the bounded derived
category of the odd isotropic Grassmannian $\mathsf{IGr}(3,9)$. The exceptional
objects are $\mathsf{Sp}_9$-equivariant vector bundles. This provides further
evidence of the Kuznetsov-Smirnov version of Dubrovin conjecture."
3,2,Unique powers-of-forms decompositions from simple Gram spectrahedra,"We consider simultaneous Waring decompositions: Given forms $ f_d $ of
degrees $ kd $, $ (d = 2,3 )$, which admit a representation as $ d $-th power
sums of $ k $-forms $ q_1,\ldots,q_m $, when is it possible to reconstruct the
addends $ q_1,\ldots,q_m $ from the power sums $ f_d $? Such powers-of-forms
decompositions model the moment problem for mixtures of centered Gaussians. The
novel approach of this paper is to use semidefinite programming in order to
perform a reduction to tensor decomposition. The proposed method works on
typical parameter sets at least as long as $ m\leq n-1 $, where $ m $ is the
rank of the decomposition and $ n $ is the number of variables. While provably
not tight, this analysis still gives the currently best known rank threshold
for decomposing third order powers-of-forms, improving on previous work in both
asymptotics and constant factors. Our algorithm can produce proofs of
uniqueness for specific decompositions. A numerical study is conducted on
Gaussian random trace-free quadratics, giving evidence that the success
probability converges to $ 1 $ in an average case setting, as long as $ m = n $
and $ n\to \infty $. Some evidence is given that the algorithm also succeeds on
instances of rank $ m = \Theta(n^2) $."
3,2,Vertex algebras from the Hull-Strominger system,"Motivated by the programme on mirror symmetry for non-K\""ahler manifolds, we
construct representations of the $N=2$ superconformal vertex algebra associated
to solutions of the Hull-Strominger system. The construction is via embeddings
of the $N=2$ superconformal vertex algebra in the chiral de Rham complex of a
string Courant algebroid, building on a universal construction by Bressler and
Heluani. Our main results depend on two conditions: firstly, on the vanishing
of a bivector field canonically associated to a solution, and, secondly, on the
connection $\nabla$, one of the unknowns of the system, being
Hermitian-Yang-Mills. Combined with a previous result by the authors, we show
that all the known solutions of the Hull-Strominger system on non-K\""ahler
Calabi-Yau threefolds satisfying the second condition have an associated $N=2$
embedding. We also prove a general existence result for these embeddings on
compact complex surfaces."
3,2,"Isoperiodic foliation of the stratum $\mathcal{H}(1,1,-2)$","On a Riemann surface, periods of a meromorphic differential along closed
loops define a period character from the absolute homology group into the
additive group of complex numbers. Fixing the period character in strata of
meromorphic differentials defines the isoperiodic foliation where the remaining
degrees of freedom are the relative periods between the zeroes of the
differential. In strata of meromorphic differentials with exactly two zeroes,
leaves have a natural structure of translation surface. In this paper, we give
a complete description of the isoperiodic leaves in stratum
$\mathcal{H}(1,1,-2)$ of meromorphic $1$-forms with two simple zeroes and a
pole of order two on an elliptic curve. For each character, the corresponding
leaf is a connected Loch Ness Monster. The translation structures of generic
leaves feature wild singularities and a ramified cover of infinite degree over
the flat torus defined by the lattice of absolute periods."
3,2,Arithmeticity for integral cohomological dimension of configuration spaces of manifolds,"Consider the configuration spaces of manifolds. We give a precise formula for
the integral cohomological dimension (the degree of top non-trivial integral
cohomology group) of unordered configuration spaces of manifolds with
non-trivial co-dimension one cohomology group, and show that the the sequence
of cohomological dimensions is arithmetic. This arithmeticity is not present in
the classical example of Arnold. Moreover, we show that the top integral
cohomology group is infinite. Furthermore, We give a lower bound for the rank
of top integral cohomology group. We also predict that the top integral
cohomology group of configuration spaces of manifolds with non-trivial
co-dimension one cohomology group is eventually finite. To the best of our
knowledge, there is no rigorous bound for the cohomological dimension of
ordered configuration spaces. As an application of main results, we give a
sharp lower bound for the cohomological dimension of ordered configuration
spaces of manifolds. The first step of the proof of main results is to define a
reduced Chevalley Eilenberg complex."
3,2,On area-minimizing Pfaffian varieties,"There are two significant families of minimal real matrix varieties:
determinantal varieties and skew-symmetric determinantal varieties, the later
ones are also known as Pfaffian varieties. In 1999, Kerckhove and Lawlor [Duke
Math.J. 96(2),401--424,1999] proved that determinantal varieties are
area-minimizing except for two families. In this paper we prove that all
Pfaffian varieties are area-minimizing with the exception of Pfaffian
hypersurfaces."
3,2,"Complements, index theorem, and minimal log discrepancies of foliated surface singularities","We present an extension of several results on pairs and varieties to foliated
surface pairs. We prove the boundedness of local complements, the local index
theorem, and the uniform boundedness of minimal log discrepancies (mlds), as
well as establishing the existence of uniform rational lc polytopes.
Furthermore, we address two questions posed by P. Cascini and C. Spicer on
foliations, providing negative responses. We also demonstrate that the
Grauert-Riemenschneider type vanishing theorem generally fails for lc
foliations on surfaces. In addition, we determine the set of minimal log
discrepancies for foliated surface pairs with specific coefficients, which
leads to the recovery of Y.-A. Chen's proof on the ascending chain condition
conjecture for mlds for foliated surfaces."
1,2,Algebraic Properties of the Fermi Variety for Periodic Graph Operators,"We present a method to estimate the number of irreducible components of the
Fermi varieties of periodic Schr\""odinger operators on graphs in terms of
suitable asymptotics. Our main theorem is an abstract bound for the number of
irreducible components of Laurent polynomials in terms of such asymptotics. We
then show how the abstract bound implies irreducibility in many lattices of
interest, including examples with more than one vertex in the fundamental cell
such as the Lieb lattice as well as certain models obtained by the process of
graph decoration."
3,2,Upper bounds for the rank of powers of quadrics,"We determine an upper bound for the rank of every power of an arbitrary
quadratic form. In particular, given any $s\in\mathbb N$, we prove that the
$s$-th power of a quadratic form of rank $n$ grows as $n^{s}$. Moreover, we
guarantee that its rank is subgeneric for every $n>(2s-1)^2$."
3,2,Brauertsch fields,"We prove a local-to-global principle for Brauer classes: for any finite
collection of non-trivial Brauer classes on a variety over a field of
transcendence degree at least 3, there are infinitely many specializations
where each class stays non-trivial. This is deduced from a
Grothendieck--Lefschetz-type theorem for Brauer groups of certain smooth
stacks. This also leads to the notion of a Brauertsch field."
3,2,Generic flexibility of affine cones over del Pezzo surfaces in Sagemath,"Generic flexibility of affine cones over del Pezzo surfaces are subject of
active study recently. The question is completely studied in degree at least 3,
and partially in degree 2.
  We present a Sagemath module that facilitates most operations for verifying
the generic flexibility of affine cones over del Pezzo surfaces and weak del
Pezzo surfaces of arbitrary degree, depending on a polarization. Using it, we
verified generic flexibility of affine cones over polarizations of surfaces of
degree 1 under certain conditions and over arbitrary very ample polarizations
of weak del Pezzo surfaces of degree 6."
3,2,Cycles relations in the affine grassmannian and applications to Breuil--Mzard for G-crystalline representations,"For a split reductive group $G$ we realise identities in the Grothendieck
group of $\widehat{G}$-representation in terms of cycle relations between
certain closed subschemes inside the affine grassmannian. These closed
subschemes are obtained as a degeneration of $e$-fold products of flag
varieties and, under a bound on the Hodge type, we relate the geometry of these
degenerations to that of moduli spaces of $G$-valued crystalline
representations of $\operatorname{Gal}(\overline{K}/K)$ for $K/\mathbb{Q}_p$ a
finite extension with ramification degree $e$. By transferring the
aforementioned cycle relations to these moduli spaces we deduce one direction
of the Breuil--M\'ezard conjecture for $G$-valued crystalline representations
with small Hodge type."
3,2,tale degree map and 0-cycles,"By using the triangulated category of \'etale motives over a field $k$, for a
smooth projective variety $X$ over $k$, we define the group
$\text{CH}^\text{\'et}_0(X)$ as an \'etale analogue of 0-cycles. We study the
properties of $\text{CH}^\text{\'et}_0(X)$, giving a description about the
birational invariance of such group. We define and present the \'etale degree
map by using Gysin morphisms in \'etale motivic cohomology and the \'etale
index as an analogue to the classical case. We give examples of smooth
projective varieties over a field $k$ without zero cycles of degree one but
with \'etale zero cycles of degree one, however, this property is not always
true as we present examples where the \'etale degree map is not surjective."
3,2,Punctual Quot schemes and Cohen--Lenstra series of the cusp singularity,"The Quot scheme of points $\mathrm{Quot}_{d,n}(X)$ on a variety $X$ over a
field $k$ parametrizes quotient sheaves of $\mathcal{O}_X^{\oplus d}$ of
zero-dimensional support and length $n$. It is a rank-$d$ generalization of the
Hilbert scheme of $n$ points. When $X$ is a reduced curve with only the cusp
singularity $\{x^2=y^3\}$ and $d\geq 0$ is fixed, the generating series for the
motives of $\mathrm{Quot}_{d,n}(X)$ in the Grothendieck ring of varieties is
studied via Gr\""obner bases, and shown to be rational. Moreover, the generating
series is computed explicitly when $d\leq 3$. The computational results exhibit
surprising patterns (despite the fact that the category of finite length
coherent modules over a cusp is wild), which not only enable us to conjecture
the exact form of the generating series for all $d$, but also suggest a general
functional equation whose $d=1$ case is the classical functional equation of
the motivic zeta function known for any Gorenstein curve.
  As another side of the story, Quot schemes are related to the Cohen--Lenstra
series. The Cohen--Lenstra series encodes the count of ""commuting matrix
points'' (or equivalently, coherent modules of finite length) of a variety over
a finite field, about which Huang formuated a ""rationality'' conjecture for
singular curves. We prove a general formula that expresses the Cohen--Lenstra
series in terms of the motives of the (punctual) Quot schemes, which together
with our main rationality theorem, provides positive evidence for Huang's
conjecture for the cusp."
1,2,Maximum likelihood thresholds of generic linear concentration models,"The maximum likelihood threshold of a statistical model is the minimum number
of datapoints required to fit the model via maximum likelihood estimation. In
this paper we determine the maximum likelihood thresholds of generic linear
concentration models. This turns out to be the number one would expect from a
naive dimension count, which is surprising and nontrivial to prove given that
the maximum likelihood threshold is a semi-algebraic concept. We also describe
geometrically how a linear concentration model can fail to exhibit this generic
behavior and briefly discuss connections to rigidity theory."
1,2,Graph rings and ideals: Wolmer Vasconcelos contributions,"This is a survey article featuring some of Wolmer Vasconcelos contributions
to commutative algebra, and explaining how Vasconcelos' work and insights have
contributed to the development of commutative algebra and its interaction with
other areas to the present."
3,2,Quasi-Isomorphisms of Commutative DG Rings and Divided Power Structures,"We prove that a quasi-isomorphism $f : A \to B$ between commutative DG rings,
where $B$ admits a divided power structure, can be factored as $f = \tilde{f}
\circ e$, where $e : A \to \tilde{B}$ is a split injective quasi-isomorphism,
and $\tilde{f} : \tilde{B} \to B$ is a surjective quasi-isomorphism.
  This result is used in our work on a DG approach to the cotangent complex,
and our work on the derived category of commutative DG rings."
3,2,Arithmetic aspects of the Jouanolou foliation,"We investigate the structure of the $p$-divisor for the Jouanolou foliation
where we show, under some conditions, that it can be irreducible or has a
$p$-factor. We study the reduction modulo $p$ of foliations on the projective
plane and its applications to the problems of holomorphic foliations. We give
new proof, via reduction modulo $2$, of the fact that the Jouanolou foliation
on the complex projective plane of odd degree, under some arithmetic
conditions, has no algebraic solutions."
3,2,Birationally solid Fano 3-fold hypersurfaces,"A Fano variety of Picard number $1$ is said to be \textit{birationally solid}
if it is not birational to a Mori fiber space over a positive dimensional base.
In this paper we complete the classification of quasi-smooth birationally solid
Fano $3$-fold weighted hypersurfaces."
3,2,Double covers of curves on Nikulin surfaces,"We survey basic results concerning Prym varieties, the Prym-Brill-Noether
theory initiated by Welters, and Brill-Noether theory of general \'etale double
covers of curves of genus g>=2. We then specialize to curves on Nikulin
surfaces and show that \'etale double covers of curves on Nikulin surfaces of
standard type do not satisfy Welters' Theorem. On the other hand, by
specialization to curves on Nikulin surfaces of non-standard type, we prove
that general double covers of curves ramified at b=2,4,6 points are
Brill-Noether general; the case b=2 was already obtained by Bud with different
techniques."
3,2,Gauss sums and Van der Geer--Van der Vlugt curves,"We study Van der Geer--Van der Vlugt curves in a ramification-theoretic view
point. We give explicit formulae on L-polynomials of these curves. As a result,
we show that these curves are supersingular and give sufficient conditions for
these curves to be maximal or minimal."
3,2,Divided prismatic Frobenius crystals of small height and the category $\mathcal{M}\mathcal{F}$,"Let $\mathcal{X}$ be a smooth $p$-adic formal scheme over a mixed
characteristic complete discrete valuation ring $\mathcal{O}_{K}$ with perfect
residue field. We introduce a general category $\mathcal{M}\mathcal{F}_{[0,
p-2]}^{tor-free}(\mathcal{X})$ of $p$-torsion free crystalline coefficient
objects and show that this category is equivalent to the category of completed
prismatic Frobenius crystals of height $p-2$, recently introduced by
Du-Liu-Moon-Shimizu. In particular this shows that the category
$\mathcal{M}\mathcal{F}^{tor-free}_{[0, p-2]}(\mathcal{X})$ is equivalent to
the category of crystalline $\mathbb{Z}_p$-local systems on $\mathcal{X}$ with
Hodge-Tate weights in $\{0,\ldots , p-2\}$, which generalizes the crystalline
part of a theorem of Breuil-Liu to higher dimensions."
3,2,tale cohomology of algebraic varieties over Stein compacta,"We prove a comparison theorem between the \'etale cohomology of algebraic
varieties over Stein compacta and the singular cohomology of their
analytifications. We deduce that the field of meromorphic functions in a
neighborhood of a connected Stein compact subset of a normal complex space of
dimension $n$ has cohomological dimension $n$. As an application of
$\textrm{Gal}(\mathbb{C}/\mathbb{R})$-equivariant variants of these results, we
obtain a quantitative version of Hilbert's 17th problem on compact subsets of
real-analytic spaces."
3,2,Simultaneous Galois points for a reducible plane curve consisting of nonsingular components,"Yoshihara's definition of Galois points for irreducible plane curves is
extended to reducible plane curves. We also define simultaneous Galois points,
weakening the conditions of the definition. We studied the number of
simultaneous Galois points for a reduced plane curve with nonsingular
components."
3,2,Bott manifolds with vanishing Futaki invariants for all Khler classes,"We prove that the only Bott manifolds such that the Futaki invariant vanishes
for any K\""ahler class are isomorphic to the products of the projective lines."
3,2,Crepant resolution of $\mathbb{A}^4/A_4$ in characteristic 2,"In this paper, we construct a crepant resolution for the quotient singularity
$\mathbb{A}^4/A_4$ in characteristic 2, where $A_4$ is the alternating group of
degree 4 with permutation action on $\mathbb{A}^4$. By computing the Euler
number of the crepant resolution, we obtain a new counterexample to an
analogous statement of McKay correspondence in positive characteristic."
3,2,Some Arithmetic Properties of Complex Local Systems,"This small text was written for the AMS Notices. It is a survey of
integrality properties of complex local systems, where I tried to single out
one example which is not entirely explicit in the literature. The focus is on
the obstruction it yields for a finitely presented group to be the topological
fundamental group of a connected smooth quasi-projective complex variety. I
thank Johan de Jong, Michael Groechenig, and Moritz Kerz.
  The material in this expository note relies on joint work or discussions with
them. [v2: There was one TeX typo on p.8 which I corrected]."
3,2,Multilinear Hyperquiver Representations,"We count singular vector tuples of a system of tensors assigned to the edges
of a directed hypergraph. To do so, we study the generalisation of quivers to
directed hypergraphs. Assigning vector spaces to the nodes of a hypergraph and
multilinear maps to its hyperedges gives a hyperquiver representation.
Hyperquiver representations generalise quiver representations (where all
hyperedges are edges) and tensors (where there is only one multilinear map).
The singular vectors of a hyperquiver representation are a compatible
assignment of vectors to the nodes. We compute the dimension and degree of the
variety of singular vectors of a sufficiently generic hyperquiver
representation. Our formula specialises to known results that count the
singular vectors and eigenvectors of a generic tensor."
3,2,Dualizing and canonical complexes on finite posets,"We develop Grothendieck's theory of dualizing complexes on finite posets, and
its subsequent theory of Cohen-Macaulayness."
3,2,Degrees of some orthogonal Deligne--Lusztig varieties,"We show a degree formula for a type of orthogonal Deligne--Lusztig varieties
and their Pl\""ucker embeddings. This is an analog of work of Li on a unitary
case."
3,2,Irreducible Holomorphic Symplectic manifolds with an action of $\mathbb Z_4^3 : \mathcal A_6$,"H\""ohn and Mason classified the possible symplectic groups acting on an
Irreducible Holomorphic Symplectic (IHS) manifold of K3$^{[2]}$-type, finding
that $\mathbb Z_3^4 : \mathcal A_6$ is the symplectic group with the biggest
order. In this paper, we study the possible IHS manifolds of K3$^{[2]}$-type
with a symplectic action of $\mathbb Z_3^4 : \mathcal A_6$ and also admitting a
non-symplectic automorphism. We characterize such IHS manifolds. In particular
we prove that there exists a IHS manifold of K3$^{[2]}$-type with finite
automorphism group of order 174960, the biggest possible order for the
automorphism group of a IHS manifold of K3$^{[2]}$-type, and it is the Fano
variety of lines of the Fermat cubic fourfold."
3,2,Quantum Lefschetz theorem revisited,"Let $X$ be any smooth Deligne-Mumford stack with projective coarse moduli,
and $Y$ be a complete intersection in $X$ associated with a direct sum of
semi-positive line bundles. For any point on the Givental's Lagrangian cone of
$X$ satisfying a mild condition called admissible series, we will show that a
hyper-geometric modification of the point lies on the Lagrangian cone of $Y$.
This confirms a prediction from Coates--Corti--Iritani--Tseng about the genus
zero quantum Lefschetz theorem beyond convexity."
3,2,Newton--Okounkov bodies and minimal models for cluster varieties,"Let $Y$ be a (partial) minimal model of a scheme $V$ with a cluster
structure. Under natural assumptions, for every choice of seed we associate a
Newton--Okounkov body to every divisor on $Y$ supported on $Y \setminus V$ and
show that these Newton--Okounkov bodies are positive sets in the sense of
Gross, Hacking, Keel and Kontsevich \cite{GHKK}. This construction essentially
reverses the procedure in loc. cit. that generalizes the polytope construction
of a toric variety to the framework of cluster varieties.
  In a closely related setting, we consider cases where $Y$ is a projective
variety whose universal torsor $\text{UT}_Y$ is a partial minimal model of a
scheme with a cluster structure of type $\mathcal A$. If the theta functions
parametrized by the integral points of the associated superpotential cone form
a basis of the ring of algebraic functions on $\text{UT}_Y$ and the action of
the torus $T_{\text{Pic}(Y)^*}$ on $\text{UT}_Y$ is compatible with the cluster
structure, then for every choice of seed we associate a Newton--Okounkov body
to every line bundle on $Y$. We prove that any such Newton--Okounkov body is a
positive set and that $Y$ is a minimal model of a quotient of a cluster
$\mathcal A$-variety by the action of a torus.
  Our constructions lead to the notion of the intrinsic Newton--Okounkov body
associated to a boundary divisor in a partial minimal model of a scheme with a
cluster structure. This provides a wide class of examples of Newton-Okoukov
bodies exhibiting a wall-crossing phenomenon in the sense of Escobar--Harada
\cite{EH20}.
  This approach includes the partial flag varieties that arise as minimal
models of cluster varieties. For the case of Grassmannians, our approach
recovers, up to interesting unimodular equivalences, the Newton--Okounkov
bodies constructed by Rietsch--Williams in \cite{RW}."
3,2,$L$-functions of Kloosterman sheaves,"In this article, we study a family of motives $\mathrm{M}_{n+1}^k$ associated
with the symmetric power of Kloosterman sheaves, as constructed by Fres\'an,
Sabbah, and Yu. They demonstrated that for $n=1$, the motivic $L$-functions of
$\mathrm{M}_{2}^k$ extend meromorphically to $\mathbb{C}$ and satisfy the
functional equations conjectured by Broadhurst and Roberts. Our work aims to
extend these results to the motivic $L$-functions of some of the motives
$\mathrm{M}_{n+1}^k$, with $n>1$, as well as other related $2$-dimensional
motives. In particular, we prove several conjectures of Evans type, which
relate traces of Kloosterman sheaves and Fourier coefficients of modular forms."
3,2,Infinite matroids in tropical differential algebra,"We consider a finite-dimensional vector space $W\subset K^E$ over an
arbitrary field $K$ and an arbitrary set $E$. We show that the set $C(W)\subset
2^E$ consisting of the minimal supports of $W$ are the circuits of a matroid on
$E$. In particular, we show that this matroid is cofinitary (hence, tame). When
the cardinality of $K$ is large enough (with respect to the cardinality of
$E$), then the set $trop(W)\subset 2^E$ consisting of all the supports of $W$
is a matroid itself.
  Afterwards we apply these results to tropical differential algebraic geometry
and study the set of supports $trop(Sol(\Sigma))\subset (2^{\mathbb{N}^{m}})^n$
of spaces of formal power series solutions $\text{Sol}(\Sigma)$ of systems of
linear differential equations $\Sigma$ in differential variables
$x_1,\ldots,x_n$ having coefficients in the ring ${K}[\![t_1,\ldots,t_m]\!]$.
If $\Sigma$ is of differential type zero, then the set $C(Sol(\Sigma))\subset
(2^{\mathbb{N}^{m}})^n$ of minimal supports defines a matroid on
$E=\mathbb{N}^{mn}$, and if the cardinality of $K$ is large enough, then the
set of supports $trop(Sol(\Sigma))$ itself is a matroid on $E$ as well. By
applying the fundamental theorem of tropical differential algebraic geometry
(fttdag), we give a necessary condition under which the set of solutions
$Sol(U)$ of a system $U$ of tropical linear differential equations to be a
matroid.
  We also give a counterexample to the fttdag for systems $\Sigma$ of linear
differential equations over countable fields. In this case, the set
$trop(Sol(\Sigma))$ may not form a matroid."
3,2,Smooth k-double covers of the plane of geometric genus 3,"In this work we classify all smooth surfaces with geometric genus equal to
three and an action of a group G isomorphic to (Z/2)^k such that the quotient
is a plane. We find 11 families. We compute the canonical map of all of them,
finding in particular a family of surfaces with canonical map of degree 16 that
we could not find in the literature. We discuss the quotients by all subgroups
of G finding several K3 surfaces with symplectic involutions. In particular we
show that six families are families of triple K3 burgers in the sense of
Laterveer."
1,3,"Epithelial-substrate coupling strength regulates the landscape of the traction in cohesive monolayers: a parametric study and a revisit to ""size effect""","Epithelial cells can assemble into cohesive colonies and collectively
interact with substrates by generating extracellular forces through focal
adhesions. Recently, a molecularly based thermodynamic model, which integrates
both the monolayer elasticity and force-mediated focal adhesion formation, has
been developed to elucidate the regulation of the cellular force landscape
induced by the active epithelial-substrate coupling. However, how
epithelial-substrate coupling strength mediate the landscapes of the traction,
the cellular displacement, and the focal adhesion distribution in a cohesive
monolayer remains unexamined in details. In this work, we follow the procedures
by the previous work to re-formulate the free energy of the
epithelial-substrate system and obtain the thermodynamic steady-state
equations. We then derive a simplified form of the complete equation system,
and solve it both semi-analytically and numerically. We find that the parameter
which characterizes the epithelial-substrate coupling strength can
significantly affect the landscapes of the traction the cellular displacement,
and the focal adhesion distribution. We also revisit the ""size effect""
addressed by previous works and demonstrate that such effect is the natural
outcome of a strong epithelial-substrate coupling without introducing any extra
factors. For epithelial-substrate coupling which is not strong enough, the
currently observed ""size effect"" does not hold. A scaling law that determines
whether the previously observed ""size effect"" holds is proposed based on our
model."
3,3,Reaction-diffusion transport into core-shell geometry: Well-posedness and stability of stationary solutions,"We investigate a nonlinear parabolic reaction-diffusion equation describing
the oxygen concentration in encapsulated pancreatic cells with a general
core-shell geometry. This geometry introduces a discontinuous diffusion
coefficient as the material properties of the core and shell differ. We apply
monotone operator theory to show well-posedness of the problem in the strong
form. Furthermore, the stationary solutions are unique and asymptotically
stable. These results rely on the gradient structure of the underlying PDE."
2,3,Metabolic Regulatory Network Kinetic Modeling with Multiple Isotopic Tracers for iPSCs,"The rapidly expanding market for regenerative medicines and cell therapies
highlights the need to advance the understanding of cellular metabolisms and
improve the prediction of cultivation production process for human induced
pluripotent stem cells (iPSCs). In this paper, a metabolic kinetic model was
developed to characterize underlying mechanisms of iPSC culture process, which
can predict cell response to environmental perturbation and support process
control. This model focuses on the central carbon metabolic network, including
glycolysis, pentose phosphate pathway (PPP), tricarboxylic acid (TCA) cycle,
and amino acid metabolism, which plays a crucial role to support iPSC
proliferation. Heterogeneous measures of extracellular metabolites and multiple
isotopic tracers collected under multiple conditions were used to learn
metabolic regulatory mechanisms. Systematic cross-validation confirmed the
model's performance in terms of providing reliable predictions on cellular
metabolism and culture process dynamics under various culture conditions. Thus,
the developed mechanistic kinetic model can support process control strategies
to strategically select optimal cell culture conditions at different times,
ensure cell product functionality, and facilitate large-scale manufacturing of
regenerative medicines and cell therapies."
1,3,Cell behavior in the face of uncertainty,"Organisms that grow and survive in uncertain environments may need to change
their physiological state as the environment changes. When the environment is
uncertain, one strategy known as bet-hedging is to make these changes randomly
and independently of the environment, to ensure that at least part of the
population is well adapted. Organisms that collect information from their
environment may also use this information to modulate their changes of
physiological states. We review these different strategies and point out
parallels with the theory of optimal financial investments."
1,3,Quorum Sensing as a long-range interaction for bacteria growth and bioluminescence,"We study the role of Quorum Sensing (QS) in the growth of bacterial colonies
and in the bioluminescence produced. These two phenomena are both regulated by
QS and the experimental data show a non-trivial correlation between them. It is
also known that the specific bacterial substrate potentially modifies the
behavior of the colony. In the specific case of the bioluminescent
gram-negative bacterium, Vibrio harveyi, we propose a three-autoinducer model
in which QS is described in terms of long-range interaction between charged
objects placed on a regular network and playing the role of bacteria. The
charges spread in the network through fictitious non-linear electrostatic
interactions. QS is monitored by analyzing the current flowing within the
network. The model parameters are determined by comparing the simulations with
the data present in the literature and related to liquid cultures. New colony
growth measurements are then performed on Vibrio campbelli (a member of the
clade Harveyi) grown on hydroxyapatite (HA) substrates relevant for biomedical
applications. Growth on the substrate differs from growth in liquid culture,
although the observed bioluminescence is similar. We exploit our model to
understand the differences between colonies in liquid culture and on substrate
in terms of the relative role and cooperation of the three autoinducers."
2,3,Machine learning traction force maps of cell monolayers,"Cellular force transmission across a hierarchy of molecular switchers is
central to mechanobiological responses. However, current cellular force
microscopies suffer from low throughput and resolution. Here we introduce and
train a generative adversarial network (GAN) to paint out traction force maps
of cell monolayers with high fidelity to the experimental traction force
microscopy (TFM). The GAN analyzes traction force maps as an image-to-image
translation problem, where its generative and discriminative neural networks
are simultaneously cross-trained by hybrid experimental and numerical datasets.
In addition to capturing the colony-size and substrate-stiffness dependent
traction force maps, the trained GAN predicts asymmetric traction force
patterns for multicellular monolayers seeding on substrates with stiffness
gradient, implicating collective durotaxis. Further, the neural network can
extract experimentally inaccessible, the hidden relationship between substrate
stiffness and cell contractility, which underlies cellular mechanotransduction.
Trained solely on datasets for epithelial cells, the GAN can be extrapolated to
other contractile cell types using only a single scaling factor. The digital
TFM serves as a high-throughput tool for mapping out cellular forces of cell
monolayers and paves the way toward data-driven discoveries in cell
mechanobiology."
2,3,A minimal physical model for curvotaxis driven by curved protein complexes at the cell's leading edge,"Cells often migrate on curved surfaces inside the body, such as curved
tissues, blood vessels or highly curved protrusions of other cells. Recent
\textit{in-vitro} experiments provide clear evidence that motile cells are
affected by the curvature of the substrate on which they migrate, preferring
certain curvatures to others, termed ``curvotaxis"". The origin and underlying
mechanism that gives rise to this curvature sensitivity are not well
understood. Here, we employ a ``minimal cell"" model which is composed of a
vesicle that contains curved membrane protein complexes, that exert protrusive
forces on the membrane (representing the pressure due to actin polymerization).
This minimal-cell model gives rise to spontaneous emergence of a motile
phenotype, driven by a lamellipodia-like leading edge. By systematically
screening the behaviour of this model on different types of curved substrates
(sinusoidal, cylinder and tube), we show that minimal ingredients and energy
terms capture the experimental data. The model recovers the observed migration
on the sinusoidal substrate, where cells move along the grooves (minima), while
avoiding motion along the ridges. In addition, the model predicts the tendency
of cells to migrate circumferentially on convex substrates and axially on
concave ones. Both of these predictions are verified experimentally, on several
cell types. Altogether, our results identify the minimization of
membrane-substrate adhesion energy and binding energy between the membrane
protein complexes as key players of curvotaxis in cell migration."
1,3,Relax! Diffusion is not the only way to estimate axon radius in vivo,"Axon radius is a potential biomarker for brain diseases and a crucial tissue
microstructure parameter that determines the speed of action potentials.
Diffusion MRI (dMRI) allows non-invasive estimation of axon radius, but
accurately estimating the radius of axons in the human brain is challenging.
Most axons in the brain have a radius below one micrometre, which falls below
the sensitivity limit of dMRI signals even when using the most advanced human
MRI scanners. Therefore, new MRI methods that are sensitive to small axon radii
are needed. In this proof-of-concept investigation, we examine whether a
surface-based axonal relaxation process could mediate a relationship between
intra-axonal T2 and T1 times and inner axon radius, as measured using
postmortem histology. A unique in vivo human diffusion-T1-T2 relaxation dataset
was acquired on a 3T MRI scanner with ultra-strong diffusion gradients, using a
strong diffusion-weighting (i.e., b=6000 s/mm2) and multiple inversion and echo
times. A second reduced diffusion-T2 dataset was collected at various echo
times to evaluate the model further. The intra-axonal relaxation times were
estimated by fitting a diffusion-relaxation model to the orientation-averaged
spherical mean signals. Our analysis revealed that the proposed surface-based
relaxation model effectively explains the relationship between the estimated
relaxation times and the histological axon radius measured in various corpus
callosum regions. Using these histological values, we developed a novel
calibration approach to predict axon radius in other areas of the corpus
callosum. Notably, the predicted radii and those determined from histological
measurements were in close agreement."
2,3,Surface-guided computing to analyze subcellular morphology and membrane-associated signals in 3D,"Signal transduction and cell function are governed by the spatiotemporal
organization of membrane-associated molecules. Despite significant advances in
visualizing molecular distributions by 3D light microscopy, cell biologists
still have limited quantitative understanding of the processes implicated in
the regulation of molecular signals at the whole cell scale. In particular,
complex and transient cell surface morphologies challenge the complete sampling
of cell geometry, membrane-associated molecular concentration and activity and
the computing of meaningful parameters such as the cofluctuation between
morphology and signals. Here, we introduce u-Unwrap3D, a framework to remap
arbitrarily complex 3D cell surfaces and membrane-associated signals into
equivalent lower dimensional representations. The mappings are bidirectional,
allowing the application of image processing operations in the data
representation best suited for the task and to subsequently present the results
in any of the other representations, including the original 3D cell surface.
Leveraging this surface-guided computing paradigm, we track segmented surface
motifs in 2D to quantify the recruitment of Septin polymers by blebbing events;
we quantify actin enrichment in peripheral ruffles; and we measure the speed of
ruffle movement along topographically complex cell surfaces. Thus, u-Unwrap3D
provides access to spatiotemporal analyses of cell biological parameters on
unconstrained 3D surface geometries and signals."
1,3,Universal Transitions between Growth and Dormancy via Intermediate Complex Formation,"A simple cell model consisting of a catalytic reaction network with
intermediate complex formation is numerically studied. As nutrients are
depleted, the transition from the exponential growth phase to the
growth-arrested dormant phase occurs along with hysteresis and a lag time for
growth recovery. This transition is caused by the accumulation of intermediate
complexes, leading to the jamming of reactions and the diversification of
components. These properties are generic in random reaction networks, as
supported by dynamical systems analyses of corresponding mean-field models."
2,3,Precise and scalable self-organization in mammalian pseudo-embryos,"During multi-cellular development, highly reproducible gene expression
patterns determine cellular fates precisely in time and space. These processes
are crucial during the earliest stages when the body plan and the future
asymmetric body axes emerge at gastrulation. In some species, such as flies and
worms, these early processes achieve near-single-cell spatial precision, even
for macroscopic patterns. However, we know little about such accuracy in
mammalian development, where quantitative approaches are limited. Using an in
vitro model for mammalian development, i.e., gastruloids, we demonstrate that
gene expression patterns are reproducible to within 20% in protein
concentration variability, which translates to a positional error close to a
single cell diameter at the tissue scale. In addition, 2-3 fold system size
changes lead to scaled gene expression patterns again on the order of an
individual cell diameter. Our results reveal developmental precision,
reproducibility, and size scaling for mammalian systems. All three properties
spontaneously arise in self-organizing cell aggregates and could thus be
fundamental features of multicellularity."
2,3,Overflow metabolism stems from growth optimization and cell heterogeneity,"A classic problem in metabolism is that fast-proliferating cells use the
seemingly wasteful fermentation pathway to generate energy in the presence of
sufficient oxygen. This counterintuitive phenomenon known as overflow
metabolism, or the Warburg effect in cancer, is universal across a wide range
of organisms, including bacteria, fungi, and mammalian cells. Despite nearly a
century of research and intense interest over the past 20 years, the origin and
function of this phenomenon remain unclear. Here, we take Escherichia coli as a
typical example and show that overflow metabolism can be understood from growth
optimization combined with cell heterogeneity. A model of optimal protein
allocation, coupled with the cell heterogeneity in enzyme catalytic rates,
quantitatively explains why and how cells make the choice between respiration
and fermentation under different nutrient conditions. In particular, our model
quantitatively illustrates the growth rate dependence of fermentation flux and
enzyme allocation under various types of perturbations, which are fully
verified by experimental results. Our work solves the long-standing puzzle of
overflow metabolism and can be broadly used to address heterogeneity-related
challenges in metabolism."
2,3,Simulations of 3D organoids suggest inhibitory neighbour-neighbour signalling as a possible growth mechanism in EGFR-L858R mutant alveolar type II cells,"Mutations in the epidermal growth factor receptor (EGFR) are common in
non-small cell lung cancer (NSCLC), particularly in never-smoker patients.
However, these mutations are not always carcinogenic, and have recently been
reported in histologically normal lung tissue from patients with and without
lung cancer. To investigate the outcome of EGFR mutation in healthy lung stem
cells, we grew murine alveolar type-II organoids monoclonally in a 3D Matrigel.
Our experiments showed that the EGFR-L858R mutation induced a change in
organoid structure: mutated organoids displayed more `budding', in comparison
to non-mutant controls, which were nearly spherical. On-lattice computational
simulation suggested that this can be explained by an increase in reproductive
fitness combined with inhibitory neighbour-neighbour signalling in mutated
organoids. We predict that these effects prohibit division by cells in the
interior of the organoid and boost the fitness of cells on the surface, since
this differential growth is sufficient to cause `budding' structures in our
simulations. These results suggest that the L858R mutation produces structures
which expand quickly from surface protrusions. We suggest that the likelihood
of L858R-fuelled tumorigenesis is affected not just by random fluctuations in
cell fitness, but by whether the mutation arises in a spatial environment that
allows mutant cells to reproduce without being forced to encounter each other.
These data may have implications for cancer prevention strategies and for
understanding NSCLC progression."
2,3,Initiation of motility on a compliant substrate,"The conditions under which biological cells switch from a static to a motile
state are fundamental to the understanding of many healthy and pathological
processes. We show that even in the presence of a fully symmetric protrusive
activity at the cell edges, such a spontaneous transition can result solely
from the mechanical interaction of the cell traction forces with an elastic
substrate. The loss of symmetry of the traction forces leading to the cell
propulsion is rooted in the fact that the surface loading follows the substrate
deformation. We analytically characterize the bifurcation between the static
and motile states and, considering the measurements performed on two cell
types, we show that such an instability can realistically occur on soft in vivo
substrates."
2,3,"Swimming, Feeding and Inversion of Multicellular Choanoflagellate Sheets","The recent discovery of the striking sheet-like multicellular
choanoflagellate species $Choanoeca~flexa$ that dynamically interconverts
between two hemispherical forms of opposite orientation raises fundamental
questions in cell and evolutionary biology, as choanoflagellates are the
closest living relatives of animals. It similarly motivates questions in fluid
and solid mechanics concerning the differential swimming speeds in the two
states and the mechanism of curvature inversion triggered by changes in the
geometry of microvilli emanating from each cell. Here we develop fluid
dynamical and mechanical models to address these observations and show that
they capture the main features of the swimming, feeding, and inversion of
$C.~flexa$ colonies."
2,3,"MigraR: an open-source, R-based application for analysis and quantification of cell migration parameters","Background and objective: Cell migration is essential for many biological
phenomena with direct impact on human health and disease. One conventional
approach to study cell migration involves the quantitative analysis of
individual cell trajectories recorded by time-lapse video microscopy. Dedicated
software tools exist to assist the automated or semi-automated tracking of
cells and translate these into coordinate positions along time. However, cell
biologists usually bump into the difficulty of plotting and computing these
data sets into biologically meaningful figures and metrics. Methods: This
report describes MigraR, an intuitive graphical user interface executed from
the RStudioTM (via the R package Shiny), which greatly simplifies the task of
translating coordinate positions of moving cells into measurable parameters of
cell migration (velocity, straightness, and direction of movement), as well as
of plotting cell trajectories and migration metrics. One innovative function of
this interface is that it allows users to refine their data sets by setting
limits based on time, velocity and straightness. Results: MigraR was tested on
different data to assess its applicability. Intended users of MigraR are cell
biologists with no prior knowledge of data analysis, seeking to accelerate the
quantification and visualization of cell migration data sets delivered in the
format of Excel files by available cell-tracking software. Conclusions: Through
the graphics it provides, MigraR is an useful tool for the analysis of
migration parameters and cellular trajectories. Since its source code is open,
it can be subject of refinement by expert users to best suit the needs of other
researchers. It is available at GitHub and can be easily reproduced."
2,3,Intercellular competitive growth dynamics with microenvironmental feedback,"Normal life activities between cells rely crucially on the homeostasis of the
cellular microenvironment, but aging and cancer will upset this balance. In
this paper, we introduce the microenvironmental feedback mechanism to the
growth dynamics of multicellular organisms, which changes the cellular
competitive ability, and thereby regulates the growth of multicellular
organisms. We show that the presence of microenvironmental feedback can
effectively delay aging, but cancer cells may grow uncontrollably due to the
emergence of the tumor microenvironment (TME). We study the effect of the
fraction of cancer cells relative to that of senescent cells on the feedback
rate of the microenvironment on the lifespan of multicellular organisms, and
find that the average lifespan shortened is close to the data for non-Hodgkin
lymphoma in Canada from 1980 to 2015. We also investigate how the competitive
ability of cancer cells affects the lifespan of multicellular organisms, and
reveal that there is an optimal value of the competitive ability of cancer
cells allowing the organism to survive longest. Interestingly, the proposed
microenvironmental feedback mechanism can give rise to the phenomenon of
Parrondo's paradox: when the competitive ability of cancer cells switches
between a too high and a too low value, multicellular organisms are able to
live longer than in each case individually. Our results may provide helpful
clues targeted therapies aimed at TME."
2,3,The effect of a linear feedback mechanism in a homeostasis model,"Feedback loops are essential for regulating cell proliferation and
maintaining the delicate balance between cell division and cell death. Thanks
to the exact solution of a few simple models of cell growth it is by now clear
that stochastic fluctuations play a central role in this process and that cell
growth (and in particular the robustness and stability of homeostasis) can be
properly addressed only as a stochastic process. Using epidermal homeostasis as
a prototypical example, we show that it is possible to discriminate among
different feedback strategies which turn out to be characterized by different,
experimentally testable, behaviours. In particular, we focus on the so-called
Dynamical Heterogeneity model, an epidermal homeostasis model that takes into
account two well known cellular features: the plasticity of the cells and their
adaptability to face environmental stimuli. We show that specific choices of
the parameter on which the feedback is applied may decrease the fluctuations of
the homeostatic population level and improve the recovery of the system after
an external perturbation."
2,3,Spatial cancer systems biology resolves heterotypic interactions and identifies disruption of spatial hierarchy as a pathological driver event,"Spatially annotated single-cell datasets provide unprecedented opportunities
to dissect cell-cell communication in development and disease. Heterotypic
signaling includes interactions between different cell types and is well
established in tissue development and spatial organization. Epithelial
organization requires several different programs that are tightly regulated.
Planar cell polarity is the organization of epithelial cells along the planar
axis orthogonal to the apical-basal axis. In this study, we investigate planar
cell polarity factors and explore the implications of developmental regulators
as malignant drivers. Utilizing cancer systems biology analysis, we derive gene
expression network for WNT-ligands (WNT) and their cognate frizzled (FZD)
receptors in skin cutaneous melanoma. The profiles supported by unsupervised
clustering of multiple-sequence alignments identify ligand-independent
signaling and implications for metastatic progression based on the underpinning
developmental spatial program. Omics studies and spatial biology connect
developmental programs with oncological events and explain key spatial features
of metastatic aggressiveness. Dysregulation of prominent planar cell polarity
factors such specific representative of the WNT and FZD families in malignant
melanoma recapitulates the development program of normal melanocytes but in an
uncontrolled and disorganized fashion."
1,3,Methods and measures for investigating microscale motility,"Motility is an essential factor for an organism's survival and
diversification. With the advent of novel single-cell technologies, analytical
frameworks and theoretical methods, we can begin to probe the complex lives of
microscopic motile organisms and answer the intertwining biological and
physical questions of how these diverse lifeforms navigate their surroundings.
Herein, we give an overview of different experimental, analytical, and
mathematical methods used to study a suite of microscale motility mechanisms
across different scales encompassing molecular-, individual- to
population-level. We identify transferable techniques, pressing challenges, and
future directions in the field. This review can serve as a starting point for
researchers who are interested in exploring and quantifying the movements of
organisms in the microscale world."
2,3,Travelling waves in a coarse-grained model of volume-filling cell invasion: Simulations and comparisons,"Many reaction-diffusion models produce travelling wave solutions that can be
interpreted as waves of invasion in biological scenarios such as wound healing
or tumour growth. These partial differential equation models have since been
adapted to describe the interactions between cells and extracellular matrix
(ECM), using a variety of different underlying assumptions. In this work, we
derive a system of reaction-diffusion equations, with cross-species
density-dependent diffusion, by coarse-graining an agent-based, volume-filling
model of cell invasion into ECM. We study the resulting travelling wave
solutions both numerically and analytically across various parameter regimes.
Subsequently, we perform a systematic comparison between the behaviours
observed in this model and those predicted by simpler models in the literature
which do not take into account volume-filling effects in the same way. Our
study justifies the use of some of these simpler, more analytically tractable
models in reproducing the qualitative properties of the solutions in some
parameter regimes, but it also reveals some interesting properties arising from
the introduction of cell and ECM volume-filling effects, where standard model
simplifications might not be appropriate."
1,3,On the Liveliness of Artificial Life,"There has been on-going philosophical debate on whether artificial life
models, also known as digital organisms, are truly alive. The main difficulty
appears to be finding an encompassing and definite definition of life. By
examining similarities and differences in recent definitions of life, we define
life as ""any system with a boundary to confine the system within a definite
volume and protect the system from external effects, consisting of a program
that is capable of improvisation, able to react and adapt to the environment,
able to regenerate parts of it-self or its entirety, with energy system
comprises of non-interference sets of secluded reactions for self-sustenance,
is considered alive or a living system. Any incomplete system containing a
program and can be re-assembled into a living system; thereby, converting the
reassembled system for the purpose of the incomplete system, are also
considered alive."" Using this definition, we argue that digital organisms may
not be the boundary case of life even though some digital organisms are not
considered alive; thereby, taking the view that some form of digital organisms
can be considered alive. In addition, we present an experimental framework
based on continuity of the overall system and potential discontinuity of
elements within the system for testing future definitions of life."
2,3,Partial differential equation-based inference of migration and proliferation mechanisms in cancer cell populations,"Targeting signaling pathways that drive cancer cell migration or
proliferation is a common therapeutic approach. A popular experimental
technique, the scratch assay, measures the migration and proliferation-driven
cell monolayer formation. Scratch assay analyses do not differentiate between
migration and proliferation effects and do not attempt to measure dynamic
effects. To improve upon these methods, we combine high-throughput scratch
assays, continuous video microscopy, and variational system identification
(VSI) to infer partial differential equation (PDE) models of cell migration and
proliferation. We capture the evolution of cell density fields over time using
live cell microscopy and automated image processing. We employ VSI techniques
to identify cell density dynamics modeled with first-order kinetics of
advection-diffusion-reaction systems. We present a comparison of our methods to
results obtained using traditional inference approaches on previously analyzed
1-dimensional scratch assay data. We demonstrate the application of this
pipeline on high throughput 2-dimensional scratch assays and find that
decreasing serum levels can decrease random cell migration by approximately
20%. Our integrated experimental and computational pipeline can be adapted for
automatically quantifying the effect of biological perturbations on cell
migration and proliferation in various cell lines."
1,3,A Fast Second-Order Explicit Predictor-Corrector Numerical Technique To Investigating And Predicting The Dynamic Of Cytokine Levels And Human Immune Cells Activation In Response To Gram-Positive Bacteria: Staphylococcus Aureus,"This paper develops a second-order explicit predictor-corrector numerical
approach for solving a mathematical model on the dynamic of cytokine
expressions and human immune cell activation in response to the bacterium
staphylococcus aureus (S. aureus). The proposed algorithm is at least
zero-stable and second-order accurate. Mathematical modeling works that analyze
the human body in response to some antigens have predicted concentrations of a
broad range of cells and cytokines. This study deals with a coupled
cellular-cytokine model which predicts cytokine expressions in response to
gram-positive bacteria S. aureus. Tumor necrosis factor alpha, interleukin 6,
interleukin 8 and interleukin 10 are included to assess the relationship
between cytokine release from macrophages and the concentration of the S.
aureus antigen. Ordinary differential equations are used to model cytokine
levels while the cellular responses are modeled by partial differential
equations. Interactions between both components provide a more robust and
complete systems of immune activation. In the numerical simulations, a low
concentration of S. aureus is used to measure cellular activation and cytokine
expressions. Numerical experiments indicate how the human immune system
responds to infections from different pathogens. Furthermore, numerical
examples suggest that the new technique is faster and more efficient than a
large class of statistical and numerical schemes discussed in the literature
for systems of nonlinear equations and can serve as a robust tool for the
integration of general systems of initial-boundary value problems."
2,3,Physics of collective cell migration,"Movement of cell clusters along extracellular matrices (ECM) during tissue
development, wound healing, and early stage of cancer invasion involve various
inter-connected migration modes such as: (1) cell movement within clusters, (2)
cluster extension (wetting) and compression (de-wetting), and (3) directional
cluster movement. It has become increasingly evident that dilational and
volumetric viscoelasticity of cell clusters and their surrounding substrate
significantly influence these migration modes through physical parameters such
as: cell and matrix surface tensions, interfacial tension between cells and
substrate, gradients of surface and interfacial tensions, as well as, the
accumulation of cell and matrix residual stresses. Inhomogeneous distribution
of cell surface tension along migrating cell cluster can appear as a
consequence of different strength of cell-cell adhesion contacts and cell
contractility between leader and follower cells. While the directional cell
migration caused by the matrix stiffness gradient (i.e. durotaxis) has been
widely elaborated, the structural changes of matrix surface caused by cell
tractions which lead to the generation of the matrix surface tension gradient
has not been considered yet. The main goal of this theoretical consideration is
to clarify the roles of various physical parameters in collective cell
migration based on the formulating biophysical model. This complex phenomenon
is discussed on the model systems such as the movement of cell clusters on the
collagen I gel matrix by simultaneously reviewing various experimental data
with and without cells."
1,3,When does humoral memory enhance infection?,"Antibodies and humoral memory are key components of the adaptive immune
system. We consider and computationally model mechanisms by which humoral
memory present at baseline might instead increase infection load; we refer to
this effect as EI-HM (enhancement of infection by humoral memory). We first
consider antibody dependent enhancement (ADE) in which antibody enhances the
growth of the pathogen, typically a virus, and typically at intermediate
""Goldilocks"" levels of antibody. Our ADE model reproduces ADE in vitro and
enhancement of infection in vivo from passive antibody transfer. But notably
the simplest implementation of our ADE model never results in EI-HM. Adding
complexity, by making the cross-reactive antibody much less neutralizing than
the de novo generated antibody or by including a sufficiently strong
non-antibody immune response, allows for ADE-mediated EI-HM. We next consider
the possibility that cross-reactive memory causes EI-HM by crowding out a
possibly superior de novo immune response. We show that, even without ADE,
EI-HM can occur when the cross-reactive response is both less potent and
""directly"" (i.e. independently of infection load) suppressive with regard to
the de novo response. In this case adding a non-antibody immune response to our
computational model greatly reduces or completely eliminates EI-HM, which
suggests that ""crowding out"" is unlikely to cause substantial EI-HM. Hence, our
results provide examples in which simple models give qualitatively opposite
results compared to models with plausible complexity. Our results may be
helpful in interpreting and reconciling disparate experimental findings,
especially from dengue, and for vaccination."
1,3,Theoretical model of membrane protrusions driven by curved active proteins,"Eukaryotic cells intrinsically change their shape, by changing the
composition of their membrane and by restructuring their underlying
cytoskeleton. We present here further studies and extensions of a minimal
physical model, describing a closed vesicle with mobile curved membrane protein
complexes. The cytoskeletal forces describe the protrusive force due to actin
polymerization which is recruited to the membrane by the curved protein
complexes. We characterize the phase diagrams of this model, as function of the
magnitude of the active forces, nearest-neighbor protein interactions and the
proteins' spontaneous curvature. It was previously shown that this model can
explain the formation of lamellipodia-like flat protrusions, and here we
explore the regimes where the model can also give rise to filopodia-like
tubular protrusions. We extend the simulation with curved components of both
convex and concave species, where we find the formation of complex ruffled
clusters, as well as internalized invaginations that resemble the process of
endocytosis and macropinocytosis. We alter the force model representing the
cytoskeleton to simulate the effects of bundled instead of branched structure,
resulting in shapes which resemble filopodia."
1,3,Multiscale modelling of heavy metals adsorption on algal-bacterial photogranules,"A multiscale mathematical model describing the genesis and ecology of
algal-bacterial photogranules and the metals biosorption on their solid matrix
within a sequencing batch reactor (SBR) is presented. The granular biofilm is
modelled as a spherical free boundary domain with radial symmetry and a
vanishing initial value. The free boundary evolution is governed by an ODE
accounting for microbial growth, attachment and detachment phenomena. The model
is based on systems of PDEs derived from mass conservation principles.
Specifically, two systems of nonlinear hyperbolic PDEs model the growth of
attached species and the dynamics of free adsorption sites; and two systems of
quasi-linear parabolic PDEs govern the diffusive transport and conversion of
nutrients and metals. The model is completed with systems of impulsive ordinary
differential equations (IDEs) describing the evolution of dissolved substrates,
metals, and planktonic and detached biomasses within the granular-based SBR.
All main phenomena involved in the process are considered in the mathematical
model. Moreover, the dual effect of metal presence on the formation process of
photogranules is accounted: metal stimulates the production of EPS by sessile
species and negatively affects the metabolic activities of microbial species.
To describe the effects related to metal presence, a stimulation term for EPS
production and an inhibition term for metal are included in all microbial
kinetics. The model is used to examine the role of the microbial species and
EPS in the adsorption process, and the effect of metal concentration and
adsorption proprieties of biofilm components on the metal removal. Numerical
results show that the model accurately describes the photogranules evolution
and ecology and confirm the applicability of algal-bacterial photogranules
systems for metal-rich wastewater treatment."
1,3,High-density magnetomyography is superior over surface electromyography for the decomposition of motor units: a simulation study,"Studying motor units (MUs) is essential for understanding motor control, the
detection of neuromuscular disorders and the control of human-machine
interfaces. Individual motor unit firings are currently identified in vivo by
decomposing electromyographic (EMG) signals. Due to our body's electric
properties, individual motor units can only be separated to a limited extent
with surface EMG. Unlike electrical signals, magnetic fields pass through
biological tissues without distortion. This physical property and emerging
technology of quantum sensors make magnetomyography (MMG) a highly promising
methodology. However, the full potential of MMG to study neuromuscular
physiology has not yet been explored. In this work, we perform in silico trials
that combine a biophysical model of EMG and MMG with state-of-the-art
algorithms for the decomposition of motor units. This allows the prediction of
an upper-bound for the motor unit decomposition accuracy. It is shown that
non-invasive MMG is superior over surface EMG for the robust identification of
the discharge patterns of individual motor units. Decomposing MMG instead of
EMG increased the number of identifiable motor units by 71%. Notably, MMG
exhibits a less pronounced bias to detect superficial motor units. The
presented simulations provide insights into methods to study the neuromuscular
system non-invasively and in vivo that would not be easily feasible by other
means. Hence, this study provides guidance for the development of novel
biomedical technologies."
1,3,Are physiological oscillations 'physiological'?,"Despite widespread and striking examples of physiological oscillations, their
functional role is often unclear. Even glycolysis, the paradigm example of
oscillatory biochemistry, has seen questions about its oscillatory function.
Here, we take a systems approach to summarize evidence that oscillations play
critical physiological roles. Oscillatory behavior enables systems to avoid
desensitization, to avoid chronically high and therefore toxic levels of
chemicals, and to become more resistant to noise. Oscillation also enables
complex physiological systems to reconcile incompatible conditions such as
oxidation and reduction, by cycling between them, and to synchronize the
oscillations of many small units into one large effect. In pancreatic beta
cells, glycolytic oscillations are in synchrony with calcium and mitochondrial
oscillations to drive pulsatile insulin release, which is pivotal for the liver
to regulate blood glucose dynamics. In addition, oscillation can keep
biological time, essential for embryonic development in promoting cell
diversity and pattern formation. The functional importance of oscillatory
processes requires a rethinking of the traditional doctrine of homeostasis,
holding that physiological quantities are maintained at constant equilibrium
values, a view that has largely failed us in the clinic. A more dynamic
approach will enable us to view health and disease through a new light and
initiate a paradigm shift in treating diseases, including depression and
cancer. This modern synthesis also takes a deeper look into the mechanisms that
create, sustain and abolish oscillatory processes, which requires the language
of nonlinear dynamics, well beyond the linearization techniques of equilibrium
control theory."
2,3,How cells wrap around virus-like particles using extracellular filamentous protein structures,"Nanoparticles, such as viruses, can enter cells via endocytosis. During
endocytosis, the cell surface wraps around the nanoparticle to effectively eat
it. Prior focus has been on how nanoparticle size and shape impacts
endocytosis. However, inspired by the noted presence of extracellular vimentin
affecting viral and bacteria uptake, as well as the structure of coronaviruses,
we construct a computational model in which both the cell-like construct and
the virus-like construct contain filamentous protein structures protruding from
their surfaces. We then study the impact of these additional degrees of freedom
on viral wrapping. We find that cells with an optimal density of filamentous
extracellular components (ECCs) are more likely to be infected as they uptake
the virus faster and use relatively less cell surface area per individual
virus. At the optimal density, the cell surface folds around the virus, and
folds are faster and more efficient at wrapping the virus than crumple-like
wrapping. We also find that cell surface bending rigidity helps generate folds,
as bending rigidity enhances force transmission across the surface. However,
changing other mechanical parameters, such as the stretching stiffness of
filamentous ECCs or virus spikes, can drive crumple-like formation of the cell
surface. We conclude with the implications of our study on the evolutionary
pressures of virus-like particles, with a particular focus on the cellular
microenvironment that may include filamentous ECCs."
2,3,Evolutionary dynamics of glucose-deprived cancer cells: insights from experimentally-informed mathematical modelling,"Glucose is a primary energy source for cancer cells. Several lines of
evidence support the idea that monocarboxylate transporters, such as MCT1,
elicit metabolic reprogramming of cancer cells in glucose-poor environments,
allowing them to reuse lactate, a byproduct of glucose metabolism, as an
alternative energy source with serious consequences for disease progression. We
employ a synergistic experimental and mathematical modelling approach to
explore the evolutionary processes at the root of cancer cell adaptation to
glucose deprivation, with particular focus on the mechanisms underlying the
increase in MCT1 expression observed in glucose-deprived aggressive cancer
cells. Data from in vitro experiments on breast cancer cells are used to inform
and calibrate a mathematical model that comprises a partial
integro-differential equation for the dynamics of a population of cancer cells
structured by the level of MCT1 expression. Analytical and numerical results of
this model indicate that environment-induced changes in MCT1 expression
mediated by lactate-associated signalling pathways enable a prompt adaptive
response of glucose-deprived cancer cells, whilst spontaneous changes due to
non-genetic instability create the substrate for environmental selection to act
upon, speeding up the selective sweep underlying cancer cell adaptation to
glucose deprivation, and may constitute a long-term bet-hedging mechanism."
2,3,Cell decision-making through the lens of Bayesian learning,"Cell decision-making refers to the process by which cells gather information
from their local microenvironment and regulate their internal states to create
appropriate responses. Microenvironmental cell sensing plays a key role in this
process. Our hypothesis is that cell decision-making regulation is dictated by
Bayesian learning. In this article, we explore the implications of this
hypothesis for internal state temporal evolution. By using a timescale
separation between internal and external variables on the mesoscopic scale, we
derive a hierarchical Fokker-Planck equation for cell-microenvironment
dynamics. By combining this with the Bayesian learning hypothesis, we find that
changes in microenvironmental entropy dominate cell state probability
distribution. Finally, we use these ideas to understand how cell sensing
impacts cell decision-making. Notably, our formalism allows us to understand
cell state dynamics even without exact biochemical information about cell
sensing processes by considering a few key parameters."
1,3,Phenomenological analysis of simple ion channel block in large populations of uncoupled cardiomyocytes,"Current understanding of arrhythmia mechanisms and design of anti-arrhythmic
drug therapies hinges on the assumption that myocytes from the same region of a
single heart have similar, if not identical, action potential waveforms and
drug responses. On the contrary, recent experiments reveal significant
heterogeneity in uncoupled healthy myocytes both from different hearts as well
as from identical regions within a single heart. In this work, a methodology is
developed for quantifying the individual electrophysiological properties of
large numbers of uncoupled cardiomyocytes under ion channel block in terms of
the parameters values of a conceptual fast-slow model of electrical
excitability. The approach is applied to a population of nearly 500 rabbit
ventricular myocytes for which action potential duration (APD) before and after
the application of the drug nifedipine was experimentally measured (Lachaud et
al., 2022, Cardiovasc. Res.). To this end, drug action is represented by a
multiplicative factor to an effective ion conductance, a closed form asymptotic
expression for APD is derived and inverted to determine model parameters as
functions of APD and dAPD (drug-induced change in APD) for each myocyte. Two
free protocol-related quantities are calibrated to experiment using an
adaptive-domain procedure based on an original assumption of optimal
excitability. The explicit APD expression and the resulting set of model
parameter values allow (a) direct evaluation of conditions necessary to
maintain fixed APD or dAPD, (b) predictions of the proportion of cells
remaining excitable after drug application, (c) predictions of stimulus period
dependency and (d) predictions of dose-response curves, the latter being in
agreement with additional experimental data."
2,3,Symmetry-Breaking Bifurcations for Compartmental Reaction Kinetics Coupled by Two Bulk Diffusing Species with Comparable Diffusivities in 2-D,"For a 2-D coupled PDE-ODE bulk-cell model, we investigate symmetry-breaking
bifurcations that can emerge when two bulk diffusing species are coupled to
two-component nonlinear intracellular reactions that are restricted to occur
only within a disjoint collection of small circular compartments, or ""cells"",
of a common small radius that are confined in a bounded 2-D domain. Outside of
the union of these cells, the two bulk species with comparable diffusivities
and bulk degradation rates diffuse and globally couple the spatially segregated
intracellular reactions through Robin boundary conditions across the cell
boundaries, which depend on certain membrane reaction rates. In the singular
limit of a small common cell radius, we construct steady-state solutions for
the bulk-cell model and formulate a nonlinear matrix eigenvalue problem that
determines the linear stability properties of the steady-states. For a certain
spatial arrangement of cells for which the steady-state and linear stability
analysis become highly tractable, we construct a symmetric steady-state
solution where the steady-states of the intracellular species are the same for
each cell. As regulated by the ratio of the membrane reaction rates on the cell
boundaries, we show for various specific prototypical intracellular reactions,
and for a specific two-cell arrangement, that our 2-D coupled PDE-ODE model
admits symmetry-breaking bifurcations from this symmetric steady-state, leading
to linearly stable asymmetric patterns, even when the bulk diffusing species
have comparable or possibly equal diffusivities. Overall, our analysis shows
that symmetry-breaking bifurcations can occur without the large diffusivity
ratio requirement for the bulk diffusing species as is well-known from a Turing
stability analysis applied to a spatially uniform steady-state for typical
two-component activator-inhibitor systems."
1,3,The Emergence of Spatial Patterns for Compartmental Reaction Kinetics Coupled by Two Bulk Diffusing Species with Comparable Diffusivities,"Originating from the pioneering study of Alan Turing, the bifurcation
analysis predicting spatial pattern formation from a spatially uniform state
for diffusing morphogens or chemical species that interact through nonlinear
reactions is a central problem in many chemical and biological systems. From a
mathematical viewpoint, one key challenge with this theory for two component
systems is that stable spatial patterns can typically only occur from a
spatially uniform state when a slowly diffusing ""activator"" species reacts with
a much faster diffusing ""inhibitor"" species. However, from a modeling
perspective, this large diffusivity ratio requirement for pattern formation is
often unrealistic in biological settings since different molecules tend to
diffuse with similar rates in extracellular spaces. As a result, one key
long-standing question is how to robustly obtain pattern formation in the
biologically realistic case where the time scales for diffusion of the
interacting species are comparable. For a coupled 1-D bulk-compartment
theoretical model, we investigate the emergence of spatial patterns for the
scenario where two bulk diffusing species with comparable diffusivities are
coupled to nonlinear reactions that occur only in localized ""compartments"",
such as on the boundaries of a 1-D domain. The exchange between the bulk medium
and the spatially localized compartments is modeled by a Robin boundary
condition with certain binding rates. As regulated by these binding rates, we
show for various specific nonlinearities that our 1-D coupled PDE-ODE model
admits symmetry-breaking bifurcations, leading to linearly stable asymmetric
steady-state patterns, even when the bulk diffusing species have equal
diffusivities. Depending on the form of the nonlinear kinetics, oscillatory
instabilities can also be triggered. Moreover, the analysis is extended to
treat a periodic chain of compartments."
2,3,A phenotype-structured model for the tumour-immune response,"This paper presents a mathematical model for tumour-immune response
interactions in the perspective of immunotherapy by immune checkpoint
inhibitors (ICIs). The model is of the integrodifferential Lotka-Volterra type,
in which heterogeneity of the cell populations is taken into account by
structuring variables that are continuous internal traits (aka phenotypes)
representing a lumped ''aggressiveness'', i.e., for tumour cells, ability to
thrive in a viable state under attack by immune cells or drugs-which we propose
to identify as a potential of de-differentiation-, and for immune cells,
ability to kill tumour cells. We analyse the asymptotic behaviour of the model
in the absence of treatment. By means of two theorems, we characterise the
limits of the integro-differential system under an a priori convergence
hypothesis. We illustrate our results with numerical simulations, which show
that our model exemplifies the three Es of immunoediting: elimination,
equilibrium, and escape."
1,3,Making sense of noise: introducing students to stochastic processes in order to better understand biological behaviors,"Biological systems are characterized by the ubiquitous roles of weak, that
is, non-covalent molecular interactions, small, often very small, numbers of
specific molecules per cell, and Brownian motion. These combine to produce
stochastic behaviors at all levels from the molecular and cellular to the
behavioral. That said, students are rarely introduced to the ubiquitous role of
stochastic processes in biological systems, and how they produce unpredictable
behaviors. Here I present the case that they need to be and provide some
suggestions as to how it might be approached."
1,3,Inferring Gene Regulatory Neural Networks for Bacterial Decision Making in Biofilms,"Bacterial cells are sensitive to a range of external signals used to learn
the environment. These incoming external signals are then processed using a
Gene Regulatory Network (GRN), exhibiting similarities to modern computing
algorithms. An in-depth analysis of gene expression dynamics suggests an
inherited Gene Regulatory Neural Network (GRNN) behavior within the GRN that
enables the cellular decision-making based on received signals from the
environment and neighbor cells. In this study, we extract a sub-network of
\textit{Pseudomonas aeruginosa} GRN that is associated with one virulence
factor: pyocyanin production as a use case to investigate the GRNN behaviors.
Further, using Graph Neural Network (GNN) architecture, we model a single
species biofilm to reveal the role of GRNN dynamics on ecosystem-wide
decision-making. Varying environmental conditions, we prove that the extracted
GRNN computes input signals similar to natural decision-making process of the
cell. Identifying of neural network behaviors in GRNs may lead to more accurate
bacterial cell activity predictive models for many applications, including
human health-related problems and agricultural applications. Further, this
model can produce data on causal relationships throughout the network, enabling
the possibility of designing tailor-made infection-controlling mechanisms. More
interestingly, these GRNNs can perform computational tasks for bio-hybrid
computing systems."
1,3,mRNA active transport in oocyte-early embryo: 3D agent-based modeling,"Axes of polarity (and primary morphogenetic gradients) are established in the
oocyte - early embryo through active transport and localization of maternal
factors. It is the oocyte - syncytial embryo of Drosophila (D. melanogaster)
that is a model object for studying the molecular machinery of such transport
systems. The attention of researchers is focused on the processes of formation,
maintenance, and functioning of active transport systems of maternal mRNAs and
proteins that are key for early Drosophila embryogenesis. Here we develop an
approach for agent-based 3D modeling of the key components of transport by
molecular motors (by elements of the cytoskeleton) of the Drosophila
oocyte-syncytial embryo. The models were developed using Skeledyne software
developed by Odell and Foe [Odell and Foe, 2008]. We start with the results of
modeling transport along oriented microtubule (MT) bundles in the oocyte. This
is a model of transport systems in the Drosophila oocyte, where three maternal
mRNAs (bicoid (bcd), oskar, and gurken) that are key to embryonic polarity are
transported along their oriented MT bundles. Then we consider models of
oriented MT networks in the volume of a cell (oocyte) generated by a single
microtubule organization center (or a pair of the centers). This model
reproduces the formation of bcd mRNA intrusions deep into the cytoplasm in the
head half of the early syncytial embryo. Finally, we consider models for the
active transport of bcd mRNA in a syncytial embryo along a randomized network
of many short MT strands. In conclusion, we consider the prospects for the
implementation of cytoplasmic fountain flows in the active transport model."
2,3,Measuring and simulating the biophysical basis of the acoustic contrast factor of biological cells,"The acoustic contrast factor (ACF) is calculated from the relative density
and compressibility differences between a fluid and an object in the fluid. To
name but one application, this acoustic contrast can be exploited using
acoustophoretic systems to isolate cancer cells from a liquid biopsy, such as a
blood sample. Knowing the ACF of a cancer cell represents a crucial step in the
design of acoustophoretic systems for this purpose, potentially allowing the
isolation of circulating cancer cells without labels or contact. For biological
cells the static compressibility is different from the high frequency
counterpart relevant for the ACF. In this study, we started by characterizing
the ACF of low vs. high metastatic cell lines with known associated differences
in phenotypic static E-modulus. The change in the static E-modulus, however,
was not reflected in a change of the ACF, prompting a more in depth analysis of
the influences on the ACF. We demonstrate that static E-modulus increased
biological cells through formaldehyde fixation have an increased ACF.
Conversely static E-modulus decreased biological cells treated with actin
polymerization inhibitor cytochalasin D have a decreased ACF. Complementing
these mechanical tests, a numerical COMSOL model was implemented and used to
parametrically explore the effects of cell density, cell density ratios,
dynamic compressibility and therefore the dynamic bulk modulus. Collectively
the combined laboratory and numerical experiments reveal that a change in the
static E-modulus alone might, but does not automatically lead to a change of
the dynamic ACF for biological cells. This highlights the need for a
multiparametic view of the biophysical basis of the cellular ACF, as well as
the challenges in harnessing acoustophoretic systems to isolate circulating
cells based on their mechanical properties alone."
2,3,The Growing Liberality Observed in Primary Animal and Plant Cultures is Common to the Social Amoeba,"Tissue culture environment liberates cells from ordinary laws of
multi-cellular organisms. This liberation enables cells several behaviors, such
as proliferation, dedifferentiation, acquisition of pluripotency,
immortalization, and reprogramming. Recently, the quantitative value of
cellular dedifferentiation and differentiation was defined as liberality, which
is measurable as Shannon entropy of numerical transcriptome data and Lempel-Zip
complexity of nucleotide sequence transcriptome data. The increasing liberality
induced by the culture environment had first been observed in animal cells and
had reconfirmed in plant cells. The phenomena may be common across the kingdom,
also in a social amoeba. We measured the liberality of the social amoeba which
disaggregated from multicellular aggregates and transferred into a liquid
medium."
1,3,Tunable intracellular transport on converging microtubule morphologies,"A common type of cytoskeletal morphology involves multiple converging
microbutubules with their minus ends collected and stabilized by a microtubule
organizing center (MTOC) in the interior of the cell. This arrangement enables
the ballistic transport of cargo bound to microtubules, both dynein mediated
transport towards the MTOC and kinesin mediated transport away from it,
interspersed with diffusion for unbound cargo-motor complexes. Spatial and
temporal positioning of the MTOC allows for bidirectional transport towards and
away from specific organelles and locations within the cell and also the
sequestering and subsequent dispersal of dynein transported cargo. The general
principles governing dynamics, efficiency and tunability of such transport in
the MTOC vicinity is not fully understood. To address this, we develop a
one-dimensional model that includes advective transport towards an attractor
(such as the MTOC), and diffusive transport that allows particles to reach
absorbing boundaries (such as cellular membranes). We calculated the mean first
passage time (MFPT) for cargo to reach the boundaries as a measure of the
effectiveness of sequestering (large MFPT) and diffusive dispersal (low MFPT).
The MFPT experiences a dramatic growth in magnitude, transitioning from a low
to high MFPT regime (dispersal to sequestering) over a window of cargo
attachment/detachment rates that is close to in vivo values. We find that
increasing either the attachment or detachment rate, while fixing the other,
can result in optimal dispersal when the attractor is placed asymmetrically.
Finally, we describe a rare event regime, where the escape location is
exponentially sensitive to the attractor positioning. Our results suggest that
structures such as the MTOC allow for the sensitive control of the spatial and
temporal features of transport and corresponding function under physiological
conditions."
1,3,Segmentation based tracking of cells in 2D+time microscopy images of macrophages,"The automated segmentation and tracking of macrophages during their migration
are challenging tasks due to their dynamically changing shapes and motions.
This paper proposes a new algorithm to achieve automatic cell tracking in
time-lapse microscopy macrophage data. First, we design a segmentation method
employing space-time filtering, local Otsu's thresholding, and the SUBSURF
(subjective surface segmentation) method. Next, the partial trajectories for
cells overlapping in the temporal direction are extracted in the segmented
images. Finally, the extracted trajectories are linked by considering their
direction of movement. The segmented images and the obtained trajectories from
the proposed method are compared with those of the semi-automatic segmentation
and manual tracking. The proposed tracking achieved 97.4% of accuracy for
macrophage data under challenging situations, feeble fluorescent intensity,
irregular shapes, and motion of macrophages. We expect that the automatically
extracted trajectories of macrophages can provide pieces of evidence of how
macrophages migrate depending on their polarization modes in the situation,
such as during wound healing."
2,3,Short time extremal response to step stimulus for a single cell {\sl E. coli},"After application of a step stimulus, in the form of a sudden change in
attractant environment, the receptor activity and tumbling bias of an {\sl E.
coli} cell change sharply to reach their extremal values before they gradually
relax to their post-stimulus adapted levels in the long time limit. We perform
numerical simulations and exact calculations to investigate the short time
response of the cell. For both activity and tumbling bias, we exactly derive
the condition for extremal response and find good agreement with simulations.
We also make experimentally verifiable prediction that there is an optimum size
of the step stimulus at which the extremal response is reached in the shortest
possible time."
1,3,How different are self and nonself?,"Biological and artificial neural networks routinely make reliable
distinctions between similar inputs, and the rules for making these
distinctions are learned. In some ways, self/nonself discrimination in the
immune system is similar, being both reliable and (partly) learned through
thymic selection. In contrast to other examples, we show that the distributions
of self and nonself peptides are nearly identical but strongly inhomogeneous.
Reliable discrimination is possible only because self peptides are a particular
finite sample drawn out of this distribution, and the immune system can target
the ``spaces'' in between these samples. In conventional learning problems,
this would constitute overfitting and lead to disaster. Here, the strong
inhomogeneities imply instead that the immune system gains by targeting
peptides which are very similar to self, with maximum sensitivity for sequences
just one substitution away. This prediction from the structure of the
underlying distribution in sequence space agrees, for example, with the
observed responses to cancer neoantigens."
1,3,Quantifying different modeling frameworks using topological data analysis: a case study with zebrafish patterns,"Mathematical models come in many forms across biological applications. In the
case of complex, spatial dynamics and pattern formation, stochastic models also
face two main challenges: pattern data is largely qualitative, and model
realizations may vary significantly. Together these issues make it difficult to
relate models and empirical data -- or even models and models -- limiting how
different approaches can be combined to offer new insights into biology. These
challenges also raise mathematical questions about how models are related,
since alternative approaches to the same problem -- e.g., cellular Potts
models; off-lattice, agent-based models; on-lattice, cellular automaton models;
and continuum approaches -- treat uncertainty and implement cell behavior in
different ways. To help open the door to future work on questions like these,
here we adapt methods from topological data analysis and computational geometry
to quantitatively relate two different models of the same biological process in
a fair, comparable way. To center our work and illustrate concrete challenges,
we focus on the example of zebrafish-skin pattern formation, and we relate
patterns that arise from agent-based and cellular automaton models."
1,3,"There's Plenty of Room Right Here: Biological Systems as Evolved, Overloaded, Multi-scale Machines","The applicability of computational models to the biological world is an
active topic of debate. We argue that a useful path forward results from
abandoning hard boundaries between categories and adopting an
observer-dependent, pragmatic view. Such a view dissolves the contingent
dichotomies driven by human cognitive biases (e.g., tendency to oversimplify)
and prior technological limitations in favor of a more continuous, gradualist
view necessitated by the study of evolution, developmental biology, and
intelligent machines. Efforts to re-shape living systems for biomedical or
bioengineering purposes require prediction and control of their function at
multiple scales. This is challenging for many reasons, one of which is that
living systems perform multiple functions in the same place at the same time.
We refer to this as ""polycomputing"" - the ability of the same substrate to
simultaneously compute different things. This ability is an important way in
which living things are a kind of computer, but not the familiar, linear,
deterministic kind; rather, living things are computers in the broad sense of
computational materials as reported in the rapidly-growing physical computing
literature. We argue that an observer-centered framework for the computations
performed by evolved and designed systems will improve the understanding of
meso-scale events, as it has already done at quantum and relativistic scales.
Here, we review examples of biological and technological polycomputing, and
develop the idea that overloading of different functions on the same hardware
is an important design principle that helps understand and build both evolved
and designed systems. Learning to hack existing polycomputing substrates, as
well as evolve and design new ones, will have massive impacts on regenerative
medicine, robotics, and computer engineering."
2,3,Recognition and reconstruction of cell differentiation patterns with deep learning,"Cell lineage decisions occur in three-dimensional spatial patterns that are
difficult to identify by eye. There is an ongoing effort to replicate such
patterns using mathematical modeling. One approach uses long ranging cell-cell
communication to replicate common spatial arrangements like checkerboard and
engulfing patterns. In this model, the cell-cell communication has been
implemented as a signal that disperses throughout the tissue. On the other
hand, machine learning models have been developed for pattern recognition and
pattern reconstruction tasks. We combined synthetic data generated by the
mathematical model with deep learning algorithms to recognize and reconstruct
spatial cell fate patterns in organoids of mouse embryonic stem cells. A graph
neural network was developed and trained on synthetic data from the model.
Application to in vitro data predicted a low signal dispersion value. To test
this result, we implemented a multilayer perceptron for the prediction of a
given cell fate based on the fates of the neighboring cells. The results show a
70% accuracy of cell fate reconstruction based on the nine nearest neighbors of
a cell. Overall, our approach combines deep learning with mathematical modeling
to link cell fate patterns with potential underlying mechanisms."
1,3,A new lipid-structured model to investigate the opposing effects of LDL and HDL on atherosclerotic plaque macrophages,"Atherosclerotic plaques form in artery walls due to a chronic inflammatory
response driven by lipid accumulation. A key component of the inflammatory
response is the interaction between monocyte-derived macrophages and
extracellular lipid. Although concentrations of low-density lipoprotein (LDL)
and high-density lipoprotein (HDL) particles in the blood are known to affect
plaque progression, their impact on the lipid load of plaque macrophages
remains unexplored. In this paper, we develop a lipid-structured mathematical
model to investigate the impact of blood LDL/HDL levels on plaque composition,
and lipid distribution in plaque macrophages. A reduced subsystem, derived by
summing the equations of the full model, describes the dynamics of biophysical
quantities relating to plaque composition (e.g. total number of macrophages,
total amount of intracellular lipid). We also derive a continuum approximation
of the model to facilitate analysis of the macrophage lipid distribution. The
results, which include time-dependent numerical solutions and asymptotic
analysis of the unique steady state solution, indicate that plaque lipid
content is sensitive to the influx of LDL relative to HDL capacity. The
macrophage lipid distribution evolves in a wave-like manner towards an
equilibrium profile which may be monotone decreasing, quasi-uniform or
unimodal, attaining its maximum value at a non-zero lipid level. Our model also
reveals that macrophage uptake may be severely impaired by lipid accumulation.
We conclude that lipid accumulation in plaque macrophages may serve as a
partial explanation for the defective uptake of apoptotic cells (efferocytosis)
often reported in atherosclerotic plaques."
1,3,Modelling and Economic Optimal Control for a Laboratory-scale Continuous Stirred Tank Reactor for Single-cell Protein Production,"In this paper, we present a novel kinetic growth model for the micro-organism
\textit{Methylococcus capsulatus} (Bath) that couples growth and pH. We apply
growth kinetics in a model for single-cell protein production in a
laboratory-scale continuous stirred tank reactor inspired by a physical
laboratory fermentor. The model contains a set of differential algebraic
equations describing growth and pH-dynamics in the system. We present a method
of simulation that ensures non-negativity in the state and algebraic variables.
Additionally, we introduce linear scaling of the algebraic equations and
variables for numerical stability in Newton's method. Finally, we conduct a
numerical experiment of economic optimal control for single-cell protein
production in the laboratory-scale reactor. The numerical experiment shows
non-trivial input profiles for biomass growth and pH tracking."
2,3,The influence of nucleus mechanics in modelling adhesion-independent cell migration in structured and confined environments,"Recent biological experiments have shown that certain types of cells are able
to move in structured and confined environment even without the activation of
focal adhesion. Focusing on this particular phenomenon and based on previous
works, we derive a novel two-dimensional mechanical model, which relies on the
following physical ingredients: the asymmetrical renewal of the actin cortex
supporting the membrane, resulting in a backward flow of material; the
mechanical description of the nucleus membrane and the inner nuclear material;
the microtubule network guiding nucleus location; the contact interactions
between the cell and the external environment. The resulting fourth order
system of partial differential equations is then solved numerically to conduct
a study of the qualitative effects of the model parameters, mainly those
governing the mechanical properties of the nucleus and the geometry of the
confining structure. Coherently with biological observations, we find that
cells characterized by a stiff nucleus are unable to migrate in channels that
can be crossed by cells with a softer nucleus. Regarding the geometry, cell
velocity and ability to migrate are influenced by the width of the channel and
the wavelength of the external structure. Even though still preliminary, these
results can be potentially useful in determining the physical limit of cell
migration in confined environment and in designing scaffold for tissue
engineering."
1,3,A qualitative analysis of a A$$-monomer model with inflammation processes for Alzheimer's disease,"We introduce and study a new model for the progression of Alzheimer's disease
incorporating the interactions of A$\beta$-monomers, oligomers, microglial
cells and interleukins with neurons through different mechanisms such as
protein polymerization, inflammation processes and neural stress reactions. In
order to understand the complete interactions between these elements, we study
a spatially-homogeneous simplified model that allows to determine the effect of
key parameters such as degradation rates in the asymptotic behavior of the
system and the stability of equilibriums. We observe that inflammation appears
to be a crucial factor in the initiation and progression of Alzheimer's disease
through a phenomenon of hysteresis, which means that there exists a critical
threshold of initial concentration of interleukins that determines if the
disease persists or not in the long term. These results give perspectives on
possible anti-inflammatory treatments that could be applied to mitigate the
progression of Alzheimer's disease. We also present numerical simulations that
allow to observe the effect of initial inflammation and concentration of
monomers in our model."
3,3,"Topological transitions, turbulent-like motion and long-time-tails driven by cell division in biological tissues","The complex spatiotemporal flow patterns in living tissues, driven by active
forces, have many of the characteristics associated with inertial turbulence
even though the Reynolds number is extremely low. Analyses of experimental data
from two-dimensional epithelial monolayers in combination with agent-based
simulations show that cell division and apoptosis lead to directed cell motion
for hours, resulting in rapid topological transitions in neighboring cells.
These transitions in turn generate both long ranged and long lived clockwise
and anticlockwise vortices, which gives rise to turbulent-like flows. Both
experiments and simulations show that at long wavelengths the wave vector ($k$)
dependent energy spectrum $E(k) \approx k^{-5/3}$, coinciding with the
Kolmogorov scaling in fully developed inertial turbulence. Using theoretical
arguments and simulations, we show that long-lived vortices lead to long-time
tails in the velocity auto-correlation function, $C_v(t) \sim t^{-1/2}$, which
has the same structure as in classical 2D fluids but with a different scaling
exponent."
1,3,Mathematical Modeling of Leukemia Chemotherapy in Bone Marrow,"Acute Lymphoblastic Leukemia (ALL) accounts for the 80% of leukemias when
coming down to pediatric ages. Survival of these patients has increased by a
considerable amount in recent years. However, around 15-20% of treatments are
unsuccessful. For this reason, it is definitely required to come up with new
strategies to study and select which patients are at higher risk of relapse.
Thus the importance to monitor the amount of leukemic cells to predict relapses
in the first treatment phase. In this work we develop a mathematical model
describing the behavior of ALL, examining the evolution of a leukemic clone
when treatment is applied. In the study of this model it can be observed how
the risk of relapse is connected with the response in the first treatment
phase. This model is able to simulate cell dynamics without treatment,
representing a virtual patient bone marrow behavior. Furthermore, several
parameters are related to treatment dynamics, therefore proposing a basis for
future works regarding childhood ALL survival improvement."
2,3,The Role of Cytonemes and Diffusive Transport in the Establishment of Morphogen Gradients,"Spatial distributions of morphogens provide positional information in
  developing systems, but how the distributions are established and maintained
  remains an open problem. Transport by diffusion has been the
  traditional mechanism, but recent experimental work has shown that cells can
  also communicate by filopodia-like structures called cytonemes that make
direct
  cell-to-cell contacts. Here we investigate the roles each may play
  individually in a complex tissue and how they can jointly establish a
reliable
  spatial distribution of a morphogen."
2,3,Formation of vascular-like structures using a chemotaxis-driven multiphase model,"We propose a continuum model for pattern formation, based on the multiphase
model framework, to explore in vitro cell patterning within an extracellular
matrix. We demonstrate that, within this framework, chemotaxis-driven cell
migration can lead to formation of cell clusters and vascular-like structures
in 1D and 2D respectively. The influence on pattern formation of additional
mechanisms commonly included in multiphase tissue models, including cell-matrix
traction, contact inhibition, and cell-cell aggregation, are also investigated.
Using sensitivity analysis, the relative impact of each model parameter on the
simulation outcomes is assessed to identify the key parameters involved.
Chemoattractant-matrix binding is further included, motivated by previous
experimental studies, and to augment the spatial scale of patterning to within
a biologically plausible range. Key findings from the in-depth parameter
analysis of the 1D models, both with and without chemoattractant-matrix
binding, are demonstrated to translate well to the 2D model, obtaining
vascular-like cell patterning for multiple parameter regimes. Overall, we
demonstrate a biologically-motivated multiphase model capable of generating
long-term pattern formation on a biologically plausible spatial scale both in
1D and 2D, with applications for modelling in vitro vascular network formation."
1,3,Contextual guidance: An integrated theory for astrocytes function in brain circuits and behavior,"The participation of astrocytes in brain computation was formally
hypothesized in 1992, coinciding with the discovery that these glial cells
display a complex form of Ca2+ excitability. This fostered conceptual advances
centered on the notion of reciprocal interactions between neurons and
astrocytes, which permitted a critical leap forward in uncovering many roles of
astrocytes in brain circuits, and signaled the rise of a major new force in
neuroscience: that of glial biology. In the past decade, a multitude of
unconventional and disparate functions of astrocytes have been documented that
are not predicted by these canonical models and that are challenging to piece
together into a holistic and parsimonious picture. This highlights a disconnect
between the rapidly evolving field of astrocyte biology and the conceptual
frameworks guiding it, and emphasizes the need for a careful reconsideration of
how we theorize the functional position of astrocytes in brain circuitry. Here,
we propose a unifying, highly transferable, data-driven, and
computationally-relevant conceptual framework for astrocyte biology, which we
coin contextual guidance. It describes astrocytes as contextual gates that
decode multiple environmental factors to shape neural circuitry in an adaptive,
state-dependent fashion. This paradigm is organically inclusive of all
fundamental features of astrocytes, many of which have remained unaccounted for
in previous theories. We find that this new concept provides an intuitive and
powerful theoretical space to improve our understanding of brain function and
computational models thereof across scales because it depicts astrocytes as a
hub for circumstantial inputs into relevant specialized circuits that permits
adaptive behaviors at the network and organism level."
1,3,Oncology and mechanics: landmark studies and promising clinical applications,"Clinical management of cancer has continuously evolved for several decades.
Biochemical, molecular and genomics approaches have brought and still bring
numerous insights into cancerous diseases. It is now accepted that some
phenomena, allowed by favorable biological conditions, emerge via mechanical
signaling at the cellular scale and via mechanical forces at the macroscale.
Mechanical phenomena in cancer have been studied in-depth over the last
decades, and their clinical applications are starting to be understood. If
numerous models and experimental setups have been proposed, only a few have led
to clinical applications. The objective of this contribution is to propose to
review a large scope of mechanical findings which have consequences on the
clinical management of cancer. This review is mainly addressed to doctoral
candidates in mechanics and applied mathematics who are faced with the
challenge of the mechanics-based modeling of cancer with the aim of clinical
applications. We show that the collaboration of the biological and mechanical
approaches has led to promising advances in terms of modeling, experimental
design and therapeutic targets. Additionally, a specific focus is brought on
imaging-informed mechanics-based models, which we believe can further the
development of new therapeutic targets and the advent of personalized medicine.
We study in detail several successful workflows on patient-specific targeted
therapies based on mechanistic modeling."
2,3,Adjusting the range of cell-cell communication enables fine-tuning of cell fate patterns from checkerboard to engulfing,"During development, spatio-temporal patterns ranging from checkerboard to
engulfing occur with precise proportions of the respective cell fates. Key
developmental regulators are intracellular transcriptional interactions and
intercellular signaling. We present an analytically tractable mathematical
model based on signaling that reliably generates different cell type patterns
with specified proportions. Employing statistical mechanics, We derived a cell
fate decision model for two cell types. A detailed steady state analysis on the
resulting dynamical system yielded necessary conditions to generate spatially
heterogeneous patterns. This allows the cell type proportions to be controlled
by a single model parameter. Cell-cell communication is realized by local and
global signaling mechanisms. These result in different cell type patterns. A
nearest neighbor signal yields checkerboard patterns. Increasing the signal
dispersion, cell fate clusters and an engulfing pattern can be generated.
Altogether, the presented model allows to reliably generate heterogeneous cell
type patterns of different kinds as well as desired proportions."
1,3,Current Landscape of Mesenchymal Stem Cell Therapy in COVID Induced Acute Respiratory Distress Syndrome,"The severe acute respiratory syndrome coronavirus 2 outbreak in Chinas Hubei
area in late 2019 has now created a global pandemic that has spread to over 150
countries. In most people, COVID 19 is a respiratory infection that produces
fever, cough, and shortness of breath. Patients with severe COVID 19 may
develop ARDS. MSCs can come from a number of places, such as bone marrow,
umbilical cord, and adipose tissue. Because of their easy accessibility and low
immunogenicity, MSCs were often used in animal and clinical research. In recent
studies, MSCs have been shown to decrease inflammation, enhance lung
permeability, improve microbial and alveolar fluid clearance, and accelerate
lung epithelial and endothelial repair. Furthermore, MSC-based therapy has
shown promising outcomes in preclinical studies and phase 1 clinical trials in
sepsis and ARDS. In this paper, we posit the therapeutic strategies using MSC
and dissect how and why MSC therapy is a potential treatment option for COVID
19 induced ARDS. We cite numerous promising clinical trials, elucidate the
potential advantages of MSC therapy for COVID 19 ARDS patients, examine the
detriments of this therapeutic strategy and suggest possibilities of subsequent
research."
2,3,Macrophage anti-inflammatory behaviour in a multiphase model of atherosclerotic plaque development,"Atherosclerosis is an inflammatory disease characterised by the formation of
plaques, which are deposits of lipids and cholesterol-laden macrophages that
form in the artery wall. The inflammation is often non-resolving, due in large
part to changes in normal macrophage anti-inflammatory behaviour that are
induced by the toxic plaque microenvironment. These changes include higher
death rates, defective efferocytic uptake of dead cells, and reduced rates of
emigration. We develop a free boundary multiphase model for early
atherosclerotic plaques, and we use it to investigate the effects of impaired
macrophage anti-inflammatory behaviour on plaque structure and growth. We find
that high rates of cell death relative to efferocytic uptake results in a
plaque populated mostly by dead cells. We also find that emigration can
potentially slow or halt plaque growth by allowing material to exit the plaque,
but this is contingent on the availability of live macrophage foam cells in the
deep plaque. Finally, we introduce an additional bead species to model
macrophage tagging via microspheres, and we use the extended model to explore
how high rates of cell death and low rates of efferocytosis and emigration
prevent the clearance of macrophages from the plaque."
1,3,Closing the Loop on Morphogenesis: A Mathematical Model of Morphogenesis by Closed-Loop Reaction-Diffusion,"Morphogenesis, the establishment and repair of emergent complex anatomy by
groups of cells, is a fascinating and biomedically-relevant problem. One of its
most fascinating aspects is that a developing embryo can reliably recover from
disturbances, such as splitting into twins. While this reliability implies some
type of goal-seeking error minimization over a morphogenic field, there are
many gaps with respect to detailed, constructive models of such a process being
used to implement the collective intelligence of cellular swarms. We describe a
closed-loop negative-feedback system for creating reaction-diffusion (RD)
patterns with high reliability. It uses a cellular automaton to characterize a
morphogen pattern, then compares it to a goal and adjusts accordingly,
providing a framework for modeling anatomical homeostasis and robust generation
of target morphologies. Specifically, we create a RD pattern with N
repetitions, where N is easily changeable. Furthermore, the individual
repetitions of the RD pattern can be easily stretched or shrunk under genetic
control to create, e.g., some morphological features larger than others.
Finally, the cellular automaton uses a computation wave that scans the
morphogen pattern unidirectionally to characterize the features that the
negative feedback then controls. By taking advantage of a prior process
asymmetrically establishing planar polarity (e.g., head vs. tail), our
automaton is greatly simplified. This work contributes to the exciting effort
of understanding design principles of morphological computation, which can be
used to understand evolved developmental mechanisms, manipulate them in
regenerative medicine settings, or embed a degree of synthetic intelligence
into novel bioengineered constructs."
1,3,Cell dynamics in microfluidic devices under heterogeneous chemotaxis and growth conditions: a mathematical study,"As motivated by studies of cellular motility driven by spatiotemporal
chemotactic gradients in microdevices, we develop a framework for constructing
approximate analytical solutions for the location, speed and cellular densities
for cell chemotaxis waves in heterogeneous fields of chemoattractant from the
underlying partial differential equation models. In particular, such
chemotactic waves are not in general translationally invariant travelling
waves, but possess a spatial variation that evolves in time, and may even may
oscillate back and forth in time, according to the details of the chemotactic
gradients. The analytical framework exploits the observation that unbiased
cellular diffusive flux is typically small compared to chemotactic fluxes and
is first developed and validated for a range of exemplar scenarios. The
framework is subsequently applied to more complex models considering the full
dynamics of the chemoattractant and how this may be driven and controlled
within a microdevice by considering a range of boundary conditions. In
particular, even though solutions cannot be constructed in all cases, a wide
variety of scenarios can be considered analytically, firstly providing global
insight into the important mechanisms and features of cell motility in complex
spatiotemporal fields of chemoattractant. Such analytical solutions also
provide a means of rapid evaluation of model predictions, with the prospect of
application in computationally demanding investigations relating theoretical
models and experimental observation, such as Bayesian parameter estimation."
2,3,Three-component contour dynamics model to simulate and analyze amoeboid cell motility,"Amoeboid cell motility is relevant in a wide variety of biomedical
applications such as wound healing, cancer metastasis, and embryonic
morphogenesis. It is characterized by pronounced changes of the cell shape
associated with expansions and retractions of the cell membrane, which result
in a crawling kind of locomotion. Despite existing computational models of
amoeboid motion, the inference of expansion and retraction components of
individual cells, the corresponding classification of cells, and the a priori
specification of the parameter regime to achieve a specific motility behavior
remain challenging open problems. We propose a novel model of the
spatio-temporal evolution of two-dimensional cell contours comprising three
biophysiologically motivated components: a stochastic term accounting for
membrane protrusions and two deterministic terms accounting for membrane
retractions by regularizing the shape and area of the contour. Mathematically,
these correspond to the intensity of a self-exciting Poisson point process, the
area-preserving curve-shortening flow, and an area adjustment flow. The model
is used to generate contour data for a variety of qualitatively different,
e.g., polarized and non-polarized, cell tracks that are hardly distinguishable
from experimental data. In application to experimental cell tracks, we inferred
the protrusion component and examined its correlation to commonly used
biomarkers: the actin concentration close to the membrane and its local motion.
Due to the low model complexity, parameter estimation is fast, straightforward
and offers a simple way to classify contour dynamics based on two locomotion
types: the amoeboid and a so-called fan-shaped type. For both types, we use
cell tracks segmented from fluorescence imaging data of the model organism D.
discoideum. An implementation of the model is provided within the open-source
software package AmoePy."
1,3,Swarmalators with delayed interactions,"We investigate the effects of delayed interactions in a population of
``swarmalators"", generalizations of phase oscillators that both synchronize in
time and swarm through space. We discover two steady collective states: a state
in which swarmalators are essentially motionless in a disk arranged in a
pseudo-crystalline order, and a boiling state in which the swarmalators again
form a disk, but now the swarmalators near the boundary perform boiling-like
convective motions. These states are reminiscent of the beating clusters seen
in photoactivated colloids and the living crystals of starfish embryos."
1,3,Spatial Heterogeneity Localizes Turing Patterns in Reaction-Cross-Diffusion Systems,"Motivated by bacterial chemotaxis and multi-species ecological interactions
in heterogeneous environments, we study a general one-dimensional
reaction-cross-diffusion system in the presence of spatial heterogeneity in
both transport and reaction terms. Under a suitable asymptotic assumption that
the transport is slow over the domain, while gradients in the reaction
heterogeneity are not too sharp, we study the stability of a heterogeneous
steady state approximated by the system in the absence of transport. Using a
WKB ansatz, we find that this steady state can undergo a Turing-type
instability in subsets of the domain, leading to the formation of localized
patterns. The boundaries of the pattern-forming regions are given
asymptotically by `local' Turing conditions corresponding to a spatially
homogeneous analysis parameterized by the spatial variable. We developed a
general open-source code which is freely available, and show numerical examples
of this localized pattern formation in a Schnakenberg cross-diffusion system, a
Keller-Segel chemotaxis model, and the Shigesada-Kawasaki-Teramoto model with
heterogeneous parameters. We numerically show that the patterns may undergo
secondary instabilities leading to spatiotemporal movement of spikes, though
these remain approximately within the asymptotically predicted localized
regions. This theory can elegantly differentiate between spatial structure due
to background heterogeneity, from spatial patterns emergent from Turing-type
instabilities."
0,3,Xanthene Dyes for Cancer Imaging and Treatment: A Material Odyssey,"Cancer is still among the leading health issues today, considering the cost,
effectiveness, complexity of detection/treatment modalities and survival rates.
One of the most important criteria for higher survival rates is the early and
sensitive diagnosis of the disease that can direct the treatment modalities
effectively. Fluorescence imaging agents emerged as an important alternative to
the current state of the art due to their spatial and temporal resolution, high
sensitivity and selectivity, ease of modification towards generating
activatable agents, ease of operation, and low cost. In addition to imaging,
light-based treatment modality, photodynamic therapy (PDT), attained remarkable
attention, as it is minimally invasive and has fewer side effects compared to
the current standard of care treatments. Even though fluorescence imaging and
PDT have these significant advantages, light that needs to excite the agent has
limited penetration in tissues, hindering widespread utilization. Hybrid
xanthene dyes, particularly ones bearing silicon or phosphine oxide as the
bridging unit of xanthene moiety, gained significant interest not only due to
their excellent photochemical properties in aqueous media, high fluorescence
quantum yield, photostability but also their proper absorption and emission
maxima that allow for deep tissue imaging and therapy. Here, the design and
synthesis strategies, key photophysical properties, their application in
fluorescence imaging applications, and surprisingly limited utilization as PDT
agents of hybrid xanthene dyes that emerged in the last decade have been
reviewed in detail."
2,3,Immune cells use active tugging forces to distinguish affinity and accelerate evolution,"Cells are known to exert forces to sense their physical surroundings for
guidance of motion and fate decisions. Here, we propose that cells might do
mechanical work to drive their own evolution, taking inspiration from the
adaptive immune system. Growing evidence indicates that immune B cells -
capable of rapid Darwinian evolution - use cytoskeletal forces to actively
extract antigen from other cells' surface. To elucidate the evolutionary
significance of force usage, we develop a theory of tug-of-war antigen
extraction that maps receptor binding characteristics to clonal reproductive
fitness, revealing physical determinants of selection strength. This framework
unifies mechanosensing and affinity-discrimination capabilities of evolving
cells: pulling against stiff antigen tethers enhances discrimination stringency
at the expense of absolute extraction. As a consequence, active force usage can
accelerate adaptation but may also cause extinction of cell populations,
resulting in an optimal range of pulling strength that matches molecular
rupture forces observed in cells. Our work suggests that nonequilibrium,
physical extraction of environmental signals can make biological systems more
evolvable at a moderate energy cost."
2,3,Differential elasticity in lineage segregation of embryonic stem cells,"The question of what guides lineage segregation is central to development,
where cellular differentiation leads to segregated cell populations destined
for specialized functions. Here, using optical tweezers measurements of mouse
embryonic stem cells (mESCs), we reveal a mechanical mechanism based on
differential elasticity in the second lineage segregation of the embryonic
inner cell mass into epiblast (EPI) cells - that will develop into the fetus -
and primitive endoderm (PrE) - which will form extraembryonic structures such
as the yolk sac. Remarkably, we find that these mechanical differences already
occur during priming and not just after a cell has committed to
differentiation. Specifically, we show that the mESCs are highly elastic
compared to any other reported cell type and that the PrE cells are
significantly more elastic than EPI-primed cells. Using a model of two cell
types differing only in elasticity we show that differential elasticity alone
can lead to segregation between cell types, suggesting that the mechanical
attributes of the cells contribute to the segregation process. Our findings
present differential elasticity as a previously unknown mechanical contributor
to the lineage segregation during the embryo morphogenesis."
2,3,Dynamic fibronectin assembly and remodeling by leader neural crest cells prevents jamming in collective cell migration,"Collective cell migration plays an essential role in vertebrate development,
yet the extent to which dynamically changing microenvironments influence this
phenomenon remains unclear. Observations of the distribution of the
extracellular matrix (ECM) component fibronectin during the migration of
loosely connected neural crest cells (NCCs) lead us to hypothesize that NCC
remodeling of an initially punctate ECM creates a scaffold for trailing cells,
enabling them to form robust and coherent stream patterns. We evaluate this
idea in a theoretical setting by developing an individual-based computational
model that incorporates reciprocal interactions between NCCs and their ECM. ECM
remodeling, haptotaxis, contact guidance, and cell-cell repulsion are
sufficient for cells to establish streams in silico, however additional
mechanisms, such as chemotaxis, are required to consistently guide cells along
the correct target corridor. Further model investigations imply that contact
guidance and differential cell-cell repulsion between leader and follower cells
are key contributors to robust collective cell migration by preventing stream
breakage. Global sensitivity analysis and simulated gain- and loss-of-function
experiments suggest that long-distance migration without jamming is most likely
to occur when leading cells specialize in creating ECM fibers, and trailing
cells specialize in responding to environmental cues by upregulating mechanisms
such as contact guidance."
2,3,Physical limits on galvanotaxis,"Eukaryotic cells of many types can polarize and migrate in response to
electric fields via ""galvanotaxis""; this ability helps skin cells heal wounds.
Recent experimental evidence suggests galvanotaxis occurs because membrane
proteins redistribute via electrophoresis, though the sensing species has not
yet been conclusively identified. We use a physical model to show that
stochasticity due to the finite number of sensing proteins limits the accuracy
of galvanotaxis via electrophoresis. Using maximum likelihood estimation, we
show how cells can best interpret this noisy signal, and how their accuracy
should depend on the cell size and electric field strength. Our model can be
fit well to data measuring galvanotaxis of keratocytes, neural crest cells, and
granulocytes. Our results show that eukaryotic cells can likely achieve
experimentally observed directionalities with either a relatively small number
(around 100) of highly-polarized proteins, or a large number (~10,000) of
proteins with a relatively small change in concentration across the cell (~7%
change from cathode to anode). This may explain why identifying the sensor
species has been difficult, as candidates need not be strongly polarized even
in large electric fields. A second prediction of the model is that the accuracy
of cells in predicting the electric field direction only weakly depends on
their size."
1,3,Concentration-Dependent Domain Evolution in Reaction-Diffusion Systems,"Pattern formation has been extensively studied in the context of evolving
(time-dependent) domains in recent years, with domain growth implicated in
ameliorating problems of pattern robustness and selection, in addition to more
realistic modelling in developmental biology. Most work to date has considered
prescribed domains evolving as given functions of time, but not the scenario of
concentration-dependent dynamics, which is also highly relevant in a
developmental setting. Here, we study such concentration-dependent domain
evolution for reaction-diffusion systems to elucidate fundamental aspects of
these more complex models. We pose a general form of one-dimensional domain
evolution, and extend this to $N$-dimensional manifolds under mild constitutive
assumptions in lieu of developing a full tissue-mechanical model. In the 1D
case, we are able to extend linear stability analysis around homogeneous
equilibria, though this is of limited utility in understanding complex pattern
dynamics in fast growth regimes. We numerically demonstrate a variety of
dynamical behaviours in 1D and 2D planar geometries, giving rise to several new
phenomena, especially near regimes of critical bifurcation boundaries such as
peak-splitting instabilities. For sufficiently fast growth and contraction,
concentration-dependence can have an enormous impact on the nonlinear dynamics
of the system both qualitatively and quantitatively. We highlight crucial
differences between 1D evolution and higher dimensional models, explaining
obstructions for linear analysis and underscoring the importance of careful
constitutive choices in defining domain evolution in higher dimensions. We
raise important questions in the modelling and analysis of biological systems,
in addition to numerous mathematical questions that appear tractable in the
one-dimensional setting, but are vastly more difficult for higher-dimensional
models."
2,3,In Situ 3D Spatiotemporal Measurement of Soluble Biomarkers in Organoid Culture,"Advanced cell culture techniques such as 3D bio-printing and hydrogel-based
cell embedding techniques harbor many new and exciting opportunities to study
cells in environments that closely recapitulate in-vivo conditions. Researchers
often study these environments using fluorescence microscopy to visualize the
protein association with objects such as cells within the 3D environment, yet
quantification of concentration profiles in the microenvironment has remained
elusive. Here, we present a method to continuously measure the time-dependent
concentration gradient of various biomarkers within a 3D cell culture assay
using bead-based immunoassays to sequester and concentrate the fluorescence
intensity of these tagged proteins. This assay allows for near real-time in
situ biomarker detection and enables spatiotemporal quantification of biomarker
concentration. Snapshots of concentration profiles can be taken, or time series
analysis can be performed enabling time-varying biomarker production
estimation. Example assays utilize an osteosacroma tumoroid as a case study for
a quantitative single-plexed gel encapsulated assay, and a qualitative
multi-plexed 3D bioprinted assay. In both cases, a time-varying cytokine
concentration gradient is measured. An estimation for the production rate of
the IL-8 cytokine per second per osteosarcoma cell results from fitting an
analytical function for continuous point source diffusion to the measured
concentration gradient and reveals that each cell produces approximately two
IL-8 cytokines per second. Proper calibration and use of this assay is
exhaustively explored for the case of diffusion-limited Langmuir kinetics of a
spherical adsorber."
1,3,Role of heterogeneity in dictating tumorigenesis in epithelial tissues,"Biological systems across various length and time scales are noisy, including
tissues. Why are biological tissues inherently chaotic? Does heterogeneity play
a role in determining the physiology and pathology of tissues? How do physical
and biochemical heterogeneity crosstalk to dictate tissue function? In this
review, we begin with a brief primer on heterogeneity in biological tissues.
Then, we take examples from recent literature indicating functional relevance
of biochemical and physical heterogeneity and discuss the impact of
heterogeneity on tissue function and pathology. We take specific examples from
studies on epithelial tissues to discuss the potential role of inherent tissue
heterogeneity in tumorigenesis."
2,3,Analysis of Cell Packing Behavior to Enhance Wound Assessment,"Wound assessment is a critical aspect of wound treatment, as the healing
progress of a wound determines the optimal approach to care. However, the
heterogeneity of burn wounds often complicates wound assessment, causing
inaccurate wound evaluation and ineffective treatment. Traditional wound
assessment methods such as Gross Area Reduction (GAR) and Percentage Area
Reduction (PAR) are prone to misinterpretation, due to irregular results.
Inaccurate wound assessment leads to higher rates of death and life-long
physical and psychological morbidities in burn patients, especially in
low-income communities that lack specialty care and medical resources.
Therefore, I propose a novel approach to wound assessment: wound healing from
the biophysical perspective of collective cell migration by analyzing cell
packing behavior. This approach was modeled through Voronoi Tessellation
simulations and applied to a wound healing system, where changes in the cell
morphology parameters of aspect ratio and shape index were plotted over time to
numerically evaluate the geometry of different cell migration packing patterns.
Experimental results demonstrate the effectiveness of measuring aspect ratio,
as a reduction in aspect ratio indicates that cell shapes become increasingly
rounded throughout wound closure. This is further proven when considering
physical principles in wound healing and changes in cell elongation. By placing
a microscope objective on a phone camera, it is possible to directly examine
any wound, with the calculations done on the phone as well. This efficient and
accurate mechanism can be especially useful in low-resource communities, as it
is accessible regardless of technical or medical background."
2,3,The analytical solution to the migration of an epithelial monolayer with a circular spreading front and its implications in the gap closure process,"The coordinated behaviors of epithelial cells are widely observed in tissue
development, such as re-epithelialization, tumor growth, and morphogenesis. In
these processes, cells either migrate collectively or organize themselves into
specific structures to serve certain purposes. In this work, we study a
spreading epithelial monolayer whose migrating front encloses a circular gap in
the monolayer center. Such tissue is usually used to mimic the wound healing
process in Virto. We model the epithelial sheet as a layer of active viscous
polar fluid. With an axisymmetric assumption, the model can be analytically
solved under two special conditions, suggesting two possible spreading modes
for the epithelial monolayer. Based on these two sets of analytical solutions,
we assess the velocity of the spreading front affected by the gap size, the
active intercellular contractility, and the purse-string contraction acting on
the spreading edge. Several critical values exist in the model parameters for
the initiation of the gap closure process, and the purse-string contraction
plays a vital role in governing the gap closure kinetics. Finally, the
instability of the morphology of the spreading front was studied. Numerical
calculations show how the perturbated velocities and the growth rates vary with
respect to different model parameters."
2,3,Active wetting of epithelial tissues: modeling considerations,"Morphogenesis, tissue regeneration and cancer invasion involve transitions in
tissue morphology. These transitions, caused by collective cell migration
(CCM), have been interpreted as active wetting/de-wetting transitions. This
phenomenon is considered on model system such as wetting of cell aggregate on
rigid substrate which includes cell aggregate movement and
isotropic/anisotropic spreading of cell monolayer around the aggregate
depending on the substrate rigidity and aggregate size. This model system
accounts for the transition between 3D epithelial aggregate and 2D cell
monolayer as a product of: (1) tissue surface tension, (2) surface tension of
substrate matrix, (3) cell-matrix interfacial tension, (4) interfacial tension
gradient, (5) viscoelasticity caused by CCM, and (6) viscoelasticity of
substrate matrix. These physical parameters depend on the cell contractility
and state of cell-cell and cell matrix adhesion contacts, as well as, the
stretching/compression of cellular systems caused by CCM. Despite extensive
research devoted to study cell wetting, we still do not understand interplay
among these physical parameters which induces oscillatory trend of cell
rearrangement. This review focuses on these physical parameters in governing
the cell rearrangement in the context of epithelial aggregate
wetting.de-wetting, and on the modelling approaches aimed at reproducing and
understanding these biological systems. In this context, we do not only review
previously-published bio-physics models for cell rearrangement caused by CCM,
but also propose new extensions of those models in order to point out the
interplay between cell-matrix interfacial tension and epithelial
viscoelasticity and the role of the interfacial tension gradient in cell
spreading."
2,3,A mathematical model integrates diverging PXY and MP interactions in cambium development,"The cambium is a meristematic tissue in plant stems. Here, cell divisions
occur that are required for radial growth of plant stems. Daughters of cell
divisions within the cambium differentiate into woody xylem cells towards the
inside of the stem, or phloem towards the outside. As such, a pattern of
xylem-cambium-phloem is present along the radial axis of the stem. A
ligand-receptor pair, TDIF-PXY promotes cell division in the cambium, as do the
phytohormones, cytokinin and auxin. An auxin response factor, MP, has been
proposed to initiate cambial cell divisions by promoting PXY expression,
however, MP has also been reported to repress cambial cell divisions later in
development where TDIF-PXY complexes are also reported to suppress MP activity.
Here, we used a mathematical modelling approach to investigate how MP cell
division-promoting activity and cell division-repressing activity might be
integrated into the same network as a negative feedback loop. In our model,
this feedback loop improved the ability of the cambium to pattern correctly and
was found to be required for normal patterning when MP was stable. The
implications of this model in early and late cambium development are discussed."
2,3,Physical Confinement and Cell Proximity Increase Cell Migration Rates and Invasiveness: A Mathematical Model of Cancer Cell Invasion through Flexible Channels,"Cancer cell migration between different body parts is the driving force
behind cancer metastasis, which is the main cause of mortality of patients.
Migration of cancer cells often proceeds by penetration through narrow cavities
in locally stiff, yet flexible tissues. In our previous work, we developed a
model for cell geometry evolution during invasion, which we extend here to
investigate whether leader and follower (cancer) cells that only interact
mechanically can benefit from sequential transmigration through narrow
micro-channels and cavities.
  We consider two cases of cells sequentially migrating through a flexible
channel: leader and follower cells being closely adjacent or distant. Using
Wilcoxon's signed-rank test on the data collected from Monte Carlo simulations,
we conclude that the modelled transmigration speed for the follower cell is
significantly larger than for the leader cell when cells are distant, i.e.
follower cells transmigrate after the leader has completed the crossing.
Furthermore, it appears that there exists an optimum with respect to the width
of the channel such that cell moves fastest. On the other hand, in the case of
closely adjacent cells, effectively performing collective migration, the leader
cell moves $12\%$ faster since the follower cell pushes it. This work shows
that mechanical interactions between cells can increase the net transmigration
speed of cancer cells, resulting in increased invasiveness. In other words,
interaction between cancer cells can accelerate metastatic invasion."
2,3,Localized growth drives spongy mesophyll morphogenesis,"The spongy mesophyll is a complex, porous tissue found in plant leaves that
enables carbon capture and provides mechanical stability. Unlike many other
biological tissues, which remain confluent throughout development, the spongy
mesophyll must develop from an initially confluent tissue into a tortuous
network of cells with a large proportion of intercellular airspace. How the
airspace in the spongy mesophyll develops while the cells remain mechanically
stable remains unknown. Here, we used computer simulations of deformable
particles to develop a purely mechanical model for the development of the
spongy mesophyll tissue. By stipulating that (1) cell perimeter grows only near
voids, (2) cells both form and break adhesive bonds, and (3) the tissue
pressure remains constant, the computational model was able to recapitulate the
developmental trajectory of the microstructure of the spongy mesophyll observed
in Arabidopsis thaliana leaves. Robust generation of pore space in the spongy
mesophyll requires a balance of cell growth, adhesion, stiffness and tissue
pressure to ensure cell networks remain both porous yet mechanically robust.
The success of this mechanical model of tissue growth and porosity evolution
suggests that simple physical principles can coordinate and drive the
development of complex plant tissues like the spongy mesophyll."
1,3,Effect of switching time scale of receptor activity on chemotactic performance of Escherichia coli,"In the chemotactic motion of Escherichia coli, the switching of transmembrane
chemoreceptors between active and inactive states is one of the most important
steps of the signaling pathway. We study the effect of this switching
time-scale on the chemotactic performance of the cell. We quantify performance
by the chemotactic drift velocity of the cell. Our extensive numerical
simulations on a detailed theoretical model show that as the activity switching
rate increases, the drift velocity increases and then saturates. Our data also
show the mean duration of a downhill run decreases strongly with the switching
rate, while that of an uphill run decreases relatively slowly. We explain this
effect from temporal variation of activity along uphill and downhill
trajectories. We show that for large and small switching rates the nature of
activity variation show qualitatively different behaviors along a downhill run
but similar behavior along an uphill run. This results in a stronger dependence
of downhill run duration on the switching rate and relatively milder dependence
for uphill run duration."
1,3,Exact lattice-based stochastic cell culture simulation algorithms incorporating spontaneous and contact-dependent reactions,"In this paper, we address the modeling issues of cell movement and division
with a special focus on the phenomenon of volume exclusion in a lattice-based,
exact stochastic simulation framework. We propose a new exact method, called
Reduced Rate Method -- RRM, that is substantially quicker than the previously
used exclusion method, for large number of cells. In addition, we introduce
three novel reaction types: the contact-inhibited, the contact-promoted, and
the spontaneous reactions. To the best of our knowledge, these reaction types
have not been taken into account in lattice-based stochastic simulations of
cell cultures. These new types of events may be easily applied to complicated
systems, enabling the generation of biologically feasible stochastic cell
culture simulations. Furthermore, we show that the exclusion algorithm and our
RRM algorithm are mathematically equivalent in the sense that the next reaction
to be realized and the corresponding sojourn time both belong to the same
reaction and time distributions in the two approaches -- even with the newly
introduced reaction types.
  Exact, agent-based, stochastic methods of cell culture simulations seem to be
undervalued and are mostly used as benchmarking tools to validate deterministic
approximations of the corresponding stochastic models. Our proposed methods are
exact, they are easy to implement, have a high predictive value, and can be
conveniently extended with new features. Therefore, these approaches promise a
great potential."
1,3,Swimming in Complex Fluids,"We review the literature on swimming in complex fluids. A classification is
proposed by comparing the length and time scales of a swimmer with those of
nearby obstacles, interpreted broadly, extending from rigid or soft confining
boundaries to molecules which confer the bulk fluid with complex stresses. A
third dimension in the classification is the concentration of swimmers, which
incorporates fluids whose complexity arises purely by the collective motion of
swimming organisms. For each of the eight system classes which we identify we
provide a background and describe modern research findings. While some classes
have seen a great deal of attention for decades, others remain uncharted waters
still open and awaiting exploration."
2,3,Stochastic failure of cell infection post viral entry: Implications for infection outcomes and antiviral therapy,"A virus infection can be initiated with very few or even a single infectious
virion, and as such can become extinct, i.e. stochastically fail to take hold
or spread significantly. There are many ways that a fully competent infectious
virion, having successfully entered a cell, can fail to cause a productive
infection, i.e. one that yields infectious virus progeny. Though many discrete,
stochastic mathematical models (DSMs) have been developed and used to estimate
a virus infection's extinction probability, these typically neglect infection
failure post viral entry. The DSM presented herein introduces parameter
$\gamma\in(0,1]$ which corresponds to the probability that a virion's entry
into a cell will result in a productive cell infection. We derive an expression
for the likelihood of infection extinction in this new DSM, and find that
prophylactic therapy with an antiviral acting to reduce $\gamma$ is best at
increasing an infection's extinction probability, compared to antivirals acting
on the rates of virus production or virus entry into cells. Using the DSM, we
investigate the difference in the fraction of cells consumed by so-called
extinct versus established virus infections, and find that this distinction
becomes biologically meaningless as the probability of extinction approaches
100%. We show that infections wherein virus is release by an infected cell as a
single burst, rather than at a constant rate over the cell's infectious
lifespan, has the same probability of infection extinction, despite previous
claims to this effect [Pearson 2011, doi:10.1371/journal.pcbi.1001058].
Instead, extending previous work by others [Yan 2016,
doi:10.1007/s00285-015-0961-5], we show how the assumed distribution for the
stochastic virus burst size, affects the extinction probability and associated
critical antiviral efficacy."
3,3,Non-trivial dynamics in a model of glial membrane voltage driven by open potassium pores,"Despite the molecular evidence that close to linear steady state I-V
relationship in mammalian astrocytes reflects a total current resulting from
more than one differently regulated K+ conductances, detailed ODE models of
membrane voltage Vm incorporating multiple conductances are lacking. Repeated
results of deregulated expressions of major K+ channels in glia, Kir4.1, in
models of disease, as well as their altered rectification when assembling
heteromeric Kir4.1/Kir5.1 channels have motivated us to attempt a detailed
model adding the weaker potassium K2P current, in addition to Kir4.1, and study
the stability of the resting state Vr. We ask whether with a deregulated Kir
conductivity the nominal resting state Vr remains stable, and the cell retains
a potassium electrode behavior with Vm following E_K. The minimal 2-dimensional
model near Vr showed that certain alterations of Kir4.1 current may result in
multistability of Vm if the model incorporates the typically observed K+
currents: Kir, K2P, and non-specific potassium leak. More specifically, a
decrease or loss of outward Kir4.1 conductance introduces instability of Vr,
near E_K. That happens through a fold bifurcation giving birth to a much more
depolarized second, stable resting state Vdr>-10 mV. Realistic timeseries were
used to perturb the membrane model, from recordings at the glial membrane
during electrographic seizures. Simulations of the perturbed system by constant
current through GJCs and transient seizure-like discharges as local field
potentials led to depolarization of the astrocyte and switching of Vm between
the two stable states, in a down-state / up-state manner. If the prolonged
depolarizations near Vdr prove experimentally plausible, such catastrophic
instability would impact all aspects of the glial function, from metabolic
support to membrane transport and practically all neuromodulatory roles
assigned to glia."
2,3,Adaptive phototaxis of Chlamydomonas and the evolutionary transition to multicellularity in Volvocine green algae,"A fundamental issue in biology is the nature of evolutionary transitions from
unicellular to multicellular organisms. Volvocine algae are models for this
transition, as they span from the unicellular biflagellate Chlamydomonas to
multicellular species of Volvox with up to 50,000 Chlamydomonas-like cells on
the surface of a spherical extracellular matrix. The mechanism of phototaxis in
these species is of particular interest since they lack a nervous system and
intercellular connections; steering is a consequence of the response of
individual cells to light. Studies of Volvox and Gonium, a 16-cell organism
with a plate-like structure, have shown that the flagellar response to changing
illumination of the cellular photosensor is adaptive, with a recovery time
tuned to the rotation period of the colony around its primary axis. Here,
combining high-resolution studies of the flagellar photoresponse with 3D
tracking of freely-swimming cells, we show that such tuning also underlies
phototaxis of Chlamydomonas. A mathematical model is developed based on the
rotations around an axis perpendicular to the flagellar beat plane that occur
through the adaptive response to oscillating light levels as the organism
spins. Exploiting a separation of time scales between the flagellar
photoresponse and phototurning, we develop an equation of motion that
accurately describes the observed photoalignment. In showing that the adaptive
time scale is tuned to the organisms' rotational period across three orders of
magnitude in cell number, our results suggest a unified picture of phototaxis
in green algae in which the asymmetry in torques that produce phototurns arise
from the individual flagella of Chlamydomonas, the flagellated edges of Gonium
and the flagellated hemispheres of Volvox."
2,3,Quantifying assays: A Modeling tale of variability in cancer therapeutics assessed on cancer cells,"Inhibiting a signalling pathway concerns controlling the cellular processes
of a cancer cell's viability, cell division, and death. Assay protocols created
to see if the molecular structures of the drugs being tested have the desired
inhibition qualities often show great variability across experiments, and it is
imperative to diminish the effects of such variability while inferences are
drawn. In this paper we propose the study of experimental data through the
lenses of a mathematical model depicting the inhibition mechanism and the
activation-inhibition dynamics. The method is exemplified through assay data
obtained from the study of inhibition of the CXCL12/CXCR4 activation axis for
the melanoma cells. To mitigate the effects of the variability of the data on
the cell viability measurement, the cell viability is theoretically constructed
as a function of time depending on several parameters. The values of these
parameters are estimated by using the experimental data. Deriving approximation
for the cell viability in a theoretically pre-determined form has the
advantages of (i) being less sensitive to data variability (ii) the estimated
values of the parameters are interpreted directly in the biological processes,
(iii) the amount of variability explained via the approximation validates the
quality of the model, (iv) with the data integrated into the model one can
derive a more complete view over the whole process. These advantages are
demonstrated in the step-by-step implementation of the outlined approach."
2,3,Spontaneous transitions between amoeboid and keratocyte-like modes of migration,"The motility of adherent eukaryotic cells is driven by the dynamics of the
actin cytoskeleton. Despite the common force-generating actin machinery,
different cell types often show diverse modes of locomotion that differ in
their shape dynamics, speed, and persistence of motion. Recently, experiments
in Dictyostelium discoideum have revealed that different motility modes can be
induced in this model organism, depending on genetic modifications,
developmental conditions, and synthetic changes of intracellular signaling.
Here, we report experimental evidence that in a mutated D. discoideum cell line
with increased Ras activity, switches between two distinct migratory modes, the
amoeboid and fan-shaped type of locomotion, can even spontaneously occur within
the same cell. We observed and characterized repeated and reversible switchings
between the two modes of locomotion, suggesting that they are distinct
behavioral traits that coexist within the same cell. We adapted an established
phenomenological motility model that combines a reaction-diffusion system for
the intracellular dynamics with a dynamic phase field to account for our
experimental findings."
2,3,YOLO2U-Net: Detection-Guided 3D Instance Segmentation for Microscopy,"Microscopy imaging techniques are instrumental for characterization and
analysis of biological structures. As these techniques typically render 3D
visualization of cells by stacking 2D projections, issues such as out-of-plane
excitation and low resolution in the $z$-axis may pose challenges (even for
human experts) to detect individual cells in 3D volumes as these
non-overlapping cells may appear as overlapping. In this work, we introduce a
comprehensive method for accurate 3D instance segmentation of cells in the
brain tissue. The proposed method combines the 2D YOLO detection method with a
multi-view fusion algorithm to construct a 3D localization of the cells. Next,
the 3D bounding boxes along with the data volume are input to a 3D U-Net
network that is designed to segment the primary cell in each 3D bounding box,
and in turn, to carry out instance segmentation of cells in the entire volume.
The promising performance of the proposed method is shown in comparison with
some current deep learning-based 3D instance segmentation methods."
2,3,A non-local kinetic model for cell migration: a study of the interplay between contact guidance and steric hindrance,"We propose a non-local model for contact guidance and steric hindrance
depending on a single external cue, namely the extracellular matrix, that
affects in a twofold way the polarization and speed of motion of the cells. We
start from a microscopic description of the stochastic processes underlying the
cell re-orientation mechanism related to the change of cell speed and
direction. Then, we formally derive the corresponding kinetic model that
implements exactly the prescribed microscopic dynamics and, from it, it is
possible to deduce the macroscopic limit in the appropriate regime. Moreover,
we test our model in several scenarios. In particular, we numerically
investigate the minimal microscopic mechanisms that are necessary to reproduce
cell dynamics by comparing the outcomes of our model with some experimental
results related to breast cancer cell migration. This allows us to validate the
proposed modeling approach and, also, to highlight its capability of predicting
the qualitative cell behaviors in diverse heterogeneous microenvironments."
1,3,Biological Robots: Perspectives on an Emerging Interdisciplinary Field,"Advances in science and engineering often reveal the limitations of classical
approaches initially used to understand, predict, and control phenomena. With
progress, conceptual categories must often be re-evaluated to better track
recently discovered invariants across disciplines. It is essential to refine
frameworks and resolve conflicting boundaries between disciplines such that
they better facilitate, not restrict, experimental approaches and capabilities.
In this essay, we discuss issues at the intersection of developmental biology,
computer science, and robotics. In the context of biological robots, we explore
changes across concepts and previously distinct fields that are driven by
recent advances in materials, information, and life sciences. Herein, each
author provides their own perspective on the subject, framed by their own
disciplinary training. We argue that as with computation, certain aspects of
developmental biology and robotics are not tied to specific materials; rather,
the consilience of these fields can help to shed light on issues of multi-scale
control, self-assembly, and relationships between form and function. We hope
new fields can emerge as boundaries arising from technological limitations are
overcome, furthering practical applications from regenerative medicine to
useful synthetic living machines."
2,3,A local continuum model of cell-cell adhesion,"Cell-cell adhesion is one the most fundamental mechanisms regulating
collective cell migration during tissue development, homeostasis and repair,
allowing cell populations to self-organize and eventually form and maintain
complex tissue shapes. Cells interact with each other via the formation of
protrusions or filopodia and they adhere to other cells through binding of cell
surface proteins. The resulting adhesive forces are then related to cell size
and shape and, often, continuum models represent them by nonlocal attractive
interactions. In this paper, we present a new continuum model of cell-cell
adhesion which can be derived from a general nonlocal model in the limit of
short-range interactions. This new model is local, resembling a system of
thin-film type equations, with the various model parameters playing the role of
surface tensions between different cell populations. Numerical simulations in
one and two dimensions reveal that the local model maintains the diversity of
cell sorting patterns observed both in experiments and in previously used
nonlocal models. In addition, it also has the advantage of having explicit
stationary solutions, which provides a direct link between the model parameters
and the differential adhesion hypothesis."
1,3,The impact of phenotypic heterogeneity on chemotactic self-organisation,"The capacity to aggregate through chemosensitive movement forms a paradigm of
self-organisation, with examples spanning cellular and animal systems. A basic
mechanism assumes a phenotypically homogeneous population that secretes its own
attractant, with the well known system introduced more than five decades ago by
Keller and Segel proving resolutely popular in modelling studies. The typical
assumption of population phenotypic homogeneity, however, often lies at odds
with the heterogeneity of natural systems, where populations may comprise
distinct phenotypes that vary according to their chemotactic ability,
attractant secretion, {\it etc}. To initiate an understanding into how this
diversity can impact on autoaggregation, we propose a simple extension to the
classical Keller and Segel model, in which the population is divided into two
distinct phenotypes: those performing chemotaxis and those producing
attractant. Using a combination of linear stability analysis and numerical
simulations, we demonstrate that switching between these phenotypic states
alters the capacity of a population to self-aggregate. Further, we show that
switching based on the local environment (population density or chemoattractant
level) leads to diverse patterning and provides a route through which a
population can effectively curb the size and density of an aggregate. We
discuss the results in the context of real world examples of chemotactic
aggregation, as well as theoretical aspects of the model such as global
existence and blow-up of solutions."
2,3,Nucleation of cadherin clusters on cell-cell interfaces,"Cadherins mediate cell-cell adhesion and help the cell determine its shape
and function. Here we study collective cadherin organization and interactions
within cell-cell contact areas, and find the cadherin density at which a
gas-liquid phase transition occurs, when cadherin monomers begin to aggregate
into dense clusters. We use a 2D lattice model of a cell-cell contact area, and
coarse-grain to the continuous number density of cadherin to map the model onto
the Cahn-Hilliard coarsening theory. This predicts the density required for
nucleation, the characteristic length scale of the process, and the number
density of clusters. The analytical predictions of the model are in good
agreement with experimental observations of cadherin clustering in epithelial
tissues."
2,3,Interplay between substrate rigidity and tissue fluidity regulates cell monolayer spreading,"Coordinated and cooperative motion of cells is essential for embryonic
development, tissue morphogenesis, wound healing and cancer invasion. A
predictive understanding of the emergent mechanical behaviors in collective
cell motion is challenging due to the complex interplay between cell-cell
interactions, cell-matrix adhesions and active cell behaviors. To overcome this
challenge, we develop a predictive cellular vertex model that can delineate the
relative roles of substrate rigidity, tissue mechanics and active cell
properties on the movement of cell collectives. We apply the model to the
specific case of collective motion in cell aggregates as they spread into a
two-dimensional cell monolayer adherent to a soft elastic matrix. Consistent
with recent experiments, we find that substrate stiffness regulates the driving
forces for the spreading of cellular monolayer, which can be pressure-driven or
crawling-based depending on substrate rigidity. On soft substrates, cell
monolayer spreading is driven by an active pressure due to the influx of cells
coming from the aggregate, whereas on stiff substrates, cell spreading is
driven primarily by active crawling forces. Our model predicts that cooperation
of cell crawling and tissue pressure drives faster spreading, while the
spreading rate is sensitive to the mechanical properties of the tissue. We find
that solid tissues spread faster on stiff substrates, with spreading rate
increasing with tissue tension. By contrast, the spreading of fluid tissues is
independent of substrate stiffness and is slower than solid tissues. We compare
our theoretical results with experimental results on traction force generation
and spreading kinetics of cell monolayers, and provide new predictions on the
role of tissue fluidity and substrate rigidity on collective cell motion."
1,3,Innovations in Integrating Machine Learning and Agent-Based Modeling of Biomedical Systems,"Agent-based modeling (ABM) is a well-established paradigm for simulating
complex systems via interactions between constituent entities. Machine learning
(ML) refers to approaches whereby statistical algorithms 'learn' from data on
their own, without imposing a priori theories of system behavior. Biological
systems -- from molecules, to cells, to entire organisms -- consist of vast
numbers of entities, governed by complex webs of interactions that span many
spatiotemporal scales and exhibit nonlinearity, stochasticity and intricate
coupling between entities. The macroscopic properties and collective dynamics
of such systems are difficult to capture via continuum modelling and mean-field
formalisms. ABM takes a 'bottom-up' approach that obviates these difficulties
by enabling one to easily propose and test a set of well-defined 'rules' to be
applied to the individual entities (agents) in a system. Evaluating a system
and propagating its state over discrete time-steps effectively simulates the
system, allowing observables to be computed and system properties to be
analyzed. Because the rules that govern an ABM can be difficult to abstract and
formulate from experimental data, there is an opportunity to use ML to help
infer optimal, system-specific ABM rules. Once such rule-sets are devised, ABM
calculations can generate a wealth of data, and ML can be applied there too --
e.g., to probe statistical measures that meaningfully describe a system's
stochastic properties. As an example of synergy in the other direction (from
ABM to ML), ABM simulations can generate realistic datasets for training ML
algorithms (e.g., for regularization, to mitigate overfitting). In these ways,
one can envision various synergistic ABM$\rightleftharpoons$ML loops. This
review summarizes how ABM and ML have been integrated in contexts that span
spatiotemporal scales, from cellular to population-level epidemiology."
1,3,A typical workflow to simulate cytoskeletal systems with Cytosim,"Many cytoskeletal systems are now sufficiently well known to permit their
precise quantitative modelling. Microtubule and actin filaments are well
characterized, and the associated proteins are often known, as well as their
abundance and the interactions between these elements. Thus, computer
simulations can be used to investigate the collective behavior of the system
precisely, in a way that is complementary to experiments. Cytosim is an Open
Source cytoskeleton simulation suite designed to handle large systems of
flexible filaments with associated proteins such as molecular motors. It also
offers the possibility to simulate passive crosslinkers, diffusible
crosslinkers, nucleators, cutters and discrete versions of the motors that only
step on unoccupied lattice sites on a filament. Other objects complement the
filaments by offering spherical or more complicated geometry that can be used
to represent chromosomes, nucleus or vesicles in the cell. Cytosim offers
simple command-line tools for running a simulation and displaying its results,
that are versatile and do not require programming skills. In this workflow,
step-by-step instructions are given to: i) install the necessary environment on
a new computer, ii) configure Cytosim to simulate the contraction of a 2D
actomyosin network, iii) produce a visual representation of the system. Next,
the system is probed by systematically varying a key parameter: the number of
crosslinkers. Finally, the visual representation of the system is complemented
by a numerical quantification of contractility to view, in a graph, how
contractility depends on the composition of the system. Overall, these
different steps constitute a typical workflow that can be applied with few
modifications, to tackle many other problems in the cytoskeletal field."
2,3,Cellular gradient flow structure connects single-cell-level rules and population-level dynamics,"In multicellular systems, the single-cell behaviors should be coordinated
consistently with the overall population dynamics and functions. However, the
interrelation between single-cell rules and the population-level goal is still
elusive. In this work, we reveal that these two levels are naturally connected
via a gradient flow structure of the heterogeneous cellular population and that
biologically prevalent single-cell rules such as unidirectional type-switching
and hierarchical order in types emerge from this structure. We also demonstrate
the gradient flow structure in a standard model of the T-cell immune response.
This theoretical framework works as a basis for understanding multicellular
dynamics and functions."
2,3,Structured foraging of soil predators unveils functional responses to bacterial defenses,"Predators and their foraging strategies often determine ecosystem structure
and function. Yet, the role of protozoan predators in microbial soil ecosystems
remains elusive despite the importance of these ecosystems to global
biogeochemical cycles. In particular, amoebae -- the most abundant soil
protozoan predators of bacteria -- remineralize soil nutrients and shape the
bacterial community. However, their foraging strategies and their role as
microbial ecosystem engineers remain unknown. Here we present a multi-scale
approach, connecting microscopic single-cell analysis and macroscopic whole
ecosystem dynamics, to expose a phylogenetically widespread foraging strategy,
in which an amoeba population spontaneously partitions between cells with fast,
polarized movement and cells with slow, unpolarized movement. Such
differentiated motion gives rise to efficient colony expansion and consumption
of the bacterial substrate. From these insights we construct a theoretical
model that predicts how disturbances to amoeba growth rate and movement disrupt
their predation efficiency. These disturbances correspond to distinct classes
of bacterial defenses, which allows us to experimentally validate our
predictions. All considered, our characterization of amoeba foraging identifies
amoeba mobility, and not amoeba growth, as the core determinant of predation
efficiency and a key target for bacterial defense systems."
1,4,Q-malizing flow and infinitesimal density ratio estimation,"Continuous normalizing flows are widely used in generative tasks, where a
flow network transports from a data distribution $P$ to a normal distribution.
A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$
and $Q$ are accessible via finite samples, would be of various application
interests, particularly in the recently developed telescoping density ratio
estimation (DRE) which calls for the construction of intermediate densities to
bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow''
by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$
(and vice versa) from empirical samples and is regularized by minimizing the
transport cost. The trained flow model allows us to perform infinitesimal DRE
along the time-parametrized $\log$-density by training an additional
continuous-time flow network using classification loss, which estimates the
time-partial derivative of the $\log$-density. Integrating the time-score
network along time provides a telescopic DRE between $P$ and $Q$ that is more
stable than a one-step DRE. The effectiveness of the proposed model is
empirically demonstrated on mutual information estimation from high-dimensional
data and energy-based generative models of image data."
0,4,Multimodal Web Navigation with Instruction-Finetuned Foundation Models,"The progress of autonomous web navigation has been hindered by the dependence
on billions of exploratory interactions via online reinforcement learning, and
domain-specific model designs that make it difficult to leverage generalization
from rich out-of-domain data. In this work, we study data-driven offline
training for web agents with vision-language foundation models. We propose an
instruction-following multimodal agent, WebGUM, that observes both webpage
screenshots and HTML pages and outputs web navigation actions, such as click
and type. WebGUM is trained by jointly finetuning an instruction-finetuned
language model and a vision transformer on a large corpus of demonstrations. We
empirically demonstrate this recipe improves the agent's ability of grounded
visual perception, HTML comprehension and multi-step reasoning, outperforming
prior works by a significant margin. On the MiniWoB benchmark, we improve over
the previous best offline methods by more than 31.9%, being close to reaching
online-finetuned SoTA. On the WebShop benchmark, our 3-billion-parameter model
achieves superior performance to the existing SoTA, PaLM-540B. We also collect
347K high-quality demonstrations using our trained models, 38 times larger than
prior work, and make them available to promote future research in this
direction."
1,4,Improving Multimodal Joint Variational Autoencoders through Normalizing Flows and Correlation Analysis,"We propose a new multimodal variational autoencoder that enables to generate
from the joint distribution and conditionally to any number of complex
modalities. The unimodal posteriors are conditioned on the Deep Canonical
Correlation Analysis embeddings which preserve the shared information across
modalities leading to more coherent cross-modal generations. Furthermore, we
use Normalizing Flows to enrich the unimodal posteriors and achieve more
diverse data generation. Finally, we propose to use a Product of Experts for
inferring one modality from several others which makes the model scalable to
any number of modalities. We demonstrate that our method improves likelihood
estimates, diversity of the generations and in particular coherence metrics in
the conditional generations on several datasets."
1,4,The probability flow ODE is provably fast,"We provide the first polynomial-time convergence guarantees for the
probability flow ODE implementation (together with a corrector step) of
score-based generative modeling. Our analysis is carried out in the wake of
recent results obtaining such guarantees for the SDE-based implementation
(i.e., denoising diffusion probabilistic modeling or DDPM), but requires the
development of novel techniques for studying deterministic dynamics without
contractivity. Through the use of a specially chosen corrector step based on
the underdamped Langevin diffusion, we obtain better dimension dependence than
prior works on DDPM ($O(\sqrt{d})$ vs. $O(d)$, assuming smoothness of the data
distribution), highlighting potential advantages of the ODE framework."
1,4,Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability,"Recent research has observed that in machine learning optimization, gradient
descent (GD) often operates at the edge of stability (EoS) [Cohen, et al.,
2021], where the stepsizes are set to be large, resulting in non-monotonic
losses induced by the GD iterates. This paper studies the convergence and
implicit bias of constant-stepsize GD for logistic regression on linearly
separable data in the EoS regime. Despite the presence of local oscillations,
we prove that the logistic loss can be minimized by GD with any constant
stepsize over a long time scale. Furthermore, we prove that with any constant
stepsize, the GD iterates tend to infinity when projected to a max-margin
direction (the hard-margin SVM direction) and converge to a fixed vector that
minimizes a strongly convex potential when projected to the orthogonal
complement of the max-margin direction. In contrast, we also show that in the
EoS regime, GD iterates may diverge catastrophically under the exponential
loss, highlighting the superiority of the logistic loss. These theoretical
findings are in line with numerical simulations and complement existing
theories on the convergence and implicit bias of GD, which are only applicable
when the stepsizes are sufficiently small."
1,4,Multi-Objective Optimization Using the R2 Utility,"The goal of multi-objective optimization is to identify a collection of
points which describe the best possible trade-offs between the multiple
objectives. In order to solve this vector-valued optimization problem,
practitioners often appeal to the use of scalarization functions in order to
transform the multi-objective problem into a collection of single-objective
problems. This set of scalarized problems can then be solved using traditional
single-objective optimization techniques. In this work, we formalise this
convention into a general mathematical framework. We show how this strategy
effectively recasts the original multi-objective optimization problem into a
single-objective optimization problem defined over sets. An appropriate class
of objective functions for this new problem is the R2 utility function, which
is defined as a weighted integral over the scalarized optimization problems. We
show that this utility function is a monotone and submodular set function,
which can be optimised effectively using greedy optimization algorithms. We
analyse the performance of these greedy algorithms both theoretically and
empirically. Our analysis largely focusses on Bayesian optimization, which is a
popular probabilistic framework for black-box optimization."
1,4,Transfer operators on graphs: Spectral clustering and beyond,"Graphs and networks play an important role in modeling and analyzing complex
interconnected systems such as transportation networks, integrated circuits,
power grids, citation graphs, and biological and artificial neural networks.
Graph clustering algorithms can be used to detect groups of strongly connected
vertices and to derive coarse-grained models. We define transfer operators such
as the Koopman operator and the Perron-Frobenius operator on graphs, study
their spectral properties, introduce Galerkin projections of these operators,
and illustrate how reduced representations can be estimated from data. In
particular, we show that spectral clustering of undirected graphs can be
interpreted in terms of eigenfunctions of the Koopman operator and propose
novel clustering algorithms for directed graphs based on generalized transfer
operators. We demonstrate the efficacy of the resulting algorithms on several
benchmark problems and provide different interpretations of clusters."
1,4,Tester-Learners for Halfspaces: Universal Algorithms,"We give the first tester-learner for halfspaces that succeeds universally
over a wide class of structured distributions. Our universal tester-learner
runs in fully polynomial time and has the following guarantee: the learner
achieves error $O(\mathrm{opt}) + \epsilon$ on any labeled distribution that
the tester accepts, and moreover, the tester accepts whenever the marginal is
any distribution that satisfies a Poincar\'e inequality. In contrast to prior
work on testable learning, our tester is not tailored to any single target
distribution but rather succeeds for an entire target class of distributions.
The class of Poincar\'e distributions includes all strongly log-concave
distributions, and, assuming the Kannan--L\'{o}vasz--Simonovits (KLS)
conjecture, includes all log-concave distributions. In the special case where
the label noise is known to be Massart, our tester-learner achieves error
$\mathrm{opt} + \epsilon$ while accepting all log-concave distributions
unconditionally (without assuming KLS). Our tests rely on checking
hypercontractivity of the unknown distribution using a sum-of-squares (SOS)
program, and crucially make use of the fact that Poincar\'e distributions are
certifiably hypercontractive in the SOS framework."
1,4,Moment Matching Denoising Gibbs Sampling,"Energy-Based Models (EBMs) offer a versatile framework for modeling complex
data distributions. However, training and sampling from EBMs continue to pose
significant challenges. The widely-used Denoising Score Matching (DSM) method
for scalable EBM training suffers from inconsistency issues, causing the energy
model to learn a `noisy' data distribution. In this work, we propose an
efficient sampling framework: (pseudo)-Gibbs sampling with moment matching,
which enables effective sampling from the underlying clean model when given a
`noisy' model that has been well-trained via DSM. We explore the benefits of
our approach compared to related methods and demonstrate how to scale the
method to high-dimensional datasets."
1,4,Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern,"This paper studies the open problem of conformalized entry prediction in a
row/column-exchangeable matrix. The matrix setting presents novel and unique
challenges, but there exists little work on this interesting topic. We
meticulously define the problem, differentiate it from closely related
problems, and rigorously delineate the boundary between achievable and
impossible goals. We then propose two practical algorithms. The first method
provides a fast emulation of the full conformal prediction, while the second
method leverages the technique of algorithmic stability for acceleration. Both
methods are computationally efficient and can effectively safeguard coverage
validity in presence of arbitrary missing pattern. Further, we quantify the
impact of missingness on prediction accuracy and establish fundamental limit
results. Empirical evidence from synthetic and real-world data sets
corroborates the superior performance of our proposed methods."
1,4,Bayesian approach to Gaussian process regression with uncertain inputs,"Conventional Gaussian process regression exclusively assumes the existence of
noise in the output data of model observations. In many scientific and
engineering applications, however, the input locations of observational data
may also be compromised with uncertainties owing to modeling assumptions,
measurement errors, etc. In this work, we propose a Bayesian method that
integrates the variability of input data into Gaussian process regression.
Considering two types of observables -- noise-corrupted outputs with fixed
inputs and those with prior-distribution-defined uncertain inputs, a posterior
distribution is estimated via a Bayesian framework to infer the uncertain data
locations. Thereafter, such quantified uncertainties of inputs are incorporated
into Gaussian process predictions by means of marginalization. The
effectiveness of this new regression technique is demonstrated through several
numerical examples, in which a consistently good performance of generalization
is observed, while a substantial reduction in the predictive uncertainties is
achieved by the Bayesian inference of uncertain inputs."
1,4,The Deep Promotion Time Cure Model,"We propose a novel method for predicting time-to-event in the presence of
cure fractions based on flexible survivals models integrated into a deep neural
network framework. Our approach allows for non-linear relationships and
high-dimensional interactions between covariates and survival and is suitable
for large-scale applications. Furthermore, we allow the method to incorporate
an identified predictor formed of an additive decomposition of interpretable
linear and non-linear effects and add an orthogonalization layer to capture
potential higher dimensional interactions. We demonstrate the usefulness and
computational efficiency of our method via simulations and apply it to a large
portfolio of US mortgage loans. Here, we find not only a better predictive
performance of our framework but also a more realistic picture of covariate
effects."
1,4,TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series,"Temporally indexed data are essential in a wide range of fields and of
interest to machine learning researchers. Time series data, however, are often
scarce or highly sensitive, which precludes the sharing of data between
researchers and industrial organizations and the application of existing and
new data-intensive ML methods. A possible solution to this bottleneck is to
generate synthetic data. In this work, we introduce Time Series Generative
Modeling (TSGM), an open-source framework for the generative modeling of
synthetic time series. TSGM includes a broad repertoire of machine learning
methods: generative models, probabilistic, and simulator-based approaches. The
framework enables users to evaluate the quality of the produced data from
different angles: similarity, downstream effectiveness, predictive consistency,
diversity, and privacy. The framework is extensible, which allows researchers
to rapidly implement their own methods and compare them in a shareable
environment. TSGM was tested on open datasets and in production and proved to
be beneficial in both cases. Additionally to the library, the project allows
users to employ command line interfaces for synthetic data generation which
lowers the entry threshold for those without a programming background."
3,4,From Random Search to Bandit Learning in Metric Measure Spaces,"Random Search is one of the most widely-used method for Hyperparameter
Optimization, and is critical to the success of deep learning models. Despite
its astonishing performance, little non-heuristic theory has been developed to
describe the underlying working mechanism. This paper gives a theoretical
accounting of Random Search. We introduce the concept of \emph{scattering
dimension} that describes the landscape of the underlying function, and
quantifies the performance of random search. We show that, when the environment
is noise-free, the output of random search converges to the optimal value in
probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T}
\right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering
dimension of the underlying function. When the observed function values are
corrupted by bounded $iid$ noise, the output of random search converges to the
optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left(
\frac{1}{T} \right)^{ \frac{1}{d_s + 1} } \right) $. In addition, based on the
principles of random search, we introduce an algorithm, called BLiN-MOS, for
Lipschitz bandits in doubling metric spaces that are also emdowed with a Borel
measure, and show that BLiN-MOS achieves a regret rate of order $
\widetilde{\mathcal{O}} \left( T^{ \frac{d_z}{d_z + 1} } \right) $, where $d_z$
is the zooming dimension of the problem instance. Our results show that in
metric spaces with a Borel measure, the classic theory of Lipschitz bandits can
be improved. This result suggests an intrinsic axiomatic gap between metric
spaces and metric measure spaces from an algorithmic perspective, since the
upper bound in a metric measure space breaks the known information-theoretical
lower bounds for Lipschitz bandits in a metric space with no measure structure."
1,4,Accelerating Convergence in Global Non-Convex Optimization with Reversible Diffusion,"Langevin Dynamics has been extensively employed in global non-convex
optimization due to the concentration of its stationary distribution around the
global minimum of the potential function at low temperatures. In this paper, we
propose to utilize a more comprehensive class of stochastic processes, known as
reversible diffusion, and apply the Euler-Maruyama discretization for global
non-convex optimization. We design the diffusion coefficient to be larger when
distant from the optimum and smaller when near, thus enabling accelerated
convergence while regulating discretization error, a strategy inspired by
landscape modifications. Our proposed method can also be seen as a time change
of Langevin Dynamics, and we prove convergence with respect to KL divergence,
investigating the trade-off between convergence speed and discretization error.
The efficacy of our proposed method is demonstrated through numerical
experiments."
1,4,Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models,"Generalized Additive Models (GAMs) have recently experienced a resurgence in
popularity due to their interpretability, which arises from expressing the
target value as a sum of non-linear transformations of the features. Despite
the current enthusiasm for GAMs, their susceptibility to concurvity - i.e.,
(possibly non-linear) dependencies between the features - has hitherto been
largely overlooked. Here, we demonstrate how concurvity can severly impair the
interpretability of GAMs and propose a remedy: a conceptually simple, yet
effective regularizer which penalizes pairwise correlations of the non-linearly
transformed feature variables. This procedure is applicable to any
differentiable additive model, such as Neural Additive Models or NeuralProphet,
and enhances interpretability by eliminating ambiguities due to self-canceling
feature contributions. We validate the effectiveness of our regularizer in
experiments on synthetic as well as real-world datasets for time-series and
tabular data. Our experiments show that concurvity in GAMs can be reduced
without significantly compromising prediction quality, improving
interpretability and reducing variance in the feature importances."
3,4,Generative Sliced MMD Flows with Riesz Kernels,"Maximum mean discrepancy (MMD) flows suffer from high computational costs in
large scale computations. In this paper, we show that MMD flows with Riesz
kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which
allow for their efficient computation. First, the MMD of Riesz kernels
coincides with the MMD of their sliced version. As a consequence, the
computation of gradients of MMDs can be performed in the one-dimensional
setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce
the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two empirical
measures with $M$ and $N$ support points. For the implementations we
approximate the gradient of the sliced MMD by using only a finite number $P$ of
slices. We show that the resulting error has complexity $O(\sqrt{d/P})$, where
$d$ is the data dimension. These results enable us to train generative models
by approximating MMD gradient flows by neural networks even for large scale
applications. We demonstrate the efficiency of our model by image generation on
MNIST, FashionMNIST and CIFAR10."
3,4,Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence,"Decentralized learning has recently been attracting increasing attention for
its applications in parallel computation and privacy preservation. Many recent
studies stated that the underlying network topology with a faster consensus
rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for
decentralized learning. However, a topology with a fast consensus rate, e.g.,
the exponential graph, generally has a large maximum degree, which incurs
significant communication costs. Thus, seeking topologies with both a fast
consensus rate and small maximum degree is important. In this study, we propose
a novel topology combining both a fast consensus rate and small maximum degree
called the Base-$(k + 1)$ Graph. Unlike the existing topologies, the Base-$(k +
1)$ Graph enables all nodes to reach the exact consensus after a finite number
of iterations for any number of nodes and maximum degree k. Thanks to this
favorable property, the Base-$(k + 1)$ Graph endows Decentralized SGD (DSGD)
with both a faster convergence rate and more communication efficiency than the
exponential graph. We conducted experiments with various topologies,
demonstrating that the Base-$(k + 1)$ Graph enables various decentralized
learning methods to achieve higher accuracy with better communication
efficiency than the existing topologies."
1,4,Few-Shot Continual Learning for Conditional Generative Adversarial Networks,"In few-shot continual learning for generative models, a target mode must be
learned with limited samples without adversely affecting the previously learned
modes. In this paper, we propose a new continual learning approach for
conditional generative adversarial networks (cGAN) based on a new mode-affinity
measure for generative modeling. Our measure is entirely based on the cGAN's
discriminator and can identify the existing modes that are most similar to the
target. Subsequently, we expand the continual learning model by including the
target mode using a weighted label derived from those of the closest modes. To
prevent catastrophic forgetting, we first generate labeled data samples using
the cGAN's generator, and then train the cGAN model for the target mode while
memory replaying with the generated data. Our experimental results demonstrate
the efficacy of our approach in improving the generation performance over the
baselines and the state-of-the-art approaches for various standard datasets
while utilizing fewer training samples."
1,4,Generalized Precision Matrix for Scalable Estimation of Nonparametric Markov Networks,"A Markov network characterizes the conditional independence structure, or
Markov property, among a set of random variables. Existing work focuses on
specific families of distributions (e.g., exponential families) and/or certain
structures of graphs, and most of them can only handle variables of a single
data type (continuous or discrete). In this work, we characterize the
conditional independence structure in general distributions for all data types
(i.e., continuous, discrete, and mixed-type) with a Generalized Precision
Matrix (GPM). Besides, we also allow general functional relations among
variables, thus giving rise to a Markov network structure learning algorithm in
one of the most general settings. To deal with the computational challenge of
the problem, especially for large graphs, we unify all cases under the same
umbrella of a regularized score matching framework. We validate the theoretical
results and demonstrate the scalability empirically in various settings."
1,4,Meta-learning for heterogeneous treatment effect estimation with closed-form solvers,"This article proposes a meta-learning method for estimating the conditional
average treatment effect (CATE) from a few observational data. The proposed
method learns how to estimate CATEs from multiple tasks and uses the knowledge
for unseen tasks. In the proposed method, based on the meta-learner framework,
we decompose the CATE estimation problem into sub-problems. For each
sub-problem, we formulate our estimation models using neural networks with
task-shared and task-specific parameters. With our formulation, we can obtain
optimal task-specific parameters in a closed form that are differentiable with
respect to task-shared parameters, making it possible to perform effective
meta-learning. The task-shared parameters are trained such that the expected
CATE estimation performance in few-shot settings is improved by minimizing the
difference between a CATE estimated with a large amount of data and one
estimated with just a few data. Our experimental results demonstrate that our
method outperforms the existing meta-learning approaches and CATE estimation
methods."
1,4,Counterfactuals for Design: A Model-Agnostic Method For Design Recommendations,"We introduce Multi-Objective Counterfactuals for Design (MCD), a novel method
for counterfactual optimization in design problems. Counterfactuals are
hypothetical situations that can lead to a different decision or choice. In
this paper, the authors frame the counterfactual search problem as a design
recommendation tool that can help identify modifications to a design, leading
to better functional performance. MCD improves upon existing counterfactual
search methods by supporting multi-objective queries, which are crucial in
design problems, and by decoupling the counterfactual search and sampling
processes, thus enhancing efficiency and facilitating objective tradeoff
visualization. The paper demonstrates MCD's core functionality using a
two-dimensional test case, followed by three case studies of bicycle design
that showcase MCD's effectiveness in real-world design problems. In the first
case study, MCD excels at recommending modifications to query designs that can
significantly enhance functional performance, such as weight savings and
improvements to the structural safety factor. The second case study
demonstrates that MCD can work with a pre-trained language model to suggest
design changes based on a subjective text prompt effectively. Lastly, the
authors task MCD with increasing a query design's similarity to a target image
and text prompt while simultaneously reducing weight and improving structural
performance, demonstrating MCD's performance on a complex multimodal query.
Overall, MCD has the potential to provide valuable recommendations for
practitioners and design automation researchers looking for answers to their
``What if'' questions by exploring hypothetical design modifications and their
impact on multiple design objectives. The code, test problems, and datasets
used in the paper are available to the public at
decode.mit.edu/projects/counterfactuals/."
0,4,On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation,"In this paper, we study the statistical efficiency of Reinforcement Learning
in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function
approximation. We introduce a new concept called Mean-Field Model-Based Eluder
Dimension (MBED), which subsumes a rich family of Mean-Field RL problems.
Additionally, we propose algorithms based on Optimistic Maximal Likelihood
Estimation, which can return an $\epsilon$-optimal policy for MFC or an
$\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial
w.r.t. relevant parameters and independent of the number of states, actions and
the number of agents. Notably, our results only require a mild assumption of
Lipschitz continuity on transition dynamics and avoid strong structural
assumptions in previous work. Finally, in the tabular setting, given the access
to a generative model, we establish an exponential lower bound for MFC setting,
while providing a novel sample-efficient model elimination algorithm to
approximate equilibrium in MFG setting. Our results reveal a fundamental
separation between RL for single-agent, MFC, and MFG from the sample efficiency
perspective."
1,4,Real-Time Variational Method for Learning Neural Trajectory and its Dynamics,"Latent variable models have become instrumental in computational neuroscience
for reasoning about neural computation. This has fostered the development of
powerful offline algorithms for extracting latent neural trajectories from
neural recordings. However, despite the potential of real time alternatives to
give immediate feedback to experimentalists, and enhance experimental design,
they have received markedly less attention. In this work, we introduce the
exponential family variational Kalman filter (eVKF), an online recursive
Bayesian method aimed at inferring latent trajectories while simultaneously
learning the dynamical system generating them. eVKF works for arbitrary
likelihoods and utilizes the constant base measure exponential family to model
the latent state stochasticity. We derive a closed-form variational analogue to
the predict step of the Kalman filter which leads to a provably tighter bound
on the ELBO compared to another online variational method. We validate our
method on synthetic and real-world data, and, notably, show that it achieves
competitive performance"
1,4,"Evidence Networks: simple losses for fast, amortized, neural Bayesian model comparison","Evidence Networks can enable Bayesian model comparison when state-of-the-art
methods (e.g. nested sampling) fail and even when likelihoods or priors are
intractable or unknown. Bayesian model comparison, i.e. the computation of
Bayes factors or evidence ratios, can be cast as an optimization problem.
Though the Bayesian interpretation of optimal classification is well-known,
here we change perspective and present classes of loss functions that result in
fast, amortized neural estimators that directly estimate convenient functions
of the Bayes factor. This mitigates numerical inaccuracies associated with
estimating individual model probabilities. We introduce the leaky parity-odd
power (l-POP) transform, leading to the novel ``l-POP-Exponential'' loss
function. We explore neural density estimation for data probability in
different models, showing it to be less accurate and scalable than Evidence
Networks. Multiple real-world and synthetic examples illustrate that Evidence
Networks are explicitly independent of dimensionality of the parameter space
and scale mildly with the complexity of the posterior probability density
function. This simple yet powerful approach has broad implications for model
inference tasks. As an application of Evidence Networks to real-world data we
compute the Bayes factor for two models with gravitational lensing data of the
Dark Energy Survey. We briefly discuss applications of our methods to other,
related problems of model comparison and evaluation in implicit inference
settings."
1,4,The noise level in linear regression with dependent data,"We derive upper bounds for random design linear regression with dependent
($\beta$-mixing) data absent any realizability assumptions. In contrast to the
strictly realizable martingale noise regime, no sharp instance-optimal
non-asymptotics are available in the literature. Up to constant factors, our
analysis correctly recovers the variance term predicted by the Central Limit
Theorem -- the noise level of the problem -- and thus exhibits graceful
degradation as we introduce misspecification. Past a burn-in, our result is
sharp in the moderate deviations regime, and in particular does not inflate the
leading order term by mixing time factors."
1,4,Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study,"The rise of machine learning (ML) systems has exacerbated their carbon
footprint due to increased capabilities and model sizes. However, there is
scarce knowledge on how the carbon footprint of ML models is actually measured,
reported, and evaluated. In light of this, the paper aims to analyze the
measurement of the carbon footprint of 1,417 ML models and associated datasets
on Hugging Face, which is the most popular repository for pretrained ML models.
The goal is to provide insights and recommendations on how to report and
optimize the carbon efficiency of ML models. The study includes the first
repository mining study on the Hugging Face Hub API on carbon emissions. This
study seeks to answer two research questions: (1) how do ML model creators
measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects
impact the carbon emissions of training ML models? The study yielded several
key findings. These include a decreasing proportion of carbon
emissions-reporting models, a slight decrease in reported carbon footprint on
Hugging Face over the past 2 years, and a continued dominance of NLP as the
main application domain. Furthermore, the study uncovers correlations between
carbon emissions and various attributes such as model size, dataset size, and
ML application domains. These results highlight the need for software
measurements to improve energy reporting practices and promote carbon-efficient
model development within the Hugging Face community. In response to this issue,
two classifications are proposed: one for categorizing models based on their
carbon emission reporting practices and another for their carbon efficiency.
The aim of these classification proposals is to foster transparency and
sustainable model development within the ML community."
1,4,Attacks on Online Learners: a Teacher-Student Analysis,"Machine learning models are famously vulnerable to adversarial attacks: small
ad-hoc perturbations of the data that can catastrophically alter the model
predictions. While a large literature has studied the case of test-time attacks
on pre-trained models, the important case of attacks in an online learning
setting has received little attention so far. In this work, we use a
control-theoretical perspective to study the scenario where an attacker may
perturb data labels to manipulate the learning dynamics of an online learner.
We perform a theoretical analysis of the problem in a teacher-student setup,
considering different attack strategies, and obtaining analytical results for
the steady state of simple linear learners. These results enable us to prove
that a discontinuous transition in the learner's accuracy occurs when the
attack strength exceeds a critical threshold. We then study empirically attacks
on learners with complex architectures using real data, confirming the insights
of our theoretical analysis. Our findings show that greedy attacks can be
extremely efficient, especially when data stream in small batches."
1,4,Statistical Foundations of Prior-Data Fitted Networks,"Prior-data fitted networks (PFNs) were recently proposed as a new paradigm
for machine learning. Instead of training the network to an observed training
set, a fixed model is pre-trained offline on small, simulated training sets
from a variety of tasks. The pre-trained model is then used to infer class
probabilities in-context on fresh training sets with arbitrary size and
distribution. Empirically, PFNs achieve state-of-the-art performance on tasks
with similar size to the ones used in pre-training. Surprisingly, their
accuracy further improves when passed larger data sets during inference. This
article establishes a theoretical foundation for PFNs and illuminates the
statistical mechanisms governing their behavior. While PFNs are motivated by
Bayesian ideas, a purely frequentistic interpretation of PFNs as pre-tuned, but
untrained predictors explains their behavior. A predictor's variance vanishes
if its sensitivity to individual training samples does and the bias vanishes
only if it is appropriately localized around the test feature. The transformer
architecture used in current PFN implementations ensures only the former. These
findings shall prove useful for designing architectures with favorable
empirical behavior."
1,4,Small noise analysis for Tikhonov and RKHS regularizations,"Regularization plays a pivotal role in ill-posed machine learning and inverse
problems. However, the fundamental comparative analysis of various
regularization norms remains open. We establish a small noise analysis
framework to assess the effects of norms in Tikhonov and RKHS regularizations,
in the context of ill-posed linear inverse problems with Gaussian noise. This
framework studies the convergence rates of regularized estimators in the small
noise limit and reveals the potential instability of the conventional
L2-regularizer. We solve such instability by proposing an innovative class of
adaptive fractional RKHS regularizers, which covers the L2 Tikhonov and RKHS
regularizations by adjusting the fractional smoothness parameter. A surprising
insight is that over-smoothing via these fractional RKHSs consistently yields
optimal convergence rates, but the optimal hyper-parameter may decay too fast
to be selected in practice."
1,4,Difference of Submodular Minimization via DC Programming,"Minimizing the difference of two submodular (DS) functions is a problem that
naturally occurs in various machine learning problems. Although it is well
known that a DS problem can be equivalently formulated as the minimization of
the difference of two convex (DC) functions, existing algorithms do not fully
exploit this connection. A classical algorithm for DC problems is called the DC
algorithm (DCA). We introduce variants of DCA and its complete form (CDCA) that
we apply to the DC program corresponding to DS minimization. We extend existing
convergence properties of DCA, and connect them to convergence properties on
the DS problem. Our results on DCA match the theoretical guarantees satisfied
by existing DS algorithms, while providing a more complete characterization of
convergence properties. In the case of CDCA, we obtain a stronger local
minimality guarantee. Our numerical results show that our proposed algorithms
outperform existing baselines on two applications: speech corpus selection and
feature selection."
1,4,A unified framework for information-theoretic generalization bounds,"This paper presents a general methodology for deriving information-theoretic
generalization bounds for learning algorithms. The main technical tool is a
probabilistic decorrelation lemma based on a change of measure and a relaxation
of Young's inequality in $L_{\psi_p}$ Orlicz spaces. Using the decorrelation
lemma in combination with other techniques, such as symmetrization, couplings,
and chaining in the space of probability measures, we obtain new upper bounds
on the generalization error, both in expectation and in high probability, and
recover as special cases many of the existing generalization bounds, including
the ones based on mutual information, conditional mutual information,
stochastic chaining, and PAC-Bayes inequalities. In addition, the
Fernique-Talagrand upper bound on the expected supremum of a subgaussian
process emerges as a special case."
1,4,High-dimensional Asymptotics of Denoising Autoencoders,"We address the problem of denoising data from a Gaussian mixture using a
two-layer non-linear autoencoder with tied weights and a skip connection. We
consider the high-dimensional limit where the number of training samples and
the input dimension jointly tend to infinity while the number of hidden units
remains bounded. We provide closed-form expressions for the denoising
mean-squared test error. Building on this result, we quantitatively
characterize the advantage of the considered architecture over the autoencoder
without the skip connection that relates closely to principal component
analysis. We further show that our results accurately capture the learning
curves on a range of real data sets."
0,4,Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL,"While policy optimization algorithms have played an important role in recent
empirical success of Reinforcement Learning (RL), the existing theoretical
understanding of policy optimization remains rather limited -- they are either
restricted to tabular MDPs or suffer from highly suboptimal sample complexity,
especial in online RL where exploration is necessary. This paper proposes a
simple efficient policy optimization framework -- Optimistic NPG for online RL.
Optimistic NPG can be viewed as simply combining of the classic natural policy
gradient (NPG) algorithm [Kakade, 2001] with optimistic policy evaluation
subroutines to encourage exploration. For $d$-dimensional linear MDPs,
Optimistic NPG is computationally efficient, and learns an
$\varepsilon$-optimal policy within $\tilde{O}(d^2/\varepsilon^3)$ samples,
which is the first computationally efficient algorithm whose sample complexity
has the optimal dimension dependence $\tilde{\Theta}(d^2)$. It also improves
over state-of-the-art results of policy optimization algorithms [Zanette et
al., 2021] by a factor of $d$. For general function approximation that subsumes
linear MDPs, Optimistic NPG, to our best knowledge, is also the first policy
optimization algorithm that achieves the polynomial sample complexity for
learning near-optimal policies."
3,4,Massively Parallel Reweighted Wake-Sleep,"Reweighted wake-sleep (RWS) is a machine learning method for performing
Bayesian inference in a very general class of models. RWS draws $K$ samples
from an underlying approximate posterior, then uses importance weighting to
provide a better estimate of the true posterior. RWS then updates its
approximate posterior towards the importance-weighted estimate of the true
posterior. However, recent work [Chattergee and Diaconis, 2018] indicates that
the number of samples required for effective importance weighting is
exponential in the number of latent variables. Attaining such a large number of
importance samples is intractable in all but the smallest models. Here, we
develop massively parallel RWS, which circumvents this issue by drawing $K$
samples of all $n$ latent variables, and individually reasoning about all $K^n$
possible combinations of samples. While reasoning about $K^n$ combinations
might seem intractable, the required computations can be performed in
polynomial time by exploiting conditional independencies in the generative
model. We show considerable improvements over standard ""global"" RWS, which
draws $K$ samples from the full joint."
1,4,Mode Connectivity in Auction Design,"Optimal auction design is a fundamental problem in algorithmic game theory.
This problem is notoriously difficult already in very simple settings. Recent
work in differentiable economics showed that neural networks can efficiently
learn known optimal auction mechanisms and discover interesting new ones. In an
attempt to theoretically justify their empirical success, we focus on one of
the first such networks, RochetNet, and a generalized version for affine
maximizer auctions. We prove that they satisfy mode connectivity, i.e., locally
optimal solutions are connected by a simple, piecewise linear path such that
every solution on the path is almost as good as one of the two local optima.
Mode connectivity has been recently investigated as an intriguing empirical and
theoretically justifiable property of neural networks used for prediction
problems. Our results give the first such analysis in the context of
differentiable economics, where neural networks are used directly for solving
non-convex optimization problems."
1,4,Dynamic Term Structure Models with Nonlinearities using Gaussian Processes,"The importance of unspanned macroeconomic variables for Dynamic Term
Structure Models has been intensively discussed in the literature. To our best
knowledge the earlier studies considered only linear interactions between the
economy and the real-world dynamics of interest rates in DTSMs. We propose a
generalized modelling setup for Gaussian DTSMs which allows for unspanned
nonlinear associations between the two and we exploit it in forecasting.
Specifically, we construct a custom sequential Monte Carlo estimation and
forecasting scheme where we introduce Gaussian Process priors to model
nonlinearities. Sequential scheme we propose can also be used with dynamic
portfolio optimization to assess the potential of generated economic value to
investors. The methodology is presented using US Treasury data and selected
macroeconomic indices. Namely, we look at core inflation and real economic
activity. We contrast the results obtained from the nonlinear model with those
stemming from an application of a linear model. Unlike for real economic
activity, in case of core inflation we find that, compared to linear models,
application of nonlinear models leads to statistically significant gains in
economic value across considered maturities."
1,4,Estimation Beyond Data Reweighting: Kernel Method of Moments,"Moment restrictions and their conditional counterparts emerge in many areas
of machine learning and statistics ranging from causal inference to
reinforcement learning. Estimators for these tasks, generally called methods of
moments, include the prominent generalized method of moments (GMM) which has
recently gained attention in causal inference. GMM is a special case of the
broader family of empirical likelihood estimators which are based on
approximating a population distribution by means of minimizing a
$\varphi$-divergence to an empirical distribution. However, the use of
$\varphi$-divergences effectively limits the candidate distributions to
reweightings of the data samples. We lift this long-standing limitation and
provide a method of moments that goes beyond data reweighting. This is achieved
by defining an empirical likelihood estimator based on maximum mean discrepancy
which we term the kernel method of moments (KMM). We provide a variant of our
estimator for conditional moment restrictions and show that it is
asymptotically first-order optimal for such problems. Finally, we show that our
method achieves competitive performance on several conditional moment
restriction tasks."
3,4,Minimum-Risk Recalibration of Classifiers,"Recalibrating probabilistic classifiers is vital for enhancing the
reliability and accuracy of predictive models. Despite the development of
numerous recalibration algorithms, there is still a lack of a comprehensive
theory that integrates calibration and sharpness (which is essential for
maintaining predictive power). In this paper, we introduce the concept of
minimum-risk recalibration within the framework of mean-squared-error (MSE)
decomposition, offering a principled approach for evaluating and recalibrating
probabilistic classifiers. Using this framework, we analyze the uniform-mass
binning (UMB) recalibration method and establish a finite-sample risk upper
bound of order $\tilde{O}(B/n + 1/B^2)$ where $B$ is the number of bins and $n$
is the sample size. By balancing calibration and sharpness, we further
determine that the optimal number of bins for UMB scales with $n^{1/3}$,
resulting in a risk bound of approximately $O(n^{-2/3})$. Additionally, we
tackle the challenge of label shift by proposing a two-stage approach that
adjusts the recalibration function using limited labeled data from the target
domain. Our results show that transferring a calibrated classifier requires
significantly fewer target samples compared to recalibrating from scratch. We
validate our theoretical findings through numerical simulations, which confirm
the tightness of the proposed bounds, the optimal number of bins, and the
effectiveness of label shift adaptation."
1,4,Functional sufficient dimension reduction through information maximization with application to classification,"Considering the case where the response variable is a categorical variable
and the predictor is a random function, two novel functional sufficient
dimensional reduction (FSDR) methods are proposed based on mutual information
and square loss mutual information. Compared to the classical FSDR methods,
such as functional sliced inverse regression and functional sliced average
variance estimation, the proposed methods are appealing because they are
capable of estimating multiple effective dimension reduction directions in the
case of a relatively small number of categories, especially for the binary
response. Moreover, the proposed methods do not require the restrictive linear
conditional mean assumption and the constant covariance assumption. They avoid
the inverse problem of the covariance operator which is often encountered in
the functional sufficient dimension reduction. The functional principal
component analysis with truncation be used as a regularization mechanism. Under
some mild conditions, the statistical consistency of the proposed methods is
established. It is demonstrated that the two methods are competitive compared
with some existing FSDR methods by simulations and real data analyses."
1,4,Discounted Thompson Sampling for Non-Stationary Bandit Problems,"Non-stationary multi-armed bandit (NS-MAB) problems have recently received
significant attention. NS-MAB are typically modelled in two scenarios: abruptly
changing, where reward distributions remain constant for a certain period and
change at unknown time steps, and smoothly changing, where reward distributions
evolve smoothly based on unknown dynamics. In this paper, we propose Discounted
Thompson Sampling (DS-TS) with Gaussian priors to address both non-stationary
settings. Our algorithm passively adapts to changes by incorporating a
discounted factor into Thompson Sampling. DS-TS method has been experimentally
validated, but analysis of the regret upper bound is currently lacking. Under
mild assumptions, we show that DS-TS with Gaussian priors can achieve nearly
optimal regret bound on the order of $\tilde{O}(\sqrt{TB_T})$ for abruptly
changing and $\tilde{O}(T^{\beta})$ for smoothly changing, where $T$ is the
number of time steps, $B_T$ is the number of breakpoints, $\beta$ is associated
with the smoothly changing environment and $\tilde{O}$ hides the parameters
independent of $T$ as well as logarithmic terms. Furthermore, empirical
comparisons between DS-TS and other non-stationary bandit algorithms
demonstrate its competitive performance. Specifically, when prior knowledge of
the maximum expected reward is available, DS-TS has the potential to outperform
state-of-the-art algorithms."
0,4,The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond,"When the data used for reinforcement learning (RL) are collected by multiple
agents in a distributed manner, federated versions of RL algorithms allow
collaborative learning without the need of sharing local data. In this paper,
we consider federated Q-learning, which aims to learn an optimal Q-function by
periodically aggregating local Q-estimates trained on local data alone.
Focusing on infinite-horizon tabular Markov decision processes, we provide
sample complexity guarantees for both the synchronous and asynchronous variants
of federated Q-learning. In both cases, our bounds exhibit a linear speedup
with respect to the number of agents and sharper dependencies on other salient
problem parameters. Moreover, existing approaches to federated Q-learning adopt
an equally-weighted averaging of local Q-estimates, which can be highly
sub-optimal in the asynchronous setting since the local trajectories can be
highly heterogeneous due to different local behavior policies. Existing sample
complexity scales inverse proportionally to the minimum entry of the stationary
state-action occupancy distributions over all agents, requiring that every
agent covers the entire state-action space. Instead, we propose a novel
importance averaging algorithm, giving larger weights to more frequently
visited state-action pairs. The improved sample complexity scales inverse
proportionally to the minimum entry of the average stationary state-action
occupancy distribution of all agents, thus only requiring the agents
collectively cover the entire state-action space, unveiling the blessing of
heterogeneity."
1,4,Posterior Inference on Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance,"From the classical and influential works of Neal (1996), it is known that the
infinite width scaling limit of a Bayesian neural network with one hidden layer
is a Gaussian process, \emph{when the network weights have bounded prior
variance}. Neal's result has been extended to networks with multiple hidden
layers and to convolutional neural networks, also with Gaussian process scaling
limits. The tractable properties of Gaussian processes then allow
straightforward posterior inference and uncertainty quantification,
considerably simplifying the study of the limit process compared to a network
of finite width. Neural network weights with unbounded variance, however, pose
unique challenges. In this case, the classical central limit theorem breaks
down and it is well known that the scaling limit is an $\alpha$-stable process
under suitable conditions. However, current literature is primarily limited to
forward simulations under these processes and the problem of posterior
inference under such a scaling limit remains largely unaddressed, unlike in the
Gaussian process case. To this end, our contribution is an interpretable and
computationally efficient procedure for posterior inference, using a
\emph{conditionally Gaussian} representation, that then allows full use of the
Gaussian process machinery for tractable posterior inference and uncertainty
quantification in the non-Gaussian regime."
1,4,Augmented Message Passing Stein Variational Gradient Descent,"Stein Variational Gradient Descent (SVGD) is a popular particle-based method
for Bayesian inference. However, its convergence suffers from the variance
collapse, which reduces the accuracy and diversity of the estimation. In this
paper, we study the isotropy property of finite particles during the
convergence process and show that SVGD of finite particles cannot spread across
the entire sample space. Instead, all particles tend to cluster around the
particle center within a certain range and we provide an analytical bound for
this cluster. To further improve the effectiveness of SVGD for high-dimensional
problems, we propose the Augmented Message Passing SVGD (AUMP-SVGD) method,
which is a two-stage optimization procedure that does not require sparsity of
the target distribution, unlike the MP-SVGD method. Our algorithm achieves
satisfactory accuracy and overcomes the variance collapse problem in various
benchmark problems."
3,4,Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models,"We focus on the task of learning a single index model $\sigma(w^\star \cdot
x)$ with respect to the isotropic Gaussian distribution in $d$ dimensions.
Prior work has shown that the sample complexity of learning $w^\star$ is
governed by the information exponent $k^\star$ of the link function $\sigma$,
which is defined as the index of the first nonzero Hermite coefficient of
$\sigma$. Ben Arous et al. (2021) showed that $n \gtrsim d^{k^\star-1}$ samples
suffice for learning $w^\star$ and that this is tight for online SGD. However,
the CSQ lower bound for gradient based methods only shows that $n \gtrsim
d^{k^\star/2}$ samples are necessary. In this work, we close the gap between
the upper and lower bounds by showing that online SGD on a smoothed loss learns
$w^\star$ with $n \gtrsim d^{k^\star/2}$ samples. We also draw connections to
statistical analyses of tensor PCA and to the implicit regularization effects
of minibatch SGD on empirical losses."
3,4,Tensor Products and Hyperdimensional Computing,"Following up on a previous analysis of graph embeddings, we generalize and
expand some results to the general setting of vector symbolic architectures
(VSA) and hyperdimensional computing (HDC). Importantly, we explore the
mathematical relationship between superposition, orthogonality, and tensor
product. We establish the tensor product representation as the central
representation, with a suite of unique properties. These include it being the
most general and expressive representation, as well as being the most
compressed representation that has errorrless unbinding and detection."
1,4,Counterfactually Comparing Abstaining Classifiers,"Abstaining classifiers have the option to abstain from making predictions on
inputs that they are unsure about. These classifiers are becoming increasingly
popular in high-stake decision-making problems, as they can withhold uncertain
predictions to improve their reliability and safety. When evaluating black-box
abstaining classifier(s), however, we lack a principled approach that accounts
for what the classifier would have predicted on its abstentions. These missing
predictions are crucial when, e.g., a radiologist is unsure of their diagnosis
or when a driver is inattentive in a self-driving car. In this paper, we
introduce a novel approach and perspective to the problem of evaluating and
comparing abstaining classifiers by treating abstentions as missing data. Our
evaluation approach is centered around defining the counterfactual score of an
abstaining classifier, defined as the expected performance of the classifier
had it not been allowed to abstain. We specify the conditions under which the
counterfactual score is identifiable: if the abstentions are stochastic, and if
the evaluation data is independent of the training data (ensuring that the
predictions are missing at random), then the score is identifiable. Note that,
if abstentions are deterministic, then the score is unidentifiable because the
classifier can perform arbitrarily poorly on its abstentions. Leveraging tools
from observational causal inference, we then develop nonparametric and doubly
robust methods to efficiently estimate this quantity under identification. Our
approach is examined in both simulated and real data experiments."
3,4,Learning Pose Image Manifolds Using Geometry-Preserving GANs and Elasticae,"This paper investigates the challenge of learning image manifolds,
specifically pose manifolds, of 3D objects using limited training data. It
proposes a DNN approach to manifold learning and for predicting images of
objects for novel, continuous 3D rotations. The approach uses two distinct
concepts: (1) Geometric Style-GAN (Geom-SGAN), which maps images to
low-dimensional latent representations and maintains the (first-order) manifold
geometry. That is, it seeks to preserve the pairwise distances between base
points and their tangent spaces, and (2) uses Euler's elastica to smoothly
interpolate between directed points (points + tangent directions) in the
low-dimensional latent space. When mapped back to the larger image space, the
resulting interpolations resemble videos of rotating objects. Extensive
experiments establish the superiority of this framework in learning paths on
rotation manifolds, both visually and quantitatively, relative to
state-of-the-art GANs and VAEs."
1,4,Learning Likelihood Ratios with Neural Network Classifiers,"The likelihood ratio is a crucial quantity for statistical inference in
science that enables hypothesis testing, construction of confidence intervals,
reweighting of distributions, and more. Many modern scientific applications,
however, make use of data- or simulation-driven models for which computing the
likelihood ratio can be very difficult or even impossible. By applying the
so-called ``likelihood ratio trick,'' approximations of the likelihood ratio
may be computed using clever parametrizations of neural network-based
classifiers. A number of different neural network setups can be defined to
satisfy this procedure, each with varying performance in approximating the
likelihood ratio when using finite training data. We present a series of
empirical studies detailing the performance of several common loss functionals
and parametrizations of the classifier output in approximating the likelihood
ratio of two univariate and multivariate Gaussian distributions as well as
simulated high-energy particle physics datasets."
1,4,On Consistency of Signatures Using Lasso,"Signature transforms are iterated path integrals of continuous and
discrete-time time series data, and their universal nonlinearity linearizes the
problem of feature selection. This paper revisits the consistency issue of
Lasso regression for the signature transform, both theoretically and
numerically. Our study shows that, for processes and time series that are
closer to Brownian motion or random walk with weaker inter-dimensional
correlations, the Lasso regression is more consistent for their signatures
defined by It\^o integrals; for mean reverting processes and time series, their
signatures defined by Stratonovich integrals have more consistency in the Lasso
regression. Our findings highlight the importance of choosing appropriate
definitions of signatures and stochastic models in statistical inference and
machine learning."
1,4,Optimality of Message-Passing Architectures for Sparse Graphs,"We study the node classification problem on feature-decorated graphs in the
sparse setting, i.e., when the expected degree of a node is $O(1)$ in the
number of nodes. Such graphs are typically known to be locally tree-like. We
introduce a notion of Bayes optimality for node classification tasks, called
asymptotic local Bayes optimality, and compute the optimal classifier according
to this criterion for a fairly general statistical data model with arbitrary
distributions of the node features and edge connectivity. The optimal
classifier is implementable using a message-passing graph neural network
architecture. We then compute the generalization error of this classifier and
compare its performance against existing learning methods theoretically on a
well-studied statistical model with naturally identifiable signal-to-noise
ratios (SNRs) in the data. We find that the optimal message-passing
architecture interpolates between a standard MLP in the regime of low graph
signal and a typical convolution in the regime of high graph signal.
Furthermore, we prove a corresponding non-asymptotic result."
1,4,Active Learning in Symbolic Regression with Physical Constraints,"Evolutionary symbolic regression (SR) fits a symbolic equation to data, which
gives a concise interpretable model. We explore using SR as a method to propose
which data to gather in an active learning setting with physical constraints.
SR with active learning proposes which experiments to do next. Active learning
is done with query by committee, where the Pareto frontier of equations is the
committee. The physical constraints improve proposed equations in very low data
settings. These approaches reduce the data required for SR and achieves state
of the art results in data required to rediscover known equations."
1,4,Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning,"This paper studies tabular reinforcement learning (RL) in the hybrid setting,
which assumes access to both an offline dataset and online interactions with
the unknown environment. A central question boils down to how to efficiently
utilize online data collection to strengthen and complement the offline dataset
and enable effective policy fine-tuning. Leveraging recent advances in
reward-agnostic exploration and model-based offline RL, we design a three-stage
hybrid RL algorithm that beats the best of both worlds -- pure offline RL and
pure online RL -- in terms of sample complexities. The proposed algorithm does
not require any reward information during data collection. Our theory is
developed based on a new notion called single-policy partial concentrability,
which captures the trade-off between distribution mismatch and miscoverage and
guides the interplay between offline and online data."
1,4,Reaching Kesten-Stigum Threshold in the Stochastic Block Model under Node Corruptions,"We study robust community detection in the context of node-corrupted
stochastic block model, where an adversary can arbitrarily modify all the edges
incident to a fraction of the $n$ vertices. We present the first
polynomial-time algorithm that achieves weak recovery at the Kesten-Stigum
threshold even in the presence of a small constant fraction of corrupted nodes.
Prior to this work, even state-of-the-art robust algorithms were known to break
under such node corruption adversaries, when close to the Kesten-Stigum
threshold.
  We further extend our techniques to the $Z_2$ synchronization problem, where
our algorithm reaches the optimal recovery threshold in the presence of similar
strong adversarial perturbations.
  The key ingredient of our algorithm is a novel identifiability proof that
leverages the push-out effect of the Grothendieck norm of principal
submatrices."
1,4,"Separability and Scatteredness (S&S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection","Support Vector Machine (SVM) is a robust machine learning algorithm with
broad applications in classification, regression, and outlier detection. SVM
requires tuning the regularization parameter (RP) which controls the model
capacity and the generalization performance. Conventionally, the optimum RP is
found by comparison of a range of values through the Cross-Validation (CV)
procedure. In addition, for non-linearly separable data, the SVM uses kernels
where a set of kernels, each with a set of parameters, denoted as a grid of
kernels, are considered. The optimal choice of RP and the grid of kernels is
through the grid-search of CV. By stochastically analyzing the behavior of the
regularization parameter, this work shows that the SVM performance can be
modeled as a function of separability and scatteredness (S&S) of the data.
Separability is a measure of the distance between classes, and scatteredness is
the ratio of the spread of data points. In particular, for the hinge loss cost
function, an S&S ratio-based table provides the optimum RP. The S&S ratio is a
powerful value that can automatically detect linear or non-linear separability
before using the SVM algorithm. The provided S&S ratio-based table can also
provide the optimum kernel and its parameters before using the SVM algorithm.
Consequently, the computational complexity of the CV grid-search is reduced to
only one time use of the SVM. The simulation results on the real dataset
confirm the superiority and efficiency of the proposed approach in the sense of
computational complexity over the grid-search CV method."
1,4,Evaluating Dynamic Conditional Quantile Treatment Effects with Applications in Ridesharing,"Many modern tech companies, such as Google, Uber, and Didi, utilize online
experiments (also known as A/B testing) to evaluate new policies against
existing ones. While most studies concentrate on average treatment effects,
situations with skewed and heavy-tailed outcome distributions may benefit from
alternative criteria, such as quantiles. However, assessing dynamic quantile
treatment effects (QTE) remains a challenge, particularly when dealing with
data from ride-sourcing platforms that involve sequential decision-making
across time and space. In this paper, we establish a formal framework to
calculate QTE conditional on characteristics independent of the treatment.
Under specific model assumptions, we demonstrate that the dynamic conditional
QTE (CQTE) equals the sum of individual CQTEs across time, even though the
conditional quantile of cumulative rewards may not necessarily equate to the
sum of conditional quantiles of individual rewards. This crucial insight
significantly streamlines the estimation and inference processes for our target
causal estimand. We then introduce two varying coefficient decision process
(VCDP) models and devise an innovative method to test the dynamic CQTE.
Moreover, we expand our approach to accommodate data from spatiotemporal
dependent experiments and examine both conditional quantile direct and indirect
effects. To showcase the practical utility of our method, we apply it to three
real-world datasets from a ride-sourcing platform. Theoretical findings and
comprehensive simulation studies further substantiate our proposal."
1,4,Algorithms for Boolean Matrix Factorization using Integer Programming,"Boolean matrix factorization (BMF) approximates a given binary input matrix
as the product of two smaller binary factors. As opposed to binary matrix
factorization which uses standard arithmetic, BMF uses the Boolean OR and
Boolean AND operations to perform matrix products, which leads to lower
reconstruction errors. BMF is an NP-hard problem. In this paper, we first
propose an alternating optimization (AO) strategy that solves the subproblem in
one factor matrix in BMF using an integer program (IP). We also provide two
ways to initialize the factors within AO. Then, we show how several solutions
of BMF can be combined optimally using another IP. This allows us to come up
with a new algorithm: it generates several solutions using AO and then combines
them in an optimal way. Experiments show that our algorithms (available on
gitlab) outperform the state of the art on medium-scale problems."
1,4,A Global-Local Approximation Framework for Large-Scale Gaussian Process Modeling,"In this work, we propose a novel framework for large-scale Gaussian process
(GP) modeling. Contrary to the global, and local approximations proposed in the
literature to address the computational bottleneck with exact GP modeling, we
employ a combined global-local approach in building the approximation. Our
framework uses a subset-of-data approach where the subset is a union of a set
of global points designed to capture the global trend in the data, and a set of
local points specific to a given testing location to capture the local trend
around the testing location. The correlation function is also modeled as a
combination of a global, and a local kernel. The performance of our framework,
which we refer to as TwinGP, is on par or better than the state-of-the-art GP
modeling methods at a fraction of their computational cost."
1,4,Automatic Hyperparameter Tuning in Sparse Matrix Factorization,"We study the problem of hyperparameter tuning in sparse matrix factorization
under Bayesian framework. In the prior work, an analytical solution of sparse
matrix factorization with Laplace prior was obtained by variational Bayes
method under several approximations. Based on this solution, we propose a novel
numerical method of hyperparameter tuning by evaluating the zero point of
normalization factor in sparse matrix prior. We also verify that our method
shows excellent performance for ground-truth sparse matrix reconstruction by
comparing it with the widely-used algorithm of sparse principal component
analysis."
0,4,A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization,"We prove Wasserstein inverse reinforcement learning enables the learner's
reward values to imitate the expert's reward values in a finite iteration for
multi-objective optimizations. Moreover, we prove Wasserstein inverse
reinforcement learning enables the learner's optimal solutions to imitate the
expert's optimal solutions for multi-objective optimizations with lexicographic
order."
1,4,Optimal Weighted Random Forests,"The random forest (RF) algorithm has become a very popular prediction method
for its great flexibility and promising accuracy. In RF, it is conventional to
put equal weights on all the base learners (trees) to aggregate their
predictions. However, the predictive performances of different trees within the
forest can be very different due to the randomization of the embedded bootstrap
sampling and feature selection. In this paper, we focus on RF for regression
and propose two optimal weighting algorithms, namely the 1 Step Optimal
Weighted RF (1step-WRF$_\mathrm{opt}$) and 2 Steps Optimal Weighted RF
(2steps-WRF$_\mathrm{opt}$), that combine the base learners through the weights
determined by weight choice criteria. Under some regularity conditions, we show
that these algorithms are asymptotically optimal in the sense that the
resulting squared loss and risk are asymptotically identical to those of the
infeasible but best possible model averaging estimator. Numerical studies
conducted on real-world data sets indicate that these algorithms outperform the
equal-weight forest and two other weighted RFs proposed in existing literature
in most cases."
1,4,Utility Theory of Synthetic Data Generation,"Evaluating the utility of synthetic data is critical for measuring the
effectiveness and efficiency of synthetic algorithms. Existing results focus on
empirical evaluations of the utility of synthetic data, whereas the theoretical
understanding of how utility is affected by synthetic data algorithms remains
largely unexplored. This paper establishes utility theory from a statistical
perspective, aiming to quantitatively assess the utility of synthetic
algorithms based on a general metric. The metric is defined as the absolute
difference in generalization between models trained on synthetic and original
datasets. We establish analytical bounds for this utility metric to investigate
critical conditions for the metric to converge. An intriguing result is that
the synthetic feature distribution is not necessarily identical to the original
one for the convergence of the utility metric as long as the model
specification in downstream learning tasks is correct. Another important
utility metric is model comparison based on synthetic data. Specifically, we
establish sufficient conditions for synthetic data algorithms so that the
ranking of generalization performances of models trained on the synthetic data
is consistent with that from the original data. Finally, we conduct extensive
experiments using non-parametric models and deep neural networks to validate
our theoretical findings."
3,4,Deep quantum neural networks form Gaussian processes,"It is well known that artificial neural networks initialized from independent
and identically distributed priors converge to Gaussian processes in the limit
of large number of neurons per hidden layer. In this work we prove an analogous
result for Quantum Neural Networks (QNNs). Namely, we show that the outputs of
certain models based on Haar random unitary or orthogonal deep QNNs converge to
Gaussian processes in the limit of large Hilbert space dimension $d$. The
derivation of this result is more nuanced than in the classical case due the
role played by the input states, the measurement observable, and the fact that
the entries of unitary matrices are not independent. An important consequence
of our analysis is that the ensuing Gaussian processes cannot be used to
efficiently predict the outputs of the QNN via Bayesian statistics.
Furthermore, our theorems imply that the concentration of measure phenomenon in
Haar random QNNs is much worse than previously thought, as we prove that
expectation values and gradients concentrate as $\mathcal{O}\left(\frac{1}{e^d
\sqrt{d}}\right)$ -- exponentially in the Hilbert space dimension. Finally, we
discuss how our results improve our understanding of concentration in
$t$-designs."
1,4,Model-based Validation as Probabilistic Inference,"Estimating the distribution over failures is a key step in validating
autonomous systems. Existing approaches focus on finding failures for a small
range of initial conditions or make restrictive assumptions about the
properties of the system under test. We frame estimating the distribution over
failure trajectories for sequential systems as Bayesian inference. Our
model-based approach represents the distribution over failure trajectories
using rollouts of system dynamics and computes trajectory gradients using
automatic differentiation. Our approach is demonstrated in an inverted pendulum
control system, an autonomous vehicle driving scenario, and a partially
observable lunar lander. Sampling is performed using an off-the-shelf
implementation of Hamiltonian Monte Carlo with multiple chains to capture
multimodality and gradient smoothing for safe trajectories. In all experiments,
we observed improvements in sample efficiency and parameter space coverage
compared to black-box baseline approaches. This work is open sourced."
1,4,A Signed Subgraph Encoding Approach via Linear Optimization for Link Sign Prediction,"In this paper, we consider the problem of inferring the sign of a link based
on limited sign data in signed networks. Regarding this link sign prediction
problem, SDGNN (Signed Directed Graph Neural Networks) provides the best
prediction performance currently to the best of our knowledge. In this paper,
we propose a different link sign prediction architecture call SELO (Subgraph
Encoding via Linear Optimization), which obtains overall leading prediction
performances compared the state-of-the-art algorithm SDGNN. The proposed model
utilizes a subgraph encoding approach to learn edge embeddings for signed
directed networks. In particular, a signed subgraph encoding approach is
introduced to embed each subgraph into a likelihood matrix instead of the
adjacency matrix through a linear optimization method. Comprehensive
experiments are conducted on six real-world signed networks with AUC, F1,
micro-F1, and Macro-F1 as the evaluation metrics. The experiment results show
that the proposed SELO model outperforms existing baseline feature-based
methods and embedding-based methods on all the six real-world networks and in
all the four evaluation metrics."
1,4,Mimetic Initialization of Self-Attention Layers,"It is notoriously difficult to train Transformers on small datasets;
typically, large pre-trained models are instead used as the starting point. We
explore the weights of such pre-trained Transformers (particularly for vision)
to attempt to find reasons for this discrepancy. Surprisingly, we find that
simply initializing the weights of self-attention layers so that they ""look""
more like their pre-trained counterparts allows us to train vanilla
Transformers faster and to higher final accuracies, particularly on vision
tasks such as CIFAR-10 and ImageNet classification, where we see gains in
accuracy of over 5% and 4%, respectively. Our initialization scheme is closed
form, learning-free, and very simple: we set the product of the query and key
weights to be approximately the identity, and the product of the value and
projection weights to approximately the negative identity. As this mimics the
patterns we saw in pre-trained Transformers, we call the technique ""mimetic
initialization""."
1,4,Outage Performance and Novel Loss Function for an ML-Assisted Resource Allocation: An Exact Analytical Framework,"Machine Learning (ML) is a popular tool that will be pivotal in enabling 6G
and beyond communications. This paper focuses on applying ML solutions to
address outage probability issues commonly encountered in these systems. In
particular, we consider a single-user multi-resource greedy allocation
strategy, where an ML binary classification predictor assists in seizing an
adequate resource. With no access to future channel state information, this
predictor foresees each resource's likely future outage status. When the
predictor encounters a resource it believes will be satisfactory, it allocates
it to the user. Critically, the goal of the predictor is to ensure that a user
avoids an unsatisfactory resource since this is likely to cause an outage. Our
main result establishes exact and asymptotic expressions for this system's
outage probability. With this, we formulate a theoretically optimal,
differentiable loss function to train our predictor. We then compare predictors
trained using this and traditional loss functions; namely, binary cross-entropy
(BCE), mean squared error (MSE), and mean absolute error (MAE). Predictors
trained using our novel loss function provide superior outage probability in
all scenarios. Our loss function sometimes outperforms predictors trained with
the BCE, MAE, and MSE loss functions by multiple orders of magnitude."
1,4,Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage,"We study distributionally robust offline reinforcement learning (robust
offline RL), which seeks to find an optimal robust policy purely from an
offline dataset that can perform well in perturbed environments. We propose a
generic algorithm framework \underline{D}oubly \underline{P}essimistic
\underline{M}odel-based \underline{P}olicy \underline{O}ptimization
($\texttt{P}^2\texttt{MPO}$) for robust offline RL, which features a novel
combination of a flexible model estimation subroutine and a doubly pessimistic
policy optimization step. The \emph{double pessimism} principle is crucial to
overcome the distributional shift incurred by i) the mismatch between behavior
policy and the family of target policies; and ii) the perturbation of the
nominal model. Under certain accuracy assumptions on the model estimation
subroutine, we show that $\texttt{P}^2\texttt{MPO}$ is provably efficient with
\emph{robust partial coverage data}, which means that the offline dataset has
good coverage of the distributions induced by the optimal robust policy and
perturbed models around the nominal model. By tailoring specific model
estimation subroutines for concrete examples including tabular Robust Markov
Decision Process (RMDP), factored RMDP, and RMDP with kernel and neural
function approximations, we show that $\texttt{P}^2\texttt{MPO}$ enjoys a
$\tilde{\mathcal{O}}(n^{-1/2})$ convergence rate, where $n$ is the number of
trajectories in the offline dataset. Notably, these models, except for the
tabular case, are first identified and proven tractable by this paper. To the
best of our knowledge, we first propose a general learning principle -- double
pessimism -- for robust offline RL and show that it is provably efficient in
the context of general function approximations."
1,4,Balancing Risk and Reward: An Automated Phased Release Strategy,"Phased releases are a common strategy in the technology industry for
gradually releasing new products or updates through a sequence of A/B tests in
which the number of treated units gradually grows until full deployment or
deprecation. Performing phased releases in a principled way requires selecting
the proportion of units assigned to the new release in a way that balances the
risk of an adverse effect with the need to iterate and learn from the
experiment rapidly. In this paper, we formalize this problem and propose an
algorithm that automatically determines the release percentage at each stage in
the schedule, balancing the need to control risk while maximizing ramp-up
speed. Our framework models the challenge as a constrained batched bandit
problem that ensures that our pre-specified experimental budget is not depleted
with high probability. Our proposed algorithm leverages an adaptive Bayesian
approach in which the maximal number of units assigned to the treatment is
determined by the posterior distribution, ensuring that the probability of
depleting the remaining budget is low. Notably, our approach analytically
solves the ramp sizes by inverting probability bounds, eliminating the need for
challenging rare-event Monte Carlo simulation. It only requires computing means
and variances of outcome subsets, making it highly efficient and
parallelizable."
1,4,The Power of Learned Locally Linear Models for Nonlinear Policy Optimization,"A common pipeline in learning-based control is to iteratively estimate a
model of system dynamics, and apply a trajectory optimization algorithm -
e.g.~$\mathtt{iLQR}$ - on the learned model to minimize a target cost. This
paper conducts a rigorous analysis of a simplified variant of this strategy for
general nonlinear systems. We analyze an algorithm which iterates between
estimating local linear models of nonlinear system dynamics and performing
$\mathtt{iLQR}$-like policy updates. We demonstrate that this algorithm attains
sample complexity polynomial in relevant problem parameters, and, by
synthesizing locally stabilizing gains, overcomes exponential dependence in
problem horizon. Experimental results validate the performance of our
algorithm, and compare to natural deep-learning baselines."
1,4,Expressiveness Remarks for Denoising Diffusion Models and Samplers,"Denoising diffusion models are a class of generative models which have
recently achieved state-of-the-art results across many domains. Gradual noise
is added to the data using a diffusion process, which transforms the data
distribution into a Gaussian. Samples from the generative model are then
obtained by simulating an approximation of the time reversal of this diffusion
initialized by Gaussian samples. Recent research has explored adapting
diffusion models for sampling and inference tasks. In this paper, we leverage
known connections to stochastic control akin to the F\""ollmer drift to extend
established neural network approximation results for the F\""ollmer drift to
denoising diffusion models and samplers."
1,4,Toward Falsifying Causal Graphs Using a Permutation-Based Test,"Understanding the causal relationships among the variables of a system is
paramount to explain and control its behaviour. Inferring the causal graph from
observational data without interventions, however, requires a lot of strong
assumptions that are not always realistic. Even for domain experts it can be
challenging to express the causal graph. Therefore, metrics that quantitatively
assess the goodness of a causal graph provide helpful checks before using it in
downstream tasks. Existing metrics provide an absolute number of
inconsistencies between the graph and the observed data, and without a
baseline, practitioners are left to answer the hard question of how many such
inconsistencies are acceptable or expected. Here, we propose a novel
consistency metric by constructing a surrogate baseline through node
permutations. By comparing the number of inconsistencies with those on the
surrogate baseline, we derive an interpretable metric that captures whether the
DAG fits significantly better than random. Evaluating on both simulated and
real data sets from various domains, including biology and cloud monitoring, we
demonstrate that the true DAG is not falsified by our metric, whereas the wrong
graphs given by a hypothetical user are likely to be falsified."
1,4,Learning from Aggregated Data: Curated Bags versus Random Bags,"Protecting user privacy is a major concern for many machine learning systems
that are deployed at scale and collect from a diverse set of population. One
way to address this concern is by collecting and releasing data labels in an
aggregated manner so that the information about a single user is potentially
combined with others. In this paper, we explore the possibility of training
machine learning models with aggregated data labels, rather than individual
labels. Specifically, we consider two natural aggregation procedures suggested
by practitioners: curated bags where the data points are grouped based on
common features and random bags where the data points are grouped randomly in
bag of similar sizes. For the curated bag setting and for a broad range of loss
functions, we show that we can perform gradient-based learning without any
degradation in performance that may result from aggregating data. Our method is
based on the observation that the sum of the gradients of the loss function on
individual data examples in a curated bag can be computed from the aggregate
label without the need for individual labels. For the random bag setting, we
provide a generalization risk bound based on the Rademacher complexity of the
hypothesis class and show how empirical risk minimization can be regularized to
achieve the smallest risk bound. In fact, in the random bag setting, there is a
trade-off between size of the bag and the achievable error rate as our bound
indicates. Finally, we conduct a careful empirical study to confirm our
theoretical findings. In particular, our results suggest that aggregate
learning can be an effective method for preserving user privacy while
maintaining model accuracy."
1,4,A Comparative Study of Methods for Estimating Conditional Shapley Values and When to Use Them,"Shapley values originated in cooperative game theory but are extensively used
today as a model-agnostic explanation framework to explain predictions made by
complex machine learning models in the industry and academia. There are several
algorithmic approaches for computing different versions of Shapley value
explanations. Here, we focus on conditional Shapley values for predictive
models fitted to tabular data. Estimating precise conditional Shapley values is
difficult as they require the estimation of non-trivial conditional
expectations. In this article, we develop new methods, extend earlier proposed
approaches, and systematize the new refined and existing methods into different
method classes for comparison and evaluation. The method classes use either
Monte Carlo integration or regression to model the conditional expectations. We
conduct extensive simulation studies to evaluate how precisely the different
method classes estimate the conditional expectations, and thereby the
conditional Shapley values, for different setups. We also apply the methods to
several real-world data experiments and provide recommendations for when to use
the different method classes and approaches. Roughly speaking, we recommend
using parametric methods when we can specify the data distribution almost
correctly, as they generally produce the most accurate Shapley value
explanations. When the distribution is unknown, both generative methods and
regression models with a similar form as the underlying predictive model are
good and stable options. Regression-based methods are often slow to train but
produce the Shapley value explanations quickly once trained. The vice versa is
true for Monte Carlo-based methods, making the different methods appropriate in
different practical situations."
1,4,Probabilistic Distance-Based Outlier Detection,"The scores of distance-based outlier detection methods are difficult to
interpret, making it challenging to determine a cut-off threshold between
normal and outlier data points without additional context. We describe a
generic transformation of distance-based outlier scores into interpretable,
probabilistic estimates. The transformation is ranking-stable and increases the
contrast between normal and outlier data points. Determining distance
relationships between data points is necessary to identify the nearest-neighbor
relationships in the data, yet, most of the computed distances are typically
discarded. We show that the distances to other data points can be used to model
distance probability distributions and, subsequently, use the distributions to
turn distance-based outlier scores into outlier probabilities. Our experiments
show that the probabilistic transformation does not impact detection
performance over numerous tabular and image benchmark datasets but results in
interpretable outlier scores with increased contrast between normal and outlier
samples. Our work generalizes to a wide range of distance-based outlier
detection methods, and because existing distance computations are used, it adds
no significant computational overhead."
1,4,Lp- and Risk Consistency of Localized SVMs,"Kernel-based regularized risk minimizers, also called support vector machines
(SVMs), are known to possess many desirable properties but suffer from their
super-linear computational requirements when dealing with large data sets. This
problem can be tackled by using localized SVMs instead, which also offer the
additional advantage of being able to apply different hyperparameters to
different regions of the input space. In this paper, localized SVMs are
analyzed with regards to their consistency. It is proven that they inherit
$L_p$- as well as risk consistency from global SVMs under very weak conditions
and even if the regions underlying the localized SVMs are allowed to change as
the size of the training data set increases."
1,4,Errors-in-variables Frchet Regression with Low-rank Covariate Approximation,"Fr\'echet regression has emerged as a promising approach for regression
analysis involving non-Euclidean response variables. However, its practical
applicability has been hindered by its reliance on ideal scenarios with
abundant and noiseless covariate data. In this paper, we present a novel
estimation method that tackles these limitations by leveraging the low-rank
structure inherent in the covariate matrix. Our proposed framework combines the
concepts of global Fr\'echet regression and principal component regression,
aiming to improve the efficiency and accuracy of the regression estimator. By
incorporating the low-rank structure, our method enables more effective
modeling and estimation, particularly in high-dimensional and
errors-in-variables regression settings. We provide a theoretical analysis of
the proposed estimator's large-sample properties, including a comprehensive
rate analysis of bias, variance, and additional variations due to measurement
errors. Furthermore, our numerical experiments provide empirical evidence that
supports the theoretical findings, demonstrating the superior performance of
our approach. Overall, this work introduces a promising framework for
regression analysis of non-Euclidean variables, effectively addressing the
challenges associated with limited and noisy covariate data, with potential
applications in diverse fields."
1,4,Transfer Causal Learning: Causal Effect Estimation with Knowledge Transfer,"A novel problem of improving causal effect estimation accuracy with the help
of knowledge transfer under the same covariate (or feature) space setting,
i.e., homogeneous transfer learning (TL), is studied, referred to as the
Transfer Causal Learning (TCL) problem. While most recent efforts in adapting
TL techniques to estimate average causal effect (ACE) have been focused on the
heterogeneous covariate space setting, those methods are inadequate for
tackling the TCL problem since their algorithm designs are based on the
decomposition into shared and domain-specific covariate spaces. To address this
issue, we propose a generic framework called \texttt{$\ell_1$-TCL}, which
incorporates $\ell_1$ regularized TL for nuisance parameter estimation and
downstream plug-in ACE estimators, including outcome regression, inverse
probability weighted, and doubly robust estimators. Most importantly, with the
help of Lasso for high-dimensional regression, we establish non-asymptotic
recovery guarantees for the generalized linear model (GLM) under the sparsity
assumption for the proposed \texttt{$\ell_1$-TCL}. Moreover, the success of
\texttt{$\ell_1$-TCL} could inspire the adaptations of many recently proposed
principled approaches in statistics literature to be adapted to this novel TCL
problem. From an empirical perspective, \texttt{$\ell_1$-TCL} is a generic
learning framework that can incorporate not only GLM but also many recently
developed non-parametric methods, which can enhance robustness to model
mis-specification. We demonstrate this empirical benefit through extensive
experiments using GLM and recent neural network based \texttt{$\ell_1$-TCL} on
both benchmark semi-synthetic and real datasets, which shows improved
performance compared with existing TL approaches for ACE estimation."
3,4,The Hessian perspective into the Nature of Convolutional Neural Networks,"While Convolutional Neural Networks (CNNs) have long been investigated and
applied, as well as theorized, we aim to provide a slightly different
perspective into their nature -- through the perspective of their Hessian maps.
The reason is that the loss Hessian captures the pairwise interaction of
parameters and therefore forms a natural ground to probe how the architectural
aspects of CNN get manifested in its structure and properties. We develop a
framework relying on Toeplitz representation of CNNs, and then utilize it to
reveal the Hessian structure and, in particular, its rank. We prove tight upper
bounds (with linear activations), which closely follow the empirical trend of
the Hessian rank and hold in practice in more general settings. Overall, our
work generalizes and establishes the key insight that, even in CNNs, the
Hessian rank grows as the square root of the number of parameters."
3,4,Convex optimization over a probability simplex,"We propose a new iteration scheme, the Cauchy-Simplex, to optimize convex
problems over the probability simplex $\{w\in\mathbb{R}^n\ |\ \sum_i w_i=1\
\textrm{and}\ w_i\geq0\}$. Other works have taken steps to enforce positivity
or unit normalization automatically but never simultaneously within a unified
setting. This paper presents a natural framework for manifestly requiring the
probability condition. Specifically, we map the simplex to the positive
quadrant of a unit sphere, envisage gradient descent in latent variables, and
map the result back in a way that only depends on the simplex variable.
Moreover, proving rigorous convergence results in this formulation leads
inherently to tools from information theory (e.g. cross entropy and KL
divergence). Each iteration of the Cauchy-Simplex consists of simple
operations, making it well-suited for high-dimensional problems. We prove that
it has a convergence rate of ${O}(1/T)$ for convex functions, and numerical
experiments of projection onto convex hulls show faster convergence than
similar algorithms. Finally, we apply our algorithm to online learning problems
and prove the convergence of the average regret for (1) Prediction with expert
advice and (2) Universal Portfolios."
1,4,Scalable and Robust Tensor Ring Decomposition for Large-scale Data,"Tensor ring (TR) decomposition has recently received increased attention due
to its superior expressive performance for high-order tensors. However, the
applicability of traditional TR decomposition algorithms to real-world
applications is hindered by prevalent large data sizes, missing entries, and
corruption with outliers. In this work, we propose a scalable and robust TR
decomposition algorithm capable of handling large-scale tensor data with
missing entries and gross corruptions. We first develop a novel auto-weighted
steepest descent method that can adaptively fill the missing entries and
identify the outliers during the decomposition process. Further, taking
advantage of the tensor ring model, we develop a novel fast Gram matrix
computation (FGMC) approach and a randomized subtensor sketching (RStS)
strategy which yield significant reduction in storage and computational
complexity. Experimental results demonstrate that the proposed method
outperforms existing TR decomposition methods in the presence of outliers, and
runs significantly faster than existing robust tensor completion algorithms."
3,4,SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels,"Toeplitz Neural Networks (TNNs) (Qin et. al. 2023) are a recent sequence
model with impressive results. They require O(n log n) computational complexity
and O(n) relative positional encoder (RPE) multi-layer perceptron (MLP) and
decay bias calls. We aim to reduce both. We first note that the RPE is a
non-SPD (symmetric positive definite) kernel and the Toeplitz matrices are
pseudo-Gram matrices. Further 1) the learned kernels display spiky behavior
near the main diagonals with otherwise smooth behavior; 2) the RPE MLP is slow.
For bidirectional models, this motivates a sparse plus low-rank Toeplitz matrix
decomposition. For the sparse component's action, we do a small 1D convolution.
For the low rank component, we replace the RPE MLP with linear interpolation
and use asymmetric Structured Kernel Interpolation (SKI) (Wilson et. al. 2015)
for O(n) complexity: we provide rigorous error analysis. For causal models,
""fast"" causal masking (Katharopoulos et. al. 2020) negates SKI's benefits.
Working in the frequency domain, we avoid an explicit decay bias. To enforce
causality, we represent the kernel via the real part of its frequency response
using the RPE and compute the imaginary part via a Hilbert transform. This
maintains O(n log n) complexity but achieves an absolute speedup. Modeling the
frequency response directly is also competitive for bidirectional training,
using one fewer FFT. We set a speed state of the art on Long Range Arena (Tay
et. al. 2020) with minimal score degradation."
1,4,A Causal Inference Framework for Leveraging External Controls in Hybrid Trials,"We consider the challenges associated with causal inference in settings where
data from a randomized trial is augmented with control data from an external
source to improve efficiency in estimating the average treatment effect (ATE).
Through the development of a formal causal inference framework, we outline
sufficient causal assumptions about the exchangeability between the internal
and external controls to identify the ATE and establish the connection to a
novel graphical criteria. We propose estimators, review efficiency bounds,
develop an approach for efficient doubly-robust estimation even when unknown
nuisance models are estimated with flexible machine learning methods, and
demonstrate finite-sample performance through a simulation study. To illustrate
the ideas and methods, we apply the framework to a trial investigating the
effect of risdisplam on motor function in patients with spinal muscular atrophy
for which there exists an external set of control patients from a previous
trial."
1,4,A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes,"The proximal policy optimization (PPO) algorithm stands as one of the most
prosperous methods in the field of reinforcement learning (RL). Despite its
success, the theoretical understanding of PPO remains deficient. Specifically,
it is unclear whether PPO or its optimistic variants can effectively solve
linear Markov decision processes (MDPs), which are arguably the simplest models
in RL with function approximation. To bridge this gap, we propose an optimistic
variant of PPO for episodic adversarial linear MDPs with full-information
feedback, and establish a $\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$ regret for
it. Here $d$ is the ambient dimension of linear MDPs, $H$ is the length of each
episode, and $K$ is the number of episodes. Compared with existing policy-based
algorithms, we achieve the state-of-the-art regret bound in both stochastic
linear MDPs and adversarial linear MDPs with full information. Additionally,
our algorithm design features a novel multi-batched updating mechanism and the
theoretical analysis utilizes a new covering number argument of value and
policy classes, which might be of independent interest."
1,4,Fair Information Spread on Social Networks with Community Structure,"Information spread through social networks is ubiquitous. Influence maximiza-
tion (IM) algorithms aim to identify individuals who will generate the greatest
spread through the social network if provided with information, and have been
largely devel- oped with marketing in mind. In social networks with community
structure, which are very common, IM algorithms focused solely on maximizing
spread may yield signifi- cant disparities in information coverage between
communities, which is problematic in settings such as public health messaging.
While some IM algorithms aim to remedy disparity in information coverage using
node attributes, none use the empirical com- munity structure within the
network itself, which may be beneficial since communities directly affect the
spread of information. Further, the use of empirical network struc- ture allows
us to leverage community detection techniques, making it possible to run
fair-aware algorithms when there are no relevant node attributes available, or
when node attributes do not accurately capture network community structure. In
contrast to other fair IM algorithms, this work relies on fitting a model to
the social network which is then used to determine a seed allocation strategy
for optimal fair information spread. We develop an algorithm to determine
optimal seed allocations for expected fair coverage, defined through maximum
entropy, provide some theoretical guarantees under appropriate conditions, and
demonstrate its empirical accuracy on both simu- lated and real networks.
Because this algorithm relies on a fitted network model and not on the network
directly, it is well-suited for partially observed and noisy social networks."
3,4,Accelerated Algorithms for Nonlinear Matrix Decomposition with the ReLU function,"In this paper, we study the following nonlinear matrix decomposition (NMD)
problem: given a sparse nonnegative matrix $X$, find a low-rank matrix $\Theta$
such that $X \approx f(\Theta)$, where $f$ is an element-wise nonlinear
function. We focus on the case where $f(\cdot) = \max(0, \cdot)$, the rectified
unit (ReLU) non-linear activation. We refer to the corresponding problem as
ReLU-NMD. We first provide a brief overview of the existing approaches that
were developed to tackle ReLU-NMD. Then we introduce two new algorithms: (1)
aggressive accelerated NMD (A-NMD) which uses an adaptive Nesterov
extrapolation to accelerate an existing algorithm, and (2) three-block NMD
(3B-NMD) which parametrizes $\Theta = WH$ and leads to a significant reduction
in the computational cost. We also propose an effective initialization strategy
based on the nuclear norm as a proxy for the rank function. We illustrate the
effectiveness of the proposed algorithms (available on gitlab) on synthetic and
real-world data sets."
1,4,"On the connections between optimization algorithms, Lyapunov functions, and differential equations: theory and insights","We study connections between differential equations and optimization
algorithms for $m$-strongly and $L$-smooth convex functions through the use of
Lyapunov functions by generalizing the Linear Matrix Inequality framework
developed by Fazylab et al. in 2018. Using the new framework we derive
analytically a new (discrete) Lyapunov function for a two-parameter family of
Nesterov optimization methods and characterize their convergence rate. This
allows us to prove a convergence rate that improves substantially on the
previously proven rate of Nesterov's method for the standard choice of
coefficients, as well as to characterize the choice of coefficients that yields
the optimal rate. We obtain a new Lyapunov function for the Polyak ODE and
revisit the connection between this ODE and the Nesterov's algorithms. In
addition discuss a new interpretation of Nesterov method as an additive
Runge-Kutta discretization and explain the structural conditions that
discretizations of the Polyak equation should satisfy in order to lead to
accelerated optimization algorithms."
1,4,Encoding Domain Expertise into Multilevel Models for Source Location,"Data from populations of systems are prevalent in many industrial
applications. Machines and infrastructure are increasingly instrumented with
sensing systems, emitting streams of telemetry data with complex
interdependencies. In practice, data-centric monitoring procedures tend to
consider these assets (and respective models) as distinct -- operating in
isolation and associated with independent data. In contrast, this work captures
the statistical correlations and interdependencies between models of a group of
systems. Utilising a Bayesian multilevel approach, the value of data can be
extended, since the population can be considered as a whole, rather than
constituent parts. Most interestingly, domain expertise and knowledge of the
underlying physics can be encoded in the model at the system, subgroup, or
population level. We present an example of acoustic emission (time-of-arrival)
mapping for source location, to illustrate how multilevel models naturally lend
themselves to representing aggregate systems in engineering. In particular, we
focus on constraining the combined models with domain knowledge to enhance
transfer learning and enable further insights at the population level."
1,4,Topological Interpretability for Deep-Learning,"With the increasing adoption of AI-based systems across everyday life, the
need to understand their decision-making mechanisms is correspondingly
accelerating. The level at which we can trust the statistical inferences made
from AI-based decision systems is an increasing concern, especially in
high-risk systems such as criminal justice or medical diagnosis, where
incorrect inferences may have tragic consequences. Despite their successes in
providing solutions to problems involving real-world data, deep learning (DL)
models cannot quantify the certainty of their predictions. And are frequently
quite confident, even when their solutions are incorrect.
  This work presents a method to infer prominent features in two DL
classification models trained on clinical and non-clinical text by employing
techniques from topological and geometric data analysis. We create a graph of a
model's prediction space and cluster the inputs into the graph's vertices by
the similarity of features and prediction statistics. We then extract subgraphs
demonstrating high-predictive accuracy for a given label. These subgraphs
contain a wealth of information about features that the DL model has recognized
as relevant to its decisions. We infer these features for a given label using a
distance metric between probability measures, and demonstrate the stability of
our method compared to the LIME interpretability method. This work demonstrates
that we may gain insights into the decision mechanism of a DL model, which
allows us to ascertain if the model is making its decisions based on
information germane to the problem or identifies extraneous patterns within the
data."
3,4,Double-Weighting for Covariate Shift Adaptation,"Supervised learning is often affected by a covariate shift in which the
marginal distributions of instances (covariates $x$) of training and testing
samples $\mathrm{p}_\text{tr}(x)$ and $\mathrm{p}_\text{te}(x)$ are different
but the label conditionals coincide. Existing approaches address such covariate
shift by either using the ratio
$\mathrm{p}_\text{te}(x)/\mathrm{p}_\text{tr}(x)$ to weight training samples
(reweighting methods) or using the ratio
$\mathrm{p}_\text{tr}(x)/\mathrm{p}_\text{te}(x)$ to weight testing samples
(robust methods). However, the performance of such approaches can be poor under
support mismatch or when the above ratios take large values. We propose a
minimax risk classification (MRC) approach for covariate shift adaptation that
avoids such limitations by weighting both training and testing samples. In
addition, we develop effective techniques that obtain both sets of weights and
generalize the conventional kernel mean matching method. We provide novel
generalization bounds for our method that show a significant increase in the
effective sample size compared with reweighted methods. The proposed method
also achieves enhanced classification performance in both synthetic and
empirical experiments."
1,4,Kernel-based Joint Independence Tests for Multivariate Stationary and Nonstationary Time-Series,"Multivariate time-series data that capture the temporal evolution of
interconnected systems are ubiquitous in diverse areas. Understanding the
complex relationships and potential dependencies among co-observed variables is
crucial for the accurate statistical modelling and analysis of such systems.
Here, we introduce kernel-based statistical tests of joint independence in
multivariate time-series by extending the d-variable Hilbert-Schmidt
independence criterion (dHSIC) to encompass both stationary and nonstationary
random processes, thus allowing broader real-world applications. By leveraging
resampling techniques tailored for both single- and multiple-realization time
series, we show how the method robustly uncovers significant higher-order
dependencies in synthetic examples, including frequency mixing data, as well as
real-world climate and socioeconomic data. Our method adds to the mathematical
toolbox for the analysis of complex high-dimensional time-series datasets."
1,4,Label Smoothing is Robustification against Model Misspecification,"Label smoothing (LS) adopts smoothed targets in classification tasks. For
example, in binary classification, instead of the one-hot target $(1,0)^\top$
used in conventional logistic regression (LR), LR with LS (LSLR) uses the
smoothed target $(1-\frac{\alpha}{2},\frac{\alpha}{2})^\top$ with a smoothing
level $\alpha\in(0,1)$, which causes squeezing of values of the logit. Apart
from the common regularization-based interpretation of LS that leads to an
inconsistent probability estimator, we regard LSLR as modifying the loss
function and consistent estimator for probability estimation. In order to study
the significance of each of these two modifications by LSLR, we introduce a
modified LSLR (MLSLR) that uses the same loss function as LSLR and the same
consistent estimator as LR, while not squeezing the logits. For the loss
function modification, we theoretically show that MLSLR with a larger smoothing
level has lower efficiency with correctly-specified models, while it exhibits
higher robustness against model misspecification than LR. Also, for the
modification of the probability estimator, an experimental comparison between
LSLR and MLSLR showed that this modification and squeezing of the logits in
LSLR have negative effects on the probability estimation and classification
performance. The understanding of the properties of LS provided by these
comparisons allows us to propose MLSLR as an improvement over LSLR."
1,4,Convergence Analysis of Mean Shift,"The mean shift (MS) algorithm seeks a mode of the kernel density estimate
(KDE). This study presents a convergence guarantee of the mode estimate
sequence generated by the MS algorithm and an evaluation of the convergence
rate, under fairly mild conditions, with the help of the argument concerning
the {\L}ojasiewicz inequality. Our findings, which extend existing ones
covering analytic kernels and the Epanechnikov kernel, are significant in that
they cover the biweight kernel that is optimal among non-negative kernels in
terms of the asymptotic statistical efficiency for the KDE-based mode
estimation."
1,4,MolHF: A Hierarchical Normalizing Flow for Molecular Graph Generation,"Molecular de novo design is a critical yet challenging task in scientific
fields, aiming to design novel molecular structures with desired property
profiles. Significant progress has been made by resorting to generative models
for graphs. However, limited attention is paid to hierarchical generative
models, which can exploit the inherent hierarchical structure (with rich
semantic information) of the molecular graphs and generate complex molecules of
larger size that we shall demonstrate to be difficult for most existing models.
The primary challenge to hierarchical generation is the non-differentiable
issue caused by the generation of intermediate discrete coarsened graph
structures. To sidestep this issue, we cast the tricky hierarchical generation
problem over discrete spaces as the reverse process of hierarchical
representation learning and propose MolHF, a new hierarchical flow-based model
that generates molecular graphs in a coarse-to-fine manner. Specifically, MolHF
first generates bonds through a multi-scale architecture, then generates atoms
based on the coarsened graph structure at each scale. We demonstrate that MolHF
achieves state-of-the-art performance in random generation and property
optimization, implying its high capacity to model data distribution.
Furthermore, MolHF is the first flow-based model that can be applied to model
larger molecules (polymer) with more than 100 heavy atoms. The code and models
are available at https://github.com/violet-sto/MolHF."
3,4,Theoretical Analysis of Inductive Biases in Deep Convolutional Networks,"In this paper, we study the inductive biases in convolutional neural networks
(CNNs), which are believed to be vital drivers behind CNNs' exceptional
performance on vision-like tasks. We first analyze the universality of CNNs,
i.e., the ability to approximate continuous functions. We prove that a depth of
$\mathcal{O}(\log d)$ is sufficient for achieving universality, where $d$ is
the input dimension. This is a significant improvement over existing results
that required a depth of $\Omega(d)$. We also prove that learning sparse
functions with CNNs needs only $\tilde{\mathcal{O}}(\log^2d)$ samples,
indicating that deep CNNs can efficiently capture long-range sparse
correlations. Note that all these are achieved through a novel combination of
increased network depth and the utilization of multichanneling and
downsampling.
  Lastly, we study the inductive biases of weight sharing and locality through
the lens of symmetry. To separate two biases, we introduce locally-connected
networks (LCNs), which can be viewed as CNNs without weight sharing.
Specifically, we compare the performance of CNNs, LCNs, and fully-connected
networks (FCNs) on a simple regression task. We prove that LCNs require
${\Omega}(d)$ samples while CNNs need only $\tilde{\mathcal{O}}(\log^2d)$
samples, which highlights the cruciality of weight sharing. We also prove that
FCNs require $\Omega(d^2)$ samples while LCNs need only
$\tilde{\mathcal{O}}(d)$ samples, demonstrating the importance of locality.
These provable separations quantify the difference between the two biases, and
our major observation behind is that weight sharing and locality break
different symmetries in the learning process."
3,4,Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs,"Recent studies have shown that episodic reinforcement learning (RL) is no
harder than bandits when the total reward is bounded by $1$, and proved regret
bounds that have a polylogarithmic dependence on the planning horizon $H$.
However, it remains an open question that if such results can be carried over
to adversarial RL, where the reward is adversarially chosen at each episode. In
this paper, we answer this question affirmatively by proposing the first
horizon-free policy search algorithm. To tackle the challenges caused by
exploration and adversarially chosen reward, our algorithm employs (1) a
variance-uncertainty-aware weighted least square estimator for the transition
kernel; and (2) an occupancy measure-based technique for the online search of a
\emph{stochastic} policy. We show that our algorithm achieves an
$\tilde{O}\big((d+\log (|\mathcal{S}|^2 |\mathcal{A}|))\sqrt{K}\big)$ regret
with full-information feedback, where $d$ is the dimension of a known feature
mapping linearly parametrizing the unknown transition kernel of the MDP, $K$ is
the number of episodes, $|\mathcal{S}|$ and $|\mathcal{A}|$ are the
cardinalities of the state and action spaces. We also provide hardness results
and regret lower bounds to justify the near optimality of our algorithm and the
unavoidability of $\log|\mathcal{S}|$ and $\log|\mathcal{A}|$ in the regret
bound."
1,4,Uniform-PAC Guarantees for Model-Based RL with Bounded Eluder Dimension,"Recently, there has been remarkable progress in reinforcement learning (RL)
with general function approximation. However, all these works only provide
regret or sample complexity guarantees. It is still an open question if one can
achieve stronger performance guarantees, i.e., the uniform probably approximate
correctness (Uniform-PAC) guarantee that can imply both a sub-linear regret
bound and a polynomial sample complexity for any target learning accuracy. We
study this problem by proposing algorithms for both nonlinear bandits and
model-based episodic RL using the general function class with a bounded eluder
dimension. The key idea of the proposed algorithms is to assign each action to
different levels according to its width with respect to the confidence set. The
achieved uniform-PAC sample complexity is tight in the sense that it matches
the state-of-the-art regret bounds or sample complexity guarantees when reduced
to the linear case. To the best of our knowledge, this is the first work for
uniform-PAC guarantees on bandit and RL that goes beyond linear cases."
1,4,Neural Boltzmann Machines,"Conditional generative models are capable of using contextual information as
input to create new imaginative outputs. Conditional Restricted Boltzmann
Machines (CRBMs) are one class of conditional generative models that have
proven to be especially adept at modeling noisy discrete or continuous data,
but the lack of expressivity in CRBMs have limited their widespread adoption.
Here we introduce Neural Boltzmann Machines (NBMs) which generalize CRBMs by
converting each of the CRBM parameters to their own neural networks that are
allowed to be functions of the conditional inputs. NBMs are highly flexible
conditional generative models that can be trained via stochastic gradient
descent to approximately maximize the log-likelihood of the data. We
demonstrate the utility of NBMs especially with normally distributed data which
has historically caused problems for Gaussian-Bernoulli CRBMs. Code to
reproduce our results can be found at
https://github.com/unlearnai/neural-boltzmann-machines."
1,4,Local Convergence of Gradient Descent-Ascent for Training Generative Adversarial Networks,"Generative Adversarial Networks (GANs) are a popular formulation to train
generative models for complex high dimensional data. The standard method for
training GANs involves a gradient descent-ascent (GDA) procedure on a minimax
optimization problem. This procedure is hard to analyze in general due to the
nonlinear nature of the dynamics. We study the local dynamics of GDA for
training a GAN with a kernel-based discriminator. This convergence analysis is
based on a linearization of a non-linear dynamical system that describes the
GDA iterations, under an \textit{isolated points model} assumption from [Becker
et al. 2022]. Our analysis brings out the effect of the learning rates,
regularization, and the bandwidth of the kernel discriminator, on the local
convergence rate of GDA. Importantly, we show phase transitions that indicate
when the system converges, oscillates, or diverges. We also provide numerical
simulations that verify our claims."
1,4,Orthogonal polynomial approximation and Extended Dynamic Mode Decomposition in chaos,"Extended Dynamic Mode Decomposition (EDMD) is a data-driven tool for
forecasting and model reduction of dynamics, which has been extensively taken
up in the physical sciences. While the method is conceptually simple, in
deterministic chaos it is unclear what its properties are or even what it
converges to. In particular, it is not clear how EDMD's least-squares
approximation treats the classes of regular functions needed to make sense of
chaotic dynamics.
  In this paper we develop a general, rigorous theory of EDMD on the simplest
examples of chaotic maps: analytic expanding maps of the circle. To do this, we
prove a new result in the theory of orthogonal polynomials on the unit circle
(OPUC) and apply methods from transfer operator theory. We show that in the
infinite-data limit, the least-squares projection is exponentially efficient
for trigonometric polynomial observable dictionaries. As a result, we show that
the forecasts and Koopman spectral data produced using EDMD in this setting
converge to the physically meaningful limits, exponentially quickly in the size
of the dictionary. This demonstrates that with only a relatively small
polynomial dictionary, EDMD can be very effective, even when the sampling
measure is not uniform. Furthermore, our OPUC result suggests that data-based
least-squares projections may be a very effective approximation strategy."
